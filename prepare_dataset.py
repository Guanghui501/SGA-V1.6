"""å‡†å¤‡è‡ªå®šä¹‰æ•°æ®é›†çš„ç¤ºä¾‹è„šæœ¬

æ”¯æŒä»å¤šç§æ ¼å¼åˆ›å»ºæ•°æ®é›†:
- CIF æ–‡ä»¶
- POSCAR æ–‡ä»¶
- æ‰‹åŠ¨æ„é€ 

ä½¿ç”¨æ–¹æ³•:
    python prepare_dataset.py --help
"""

import json
import argparse
import os
from pathlib import Path
from jarvis.core.atoms import Atoms


def from_cif_files(cif_dir, target_file, output_file):
    """ä» CIF æ–‡ä»¶ç›®å½•åˆ›å»ºæ•°æ®é›†

    Args:
        cif_dir: CIF æ–‡ä»¶ç›®å½•
        target_file: ç›®æ ‡å€¼ CSV æ–‡ä»¶ (æ ¼å¼: filename,target_value)
        output_file: è¾“å‡º JSON æ–‡ä»¶
    """
    import pandas as pd

    print(f"ğŸ“‚ ä» CIF æ–‡ä»¶åˆ›å»ºæ•°æ®é›†...")
    print(f"   CIF ç›®å½•: {cif_dir}")
    print(f"   ç›®æ ‡æ–‡ä»¶: {target_file}")

    # è¯»å–ç›®æ ‡å€¼
    targets_df = pd.read_csv(target_file)
    print(f"   è¯»å– {len(targets_df)} ä¸ªç›®æ ‡å€¼")

    dataset = []
    for idx, row in targets_df.iterrows():
        filename = row['filename']
        target_value = row['target_value']

        cif_path = os.path.join(cif_dir, filename)
        if not os.path.exists(cif_path):
            print(f"   âš ï¸  è·³è¿‡: {filename} (æ–‡ä»¶ä¸å­˜åœ¨)")
            continue

        try:
            # ä» CIF åŠ è½½ç»“æ„
            atoms = Atoms.from_cif(cif_path)

            # åˆ›å»ºæ•°æ®æ¡ç›®
            data_entry = {
                "jid": Path(filename).stem,  # ä½¿ç”¨æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸º ID
                "atoms": atoms.to_dict(),
                "formation_energy_peratom": float(target_value),
                "text_description": f"Structure from {filename}"
            }

            dataset.append(data_entry)

        except Exception as e:
            print(f"   âŒ é”™è¯¯: {filename} - {str(e)}")

    # ä¿å­˜æ•°æ®é›†
    with open(output_file, "w") as f:
        json.dump(dataset, f, indent=2)

    print(f"\nâœ… æˆåŠŸåˆ›å»ºæ•°æ®é›†: {output_file}")
    print(f"   æ€»æ ·æœ¬æ•°: {len(dataset)}")


def from_poscar_files(poscar_dir, target_file, output_file):
    """ä» POSCAR æ–‡ä»¶ç›®å½•åˆ›å»ºæ•°æ®é›†"""
    import pandas as pd

    print(f"ğŸ“‚ ä» POSCAR æ–‡ä»¶åˆ›å»ºæ•°æ®é›†...")
    print(f"   POSCAR ç›®å½•: {poscar_dir}")
    print(f"   ç›®æ ‡æ–‡ä»¶: {target_file}")

    targets_df = pd.read_csv(target_file)
    print(f"   è¯»å– {len(targets_df)} ä¸ªç›®æ ‡å€¼")

    dataset = []
    for idx, row in targets_df.iterrows():
        filename = row['filename']
        target_value = row['target_value']

        poscar_path = os.path.join(poscar_dir, filename)
        if not os.path.exists(poscar_path):
            print(f"   âš ï¸  è·³è¿‡: {filename} (æ–‡ä»¶ä¸å­˜åœ¨)")
            continue

        try:
            atoms = Atoms.from_poscar(poscar_path)

            data_entry = {
                "jid": Path(filename).stem,
                "atoms": atoms.to_dict(),
                "formation_energy_peratom": float(target_value),
                "text_description": f"Structure from {filename}"
            }

            dataset.append(data_entry)

        except Exception as e:
            print(f"   âŒ é”™è¯¯: {filename} - {str(e)}")

    with open(output_file, "w") as f:
        json.dump(dataset, f, indent=2)

    print(f"\nâœ… æˆåŠŸåˆ›å»ºæ•°æ®é›†: {output_file}")
    print(f"   æ€»æ ·æœ¬æ•°: {len(dataset)}")


def create_example_dataset(output_file, num_samples=10):
    """åˆ›å»ºç¤ºä¾‹æ•°æ®é›†ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    import numpy as np
    from jarvis.core.lattice import Lattice

    print(f"ğŸ“ åˆ›å»ºç¤ºä¾‹æ•°æ®é›† ({num_samples} ä¸ªæ ·æœ¬)...")

    dataset = []

    # åˆ›å»ºä¸åŒçš„ç¤ºä¾‹ç»“æ„
    structures = [
        # ç®€å•ç«‹æ–¹
        {
            "lattice": [[4.0, 0, 0], [0, 4.0, 0], [0, 0, 4.0]],
            "coords": [[0, 0, 0]],
            "elements": ["Si"],
            "name": "simple_cubic_Si"
        },
        # é¢å¿ƒç«‹æ–¹
        {
            "lattice": [[4.0, 0, 0], [0, 4.0, 0], [0, 0, 4.0]],
            "coords": [[0, 0, 0], [0.5, 0.5, 0], [0.5, 0, 0.5], [0, 0.5, 0.5]],
            "elements": ["Al", "Al", "Al", "Al"],
            "name": "fcc_Al"
        },
        # é‡‘åˆšçŸ³ç»“æ„
        {
            "lattice": [[5.43, 0, 0], [0, 5.43, 0], [0, 0, 5.43]],
            "coords": [
                [0, 0, 0], [0.25, 0.25, 0.25],
                [0.5, 0.5, 0], [0.75, 0.75, 0.25],
                [0.5, 0, 0.5], [0.75, 0.25, 0.75],
                [0, 0.5, 0.5], [0.25, 0.75, 0.75]
            ],
            "elements": ["Si"] * 8,
            "name": "diamond_Si"
        }
    ]

    for i in range(num_samples):
        # é€‰æ‹©ä¸€ä¸ªç»“æ„æ¨¡æ¿
        template = structures[i % len(structures)]

        # ç¨å¾®ä¿®æ”¹æ™¶æ ¼å‚æ•°ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
        lattice_scale = 1.0 + np.random.randn() * 0.05
        lattice = Lattice(np.array(template["lattice"]) * lattice_scale)

        # åˆ›å»ºåŸå­ç»“æ„
        atoms = Atoms(
            lattice_mat=lattice.matrix,
            coords=template["coords"],
            elements=template["elements"]
        )

        # ç”Ÿæˆéšæœºç›®æ ‡å€¼ï¼ˆç¤ºä¾‹ï¼‰
        target_value = -3.0 + np.random.randn() * 1.0

        data_entry = {
            "jid": f"{template['name']}_{i:03d}",
            "atoms": atoms.to_dict(),
            "formation_energy_peratom": float(target_value),
            "text_description": f"Example {template['name']} structure with scale {lattice_scale:.3f}"
        }

        dataset.append(data_entry)

    # ä¿å­˜æ•°æ®é›†
    with open(output_file, "w") as f:
        json.dump(dataset, f, indent=2)

    print(f"\nâœ… æˆåŠŸåˆ›å»ºç¤ºä¾‹æ•°æ®é›†: {output_file}")
    print(f"   æ€»æ ·æœ¬æ•°: {len(dataset)}")
    print(f"   ç»“æ„ç±»å‹: {len(structures)}")


def validate_dataset(dataset_file):
    """éªŒè¯æ•°æ®é›†æ–‡ä»¶"""
    print(f"ğŸ” éªŒè¯æ•°æ®é›†: {dataset_file}")

    with open(dataset_file, "r") as f:
        dataset = json.load(f)

    print(f"   æ€»æ ·æœ¬æ•°: {len(dataset)}")

    # æ£€æŸ¥æ ¼å¼
    required_keys = ["jid", "atoms"]
    errors = []

    for i, data in enumerate(dataset[:10]):  # åªæ£€æŸ¥å‰10ä¸ª
        for key in required_keys:
            if key not in data:
                errors.append(f"æ ·æœ¬ {i}: ç¼ºå°‘ '{key}'")

        if "atoms" in data:
            atoms_keys = ["lattice_mat", "coords", "elements"]
            for key in atoms_keys:
                if key not in data["atoms"]:
                    errors.append(f"æ ·æœ¬ {i}: atoms ç¼ºå°‘ '{key}'")

    if errors:
        print("   âŒ å‘ç°é—®é¢˜:")
        for error in errors:
            print(f"      - {error}")
    else:
        print("   âœ… æ ¼å¼æ­£ç¡®")


def main():
    parser = argparse.ArgumentParser(description="å‡†å¤‡è‡ªå®šä¹‰æ•°æ®é›†")
    subparsers = parser.add_subparsers(dest="command", help="å‘½ä»¤")

    # from-cif å‘½ä»¤
    cif_parser = subparsers.add_parser("from-cif", help="ä» CIF æ–‡ä»¶åˆ›å»ºæ•°æ®é›†")
    cif_parser.add_argument("--cif-dir", required=True, help="CIF æ–‡ä»¶ç›®å½•")
    cif_parser.add_argument("--target-file", required=True, help="ç›®æ ‡å€¼ CSV æ–‡ä»¶")
    cif_parser.add_argument("--output", default="my_dataset.json", help="è¾“å‡º JSON æ–‡ä»¶")

    # from-poscar å‘½ä»¤
    poscar_parser = subparsers.add_parser("from-poscar", help="ä» POSCAR æ–‡ä»¶åˆ›å»ºæ•°æ®é›†")
    poscar_parser.add_argument("--poscar-dir", required=True, help="POSCAR æ–‡ä»¶ç›®å½•")
    poscar_parser.add_argument("--target-file", required=True, help="ç›®æ ‡å€¼ CSV æ–‡ä»¶")
    poscar_parser.add_argument("--output", default="my_dataset.json", help="è¾“å‡º JSON æ–‡ä»¶")

    # example å‘½ä»¤
    example_parser = subparsers.add_parser("example", help="åˆ›å»ºç¤ºä¾‹æ•°æ®é›†")
    example_parser.add_argument("--output", default="example_dataset.json", help="è¾“å‡º JSON æ–‡ä»¶")
    example_parser.add_argument("--num-samples", type=int, default=10, help="æ ·æœ¬æ•°é‡")

    # validate å‘½ä»¤
    validate_parser = subparsers.add_parser("validate", help="éªŒè¯æ•°æ®é›†")
    validate_parser.add_argument("dataset_file", help="æ•°æ®é›† JSON æ–‡ä»¶")

    args = parser.parse_args()

    if args.command == "from-cif":
        from_cif_files(args.cif_dir, args.target_file, args.output)
    elif args.command == "from-poscar":
        from_poscar_files(args.poscar_dir, args.target_file, args.output)
    elif args.command == "example":
        create_example_dataset(args.output, args.num_samples)
    elif args.command == "validate":
        validate_dataset(args.dataset_file)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()

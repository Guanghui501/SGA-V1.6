"""è®­ç»ƒè‡ªå®šä¹‰æ•°æ®é›†çš„ç¤ºä¾‹è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    1. å‡†å¤‡æ•°æ®é›† JSON æ–‡ä»¶ (my_dataset.json)
    2. ä¿®æ”¹é…ç½®æ–‡ä»¶ (config_custom_dataset.json)
    3. è¿è¡Œ: python train_custom_data.py
"""

import json
import os
import argparse
from config import TrainingConfig
from train import train_dgl
from data import get_train_val_loaders


def load_dataset(dataset_file):
    """åŠ è½½æ•°æ®é›† JSON æ–‡ä»¶"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {dataset_file}")

    if not os.path.exists(dataset_file):
        raise FileNotFoundError(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")

    with open(dataset_file, "r") as f:
        dataset = json.load(f)

    print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset)} ä¸ªæ ·æœ¬")
    return dataset


def validate_dataset(dataset, target_key):
    """éªŒè¯æ•°æ®é›†æ ¼å¼"""
    print("\nğŸ” éªŒè¯æ•°æ®é›†æ ¼å¼...")

    required_keys = ["jid", "atoms"]
    errors = []

    for i, data in enumerate(dataset):
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for key in required_keys:
            if key not in data:
                errors.append(f"æ ·æœ¬ {i}: ç¼ºå°‘å­—æ®µ '{key}'")

        # æ£€æŸ¥ atoms ç»“æ„
        if "atoms" in data:
            atoms = data["atoms"]
            required_atom_keys = ["lattice_mat", "coords", "elements"]
            for key in required_atom_keys:
                if key not in atoms:
                    errors.append(f"æ ·æœ¬ {i}: atoms ç¼ºå°‘å­—æ®µ '{key}'")

        # æ£€æŸ¥ç›®æ ‡å€¼
        if target_key not in data:
            errors.append(f"æ ·æœ¬ {i}: ç¼ºå°‘ç›®æ ‡å­—æ®µ '{target_key}'")
        elif data[target_key] is None:
            errors.append(f"æ ·æœ¬ {i}: ç›®æ ‡å€¼ä¸º None")

    if errors:
        print("âŒ å‘ç°ä»¥ä¸‹é—®é¢˜:")
        for error in errors[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
            print(f"   - {error}")
        if len(errors) > 10:
            print(f"   ... è¿˜æœ‰ {len(errors) - 10} ä¸ªé”™è¯¯")
        raise ValueError("æ•°æ®é›†éªŒè¯å¤±è´¥")

    print("âœ… æ•°æ®é›†æ ¼å¼æ­£ç¡®")


def print_dataset_stats(dataset, target_key):
    """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    import numpy as np

    print("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(dataset)}")

    # ç›®æ ‡å€¼ç»Ÿè®¡
    targets = [d[target_key] for d in dataset if d.get(target_key) is not None]
    if targets:
        if isinstance(targets[0], list):
            print(f"   ç›®æ ‡ç±»å‹: å¤šè¾“å‡º (ç»´åº¦={len(targets[0])})")
        else:
            print(f"   ç›®æ ‡èŒƒå›´: [{min(targets):.4f}, {max(targets):.4f}]")
            print(f"   ç›®æ ‡å‡å€¼: {np.mean(targets):.4f}")
            print(f"   ç›®æ ‡æ ‡å‡†å·®: {np.std(targets):.4f}")

    # åŸå­æ•°ç»Ÿè®¡
    num_atoms = [len(d["atoms"]["elements"]) for d in dataset]
    print(f"   åŸå­æ•°èŒƒå›´: [{min(num_atoms)}, {max(num_atoms)}]")
    print(f"   å¹³å‡åŸå­æ•°: {np.mean(num_atoms):.1f}")

    # æ–‡æœ¬æè¿°ç»Ÿè®¡
    has_text = sum(1 for d in dataset if d.get("text_description"))
    print(f"   åŒ…å«æ–‡æœ¬æè¿°: {has_text}/{len(dataset)} ({has_text/len(dataset)*100:.1f}%)")


def main():
    parser = argparse.ArgumentParser(description="è®­ç»ƒè‡ªå®šä¹‰æ•°æ®é›†")
    parser.add_argument("--dataset", type=str, default="my_dataset.json",
                       help="æ•°æ®é›† JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--config", type=str, default="config_custom_dataset.json",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--validate-only", action="store_true",
                       help="ä»…éªŒè¯æ•°æ®é›†ï¼Œä¸è®­ç»ƒ")
    args = parser.parse_args()

    # 1. åŠ è½½æ•°æ®é›†
    dataset = load_dataset(args.dataset)

    # 2. åŠ è½½é…ç½®
    print(f"\nğŸ“‹ åŠ è½½é…ç½®: {args.config}")
    if not os.path.exists(args.config):
        raise FileNotFoundError(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")

    with open(args.config, "r") as f:
        config_dict = json.load(f)

    # 3. éªŒè¯æ•°æ®é›†
    target_key = config_dict.get("target", "formation_energy_peratom")
    validate_dataset(dataset, target_key)
    print_dataset_stats(dataset, target_key)

    if args.validate_only:
        print("\nâœ… éªŒè¯å®Œæˆï¼Œé€€å‡º")
        return

    # 4. åˆ›å»ºè®­ç»ƒé…ç½®
    print("\nâš™ï¸  åˆ›å»ºè®­ç»ƒé…ç½®...")
    config = TrainingConfig(**config_dict)

    # 5. å‡†å¤‡æ•°æ®åŠ è½½å™¨
    print("\nğŸ”„ å‡†å¤‡æ•°æ®åŠ è½½å™¨...")

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ DenseGNNï¼ˆä¸éœ€è¦çº¿å›¾ï¼‰
    use_line_graph = config.model.name not in ["densegnn"]

    train_loader, val_loader, test_loader, prepare_batch = get_train_val_loaders(
        dataset="user_data",
        dataset_array=dataset,
        target=config.target,
        atom_features=config.atom_features,
        id_tag=config.id_tag,
        batch_size=config.batch_size,
        split_seed=config.random_seed,
        train_ratio=config.train_ratio,
        val_ratio=config.val_ratio,
        test_ratio=config.test_ratio,
        cutoff=config.cutoff,
        max_neighbors=config.max_neighbors,
        line_graph=use_line_graph,
        workers=config.num_workers,
        output_dir=config.output_dir,
        use_canonize=config.use_canonize,
        save_dataloader=config.save_dataloader,
        keep_data_order=config.keep_data_order
    )

    print(f"\nâœ… æ•°æ®åŠ è½½å™¨å‡†å¤‡å®Œæˆ")
    print(f"   è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(test_loader.dataset)} æ ·æœ¬")

    # 6. å¼€å§‹è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {config.model.name.upper()} æ¨¡å‹...")
    print(f"   æ¨¡å‹: {config.model.name}")
    print(f"   è¾“å‡ºç›®å½•: {config.output_dir}")
    print(f"   è®­ç»ƒè½®æ•°: {config.epochs}")
    print(f"   æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"   å­¦ä¹ ç‡: {config.learning_rate}")
    print("=" * 80)

    history = train_dgl(
        config=config,
        train_val_test_loaders=[train_loader, val_loader, test_loader, prepare_batch]
    )

    # 7. è®­ç»ƒå®Œæˆ
    print("\n" + "=" * 80)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)
    print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {config.output_dir}")
    print(f"   - best_val_model.pt: æœ€ä½³éªŒè¯é›†æ¨¡å‹")
    print(f"   - best_test_model.pt: æœ€ä½³æµ‹è¯•é›†æ¨¡å‹")
    print(f"   - config.json: è®­ç»ƒé…ç½®")
    print(f"   - history_*.json: è®­ç»ƒå†å²")
    print(f"   - predictions_*.csv: é¢„æµ‹ç»“æœ")

    # æ‰“å°æœ€ç»ˆç»“æœ
    if history and "validation" in history:
        final_val_mae = history["validation"]["mae"][-1]
        final_test_mae = history["test"]["mae"][-1]
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   éªŒè¯é›† MAE: {final_val_mae:.4f}")
        print(f"   æµ‹è¯•é›† MAE: {final_test_mae:.4f}")


if __name__ == "__main__":
    main()

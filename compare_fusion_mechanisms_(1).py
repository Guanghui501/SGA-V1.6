#!/usr/bin/env python
"""
å¯¹æ¯”ä¸åŒèåˆæœºåˆ¶çš„æ•ˆæœ
é€šè¿‡æ¶ˆèå®éªŒç›´è§‚å±•ç¤ºå„ä¸ªæ¨¡å—çš„ä½œç”¨
ç‰ˆæœ¬2: ä½¿ç”¨return_intermediate_featureså‚æ•°ï¼Œé¿å…åŠ¨æ€ä¿®æ”¹æ¨¡å‹æ¶æ„
"""

import os
import sys
import argparse
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, davies_bouldin_score, r2_score, mean_absolute_error, mean_squared_error
from sklearn.metrics.pairwise import cosine_similarity
from scipy.stats import pearsonr, spearmanr
import warnings
warnings.filterwarnings('ignore')

from models.alignn import ALIGNN, ALIGNNConfig
from data import get_train_val_loaders

sns.set_style("whitegrid")
plt.rcParams['font.size'] = 10


def centered_kernel_alignment(X, Y):
    """
    è®¡ç®— CKA (Centered Kernel Alignment) ç›¸ä¼¼åº¦

    Args:
        X: ç‰¹å¾çŸ©é˜µ1 [N, D1]
        Y: ç‰¹å¾çŸ©é˜µ2 [N, D2]

    Returns:
        CKA score (0-1ä¹‹é—´ï¼Œè¶Šé«˜è¶Šç›¸ä¼¼)
    """
    X = X - X.mean(axis=0)
    Y = Y - Y.mean(axis=0)
    K = X @ X.T
    L = Y @ Y.T
    hsic = np.sum(K * L)
    denom = np.sqrt(np.sum(K * K) * np.sum(L * L))
    return hsic / denom if denom > 0 else 0.0


class FusionComparator:
    """èåˆæœºåˆ¶å¯¹æ¯”å™¨"""

    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.model.eval()

    def extract_features_ablation(self, data_loader, max_samples=None):
        """
        æå–ä¸åŒé˜¶æ®µçš„ç‰¹å¾ï¼ˆæ¶ˆèå®éªŒï¼‰

        Returns:
            features_dict: {
                'graph_base': å›¾åŸºç¡€ç‰¹å¾ï¼ˆæŠ•å½±åï¼Œèåˆå‰ï¼‰,
                'text_base': æ–‡æœ¬åŸºç¡€ç‰¹å¾ï¼ˆæŠ•å½±åï¼Œèåˆå‰ï¼‰,
                'graph_cross': åº”ç”¨å…¨å±€æ³¨æ„åŠ›åçš„å›¾ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰,
                'text_cross': åº”ç”¨å…¨å±€æ³¨æ„åŠ›åçš„æ–‡æœ¬ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰,
                'graph_final': æœ€ç»ˆå›¾ç‰¹å¾,
                'text_final': æœ€ç»ˆæ–‡æœ¬ç‰¹å¾,
                'fused': æœ€ç»ˆèåˆç‰¹å¾
            }
            targets: ç›®æ ‡å€¼
            ids: æ ·æœ¬ID
        """
        print("ğŸ”„ æå–ä¸åŒé˜¶æ®µçš„ç‰¹å¾ï¼ˆæ¶ˆèå®éªŒï¼‰...")

        # æ£€æŸ¥æ¨¡å‹é…ç½®
        has_middle = self.model.use_middle_fusion
        has_fine = self.model.use_fine_grained_attention
        has_cross = self.model.use_cross_modal_attention

        print(f"   æ¨¡å‹é…ç½®: ä¸­é—´èåˆ={has_middle}, ç»†ç²’åº¦æ³¨æ„åŠ›={has_fine}, å…¨å±€æ³¨æ„åŠ›={has_cross}")

        features_dict = {
            'graph_base': [],
            'text_base': [],
            'graph_middle': [],
            'graph_fine': [],
            'text_fine': [],
            'graph_cross': [],
            'text_cross': [],
            'graph_final': [],
            'text_final': [],
            'fused': []
        }
        targets = []
        ids = []

        sample_count = 0

        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(data_loader, desc="æå–ç‰¹å¾")):
                if len(batch) == 3:
                    g, text, target = batch
                    lg = None
                elif len(batch) == 4:
                    g, lg, text, target = batch
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„batchæ ¼å¼: {len(batch)}ä¸ªå…ƒç´ ")

                g = g.to(self.device)
                if lg is not None:
                    lg = lg.to(self.device)

                # å¤„ç†text
                if isinstance(text, dict):
                    text = {k: v.to(self.device) for k, v in text.items()}
                elif isinstance(text, (list, tuple)):
                    # textæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¿æŒä¸åŠ¨
                    pass
                elif torch.is_tensor(text):
                    text = text.to(self.device)

                batch_size = target.size(0)

                # æ„å»ºæ¨¡å‹è¾“å…¥
                if lg is not None:
                    model_input = (g, lg, text)
                else:
                    model_input = (g, text)

                # æå–ä¸­é—´ç‰¹å¾ï¼ˆä½¿ç”¨æ–°çš„return_intermediate_featureså‚æ•°ï¼‰
                output = self.model(model_input, return_intermediate_features=True)

                # åŸºç¡€ç‰¹å¾ï¼ˆèåˆå‰ï¼‰
                features_dict['graph_base'].append(output['graph_base'].cpu().numpy())
                features_dict['text_base'].append(output['text_base'].cpu().numpy())

                # ä¸­é—´èåˆåçš„ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if has_middle and 'graph_middle' in output:
                    features_dict['graph_middle'].append(output['graph_middle'].cpu().numpy())

                # ç»†ç²’åº¦æ³¨æ„åŠ›åçš„ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if has_fine and 'graph_fine' in output:
                    features_dict['graph_fine'].append(output['graph_fine'].cpu().numpy())
                    features_dict['text_fine'].append(output['text_fine'].cpu().numpy())

                # å…¨å±€æ³¨æ„åŠ›åçš„ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if has_cross and 'graph_cross' in output:
                    features_dict['graph_cross'].append(output['graph_cross'].cpu().numpy())
                    features_dict['text_cross'].append(output['text_cross'].cpu().numpy())

                # æœ€ç»ˆç‰¹å¾
                features_dict['graph_final'].append(output['graph_features'].cpu().numpy())
                features_dict['text_final'].append(output['text_features'].cpu().numpy())

                # èåˆç‰¹å¾
                fused = np.concatenate([
                    output['graph_features'].cpu().numpy(),
                    output['text_features'].cpu().numpy()
                ], axis=1)
                features_dict['fused'].append(fused)

                targets.append(target.cpu().numpy())

                # è®°å½•æ ·æœ¬IDï¼ˆå¦‚æœæœ‰ï¼‰
                if hasattr(g, 'ndata') and 'jid' in g.ndata:
                    batch_ids = [g.ndata['jid'][i] for i in range(g.batch_size)]
                    ids.extend(batch_ids)

                sample_count += batch_size
                if max_samples and sample_count >= max_samples:
                    break

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        for key in features_dict:
            if len(features_dict[key]) > 0:
                features_dict[key] = np.concatenate(features_dict[key], axis=0)
            else:
                features_dict[key] = None

        targets = np.concatenate(targets, axis=0)

        # ç§»é™¤ç©ºç‰¹å¾
        features_dict = {k: v for k, v in features_dict.items() if v is not None}

        print(f"âœ… æå–å®Œæˆ! æ ·æœ¬æ•°: {len(targets)}, ç‰¹å¾ç±»å‹: {list(features_dict.keys())}")

        return features_dict, targets, ids

    def visualize_tsne(self, features_dict, targets, save_dir):
        """ä½¿ç”¨t-SNEå¯è§†åŒ–ä¸åŒé˜¶æ®µçš„ç‰¹å¾"""
        print("\nğŸ“Š ç”Ÿæˆt-SNEå¯è§†åŒ–...")

        # ç¡®å®šè¦å¯è§†åŒ–çš„ç‰¹å¾
        feature_names = []
        feature_data = []

        if 'graph_base' in features_dict:
            feature_names.append('Graph Base')
            feature_data.append(features_dict['graph_base'])

        if 'text_base' in features_dict:
            feature_names.append('Text Base')
            feature_data.append(features_dict['text_base'])

        if 'graph_middle' in features_dict:
            feature_names.append('Graph + Middle Fusion')
            feature_data.append(features_dict['graph_middle'])

        if 'graph_fine' in features_dict:
            feature_names.append('Graph + Fine-grained Attn')
            feature_data.append(features_dict['graph_fine'])

        if 'text_fine' in features_dict:
            feature_names.append('Text + Fine-grained Attn')
            feature_data.append(features_dict['text_fine'])

        if 'graph_cross' in features_dict:
            feature_names.append('Graph + Cross-Modal')
            feature_data.append(features_dict['graph_cross'])

        if 'text_cross' in features_dict:
            feature_names.append('Text + Cross-Modal')
            feature_data.append(features_dict['text_cross'])

        if 'graph_final' in features_dict:
            feature_names.append('Graph Final')
            feature_data.append(features_dict['graph_final'])

        if 'text_final' in features_dict:
            feature_names.append('Text Final')
            feature_data.append(features_dict['text_final'])

        if 'fused' in features_dict:
            feature_names.append('Fused')
            feature_data.append(features_dict['fused'])

        n_features = len(feature_names)
        if n_features == 0:
            print("âš ï¸  æ²¡æœ‰å¯è§†åŒ–çš„ç‰¹å¾!")
            return

        # å…ˆå¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œt-SNEï¼Œæ”¶é›†æ‰€æœ‰åæ ‡ç”¨äºç»Ÿä¸€åæ ‡è½´èŒƒå›´
        print("   ç¬¬ä¸€æ­¥: è®¡ç®—æ‰€æœ‰t-SNEåµŒå…¥...")
        all_features_2d = []
        for name, features in zip(feature_names, feature_data):
            tsne = TSNE(n_components=2, random_state=42, perplexity=30)
            features_2d = tsne.fit_transform(features)
            all_features_2d.append(features_2d)

        # è®¡ç®—å…¨å±€åæ ‡èŒƒå›´
        all_coords = np.vstack(all_features_2d)
        x_min, x_max = all_coords[:, 0].min(), all_coords[:, 0].max()
        y_min, y_max = all_coords[:, 1].min(), all_coords[:, 1].max()

        # æ·»åŠ è¾¹è·
        x_margin = (x_max - x_min) * 0.05
        y_margin = (y_max - y_min) * 0.05
        x_lim = [x_min - x_margin, x_max + x_margin]
        y_lim = [y_min - y_margin, y_max + y_margin]

        print(f"   å…¨å±€åæ ‡èŒƒå›´: x=[{x_lim[0]:.1f}, {x_lim[1]:.1f}], y=[{y_lim[0]:.1f}, {y_lim[1]:.1f}]")

        # åˆ›å»ºç½‘æ ¼å¸ƒå±€
        n_cols = min(3, n_features)
        n_rows = (n_features + n_cols - 1) // n_cols
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))
        if n_features == 1:
            axes = [axes]
        else:
            axes = axes.flatten() if n_features > 1 else [axes]

        # ç¬¬äºŒæ­¥: ç»˜åˆ¶æ¯ä¸ªç‰¹å¾
        print("   ç¬¬äºŒæ­¥: ç»˜åˆ¶å¯è§†åŒ–...")
        for idx, (name, features_2d) in enumerate(zip(feature_names, all_features_2d)):
            ax = axes[idx]
            scatter = ax.scatter(features_2d[:, 0], features_2d[:, 1],
                                c=targets, cmap='viridis', alpha=0.6, s=20)
            ax.set_title(name, fontsize=12, fontweight='bold')
            ax.set_xlabel('t-SNE 1')
            ax.set_ylabel('t-SNE 2')

            # è®¾ç½®ç»Ÿä¸€çš„åæ ‡èŒƒå›´
            ax.set_xlim(x_lim)
            ax.set_ylim(y_lim)

            # è®¾ç½®ç™½è‰²ç½‘æ ¼çº¿
            ax.grid(True, color='white', linewidth=0.8, alpha=0.7)

            plt.colorbar(scatter, ax=ax, label='Target Value')

        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(n_features, len(axes)):
            axes[idx].axis('off')

        plt.tight_layout()
        save_path = os.path.join(save_dir, 'tsne_comparison.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… t-SNEå¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        plt.close()

    def compute_metrics(self, features_dict, targets, save_dir):
        """è®¡ç®—ä¸åŒç‰¹å¾çš„è´¨é‡æŒ‡æ ‡"""
        print("\nğŸ“ˆ è®¡ç®—ç‰¹å¾è´¨é‡æŒ‡æ ‡...")

        metrics_list = []

        for name, features in features_dict.items():
            if features is None or len(features) == 0:
                continue

            print(f"   åˆ†æ {name}...")

            # Silhouette Score (è½®å»“ç³»æ•°, è¶Šå¤§è¶Šå¥½)
            try:
                sil_score = silhouette_score(features, targets)
            except:
                sil_score = np.nan

            # Davies-Bouldin Index (è¶Šå°è¶Šå¥½)
            try:
                db_score = davies_bouldin_score(features, targets)
            except:
                db_score = np.nan

            # Intra-class similarity (ç±»å†…ç›¸ä¼¼åº¦, è¶Šå¤§è¶Šå¥½)
            intra_sim = self._compute_intra_class_similarity(features, targets)

            # Inter-class separation (ç±»é—´åˆ†ç¦»åº¦, è¶Šå¤§è¶Šå¥½)
            inter_sep = self._compute_inter_class_separation(features, targets)

            metrics_list.append({
                'Feature': name,
                'Silhouette Score': sil_score,
                'Davies-Bouldin Index': db_score,
                'Intra-class Similarity': intra_sim,
                'Inter-class Separation': inter_sep
            })

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(metrics_list)
        save_path = os.path.join(save_dir, 'feature_metrics.csv')
        df.to_csv(save_path, index=False)
        print(f"\nâœ… æŒ‡æ ‡å·²ä¿å­˜: {save_path}")
        print("\n" + df.to_string(index=False))

        # å¯è§†åŒ–æŒ‡æ ‡
        self._plot_metrics(df, save_dir)

        return df

    def _compute_intra_class_similarity(self, features, targets):
        """è®¡ç®—ç±»å†…ç›¸ä¼¼åº¦"""
        unique_targets = np.unique(targets)
        if len(unique_targets) < 2:
            return 1.0

        sims = []
        for target in unique_targets[:10]:  # åªå–å‰10ä¸ªç±»åˆ«é¿å…è®¡ç®—è¿‡æ…¢
            mask = targets == target
            if np.sum(mask) < 2:
                continue
            class_features = features[mask]
            sim_matrix = cosine_similarity(class_features)
            # å–ä¸Šä¸‰è§’ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
            upper_tri = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]
            sims.append(np.mean(upper_tri))

        return np.mean(sims) if len(sims) > 0 else 0.0

    def _compute_inter_class_separation(self, features, targets):
        """è®¡ç®—ç±»é—´åˆ†ç¦»åº¦"""
        unique_targets = np.unique(targets)
        if len(unique_targets) < 2:
            return 0.0

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ä¸­å¿ƒ
        centroids = []
        for target in unique_targets[:10]:  # åªå–å‰10ä¸ªç±»åˆ«
            mask = targets == target
            if np.sum(mask) == 0:
                continue
            centroids.append(np.mean(features[mask], axis=0))

        if len(centroids) < 2:
            return 0.0

        centroids = np.array(centroids)
        # è®¡ç®—ä¸­å¿ƒä¹‹é—´çš„å¹³å‡è·ç¦»
        distances = []
        for i in range(len(centroids)):
            for j in range(i+1, len(centroids)):
                dist = np.linalg.norm(centroids[i] - centroids[j])
                distances.append(dist)

        return np.mean(distances)

    def _plot_metrics(self, df, save_dir):
        """å¯è§†åŒ–æŒ‡æ ‡å¯¹æ¯”"""
        metrics = ['Silhouette Score', 'Davies-Bouldin Index',
                   'Intra-class Similarity', 'Inter-class Separation']

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()

        for idx, metric in enumerate(metrics):
            ax = axes[idx]
            data = df[['Feature', metric]].dropna()

            if len(data) == 0:
                continue

            x = range(len(data))
            y = data[metric].values
            labels = data['Feature'].values

            bars = ax.bar(x, y, alpha=0.7, color=sns.color_palette("husl", len(data)))
            ax.set_xticks(x)
            ax.set_xticklabels(labels, rotation=45, ha='right')
            ax.set_ylabel(metric)
            ax.set_title(f'{metric} Comparison', fontweight='bold')
            ax.grid(axis='y', alpha=0.3)

            # æ ‡æ³¨æ•°å€¼
            for i, v in enumerate(y):
                ax.text(i, v + 0.01*max(y), f'{v:.3f}', ha='center', va='bottom', fontsize=9)

        plt.tight_layout()
        save_path = os.path.join(save_dir, 'metrics_comparison.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
        plt.close()

    def compute_regression_metrics(self, features_dict, targets, save_dir):
        """è®¡ç®—å›å½’ä»»åŠ¡çš„ç‰¹å¾è´¨é‡æŒ‡æ ‡"""
        print("\nğŸ“Š è®¡ç®—å›å½’ä»»åŠ¡æŒ‡æ ‡...")

        metrics_list = []

        for name, features in features_dict.items():
            if features is None or len(features) == 0:
                continue

            print(f"   åˆ†æ {name}...")

            # 1. ç‰¹å¾-ç›®æ ‡ç›¸å…³æ€§ (Pearson)
            feature_dim = features.shape[1]
            pearson_correlations = []
            for i in range(feature_dim):
                try:
                    corr, _ = pearsonr(features[:, i], targets)
                    if not np.isnan(corr):
                        pearson_correlations.append(abs(corr))
                except:
                    pass

            avg_pearson = np.mean(pearson_correlations) if len(pearson_correlations) > 0 else 0.0
            max_pearson = np.max(pearson_correlations) if len(pearson_correlations) > 0 else 0.0

            # 2. ç‰¹å¾-ç›®æ ‡ç›¸å…³æ€§ (Spearman)
            spearman_correlations = []
            for i in range(feature_dim):
                try:
                    corr, _ = spearmanr(features[:, i], targets)
                    if not np.isnan(corr):
                        spearman_correlations.append(abs(corr))
                except:
                    pass

            avg_spearman = np.mean(spearman_correlations) if len(spearman_correlations) > 0 else 0.0

            # 3. ç‰¹å¾æ–¹å·® (è¡¨ç¤ºç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›)
            feature_variance = np.mean(np.var(features, axis=0))

            # 4. ç‰¹å¾æ ‡å‡†å·®
            feature_std = np.mean(np.std(features, axis=0))

            # 5. ç‰¹å¾èŒƒæ•°
            feature_norm = np.mean(np.linalg.norm(features, axis=1))

            metrics_list.append({
                'Feature': name,
                'Avg Pearson Corr': avg_pearson,
                'Max Pearson Corr': max_pearson,
                'Avg Spearman Corr': avg_spearman,
                'Feature Variance': feature_variance,
                'Feature Std': feature_std,
                'Feature Norm': feature_norm
            })

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(metrics_list)
        save_path = os.path.join(save_dir, 'regression_metrics.csv')
        df.to_csv(save_path, index=False)
        print(f"\nâœ… å›å½’æŒ‡æ ‡å·²ä¿å­˜: {save_path}")
        print("\n" + df.to_string(index=False))

        # å¯è§†åŒ–å›å½’æŒ‡æ ‡
        self._plot_regression_metrics(df, save_dir)

        return df

    def _plot_regression_metrics(self, df, save_dir):
        """å¯è§†åŒ–å›å½’æŒ‡æ ‡å¯¹æ¯”"""
        metrics = ['Avg Pearson Corr', 'Max Pearson Corr',
                   'Avg Spearman Corr', 'Feature Variance']

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()

        for idx, metric in enumerate(metrics):
            ax = axes[idx]
            data = df[['Feature', metric]].dropna()

            if len(data) == 0:
                continue

            x = range(len(data))
            y = data[metric].values
            labels = data['Feature'].values

            # ä½¿ç”¨é¢œè‰²åŒºåˆ†æ€§èƒ½
            colors = sns.color_palette("RdYlGn", len(data))
            if metric in ['Avg Pearson Corr', 'Max Pearson Corr', 'Avg Spearman Corr']:
                # ç›¸å…³æ€§è¶Šé«˜è¶Šå¥½ï¼Œæ’åºåä¸Šè‰²
                sorted_indices = np.argsort(y)
                bar_colors = [colors[np.where(sorted_indices == i)[0][0]] for i in range(len(y))]
            else:
                bar_colors = sns.color_palette("husl", len(data))

            bars = ax.bar(x, y, alpha=0.7, color=bar_colors)
            ax.set_xticks(x)
            ax.set_xticklabels(labels, rotation=45, ha='right')
            ax.set_ylabel(metric)
            ax.set_title(f'{metric} Comparison', fontweight='bold')
            ax.grid(axis='y', alpha=0.3, color='white', linewidth=0.8)

            # æ ‡æ³¨æ•°å€¼
            for i, v in enumerate(y):
                ax.text(i, v + 0.01*max(abs(y)), f'{v:.4f}',
                       ha='center', va='bottom', fontsize=9)

            # æ·»åŠ å‚è€ƒçº¿
            if metric in ['Avg Pearson Corr', 'Max Pearson Corr', 'Avg Spearman Corr']:
                ax.axhline(y=0.3, color='orange', linestyle='--',
                          linewidth=1, alpha=0.5, label='Moderate (0.3)')
                ax.axhline(y=0.5, color='red', linestyle='--',
                          linewidth=1, alpha=0.5, label='Strong (0.5)')
                ax.legend(fontsize=8)

        plt.tight_layout()
        save_path = os.path.join(save_dir, 'regression_metrics_comparison.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… å›å½’æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
        plt.close()

    def compute_cka_matrix(self, features_dict, save_dir):
        """
        è®¡ç®—æ‰€æœ‰ç‰¹å¾å¯¹ä¹‹é—´çš„ CKA ç›¸ä¼¼åº¦çŸ©é˜µ

        Args:
            features_dict: ç‰¹å¾å­—å…¸
            save_dir: ä¿å­˜ç›®å½•

        Returns:
            CKA çŸ©é˜µ DataFrame
        """
        print("\nğŸ” è®¡ç®— CKA ç›¸ä¼¼åº¦çŸ©é˜µ...")

        # è·å–æ‰€æœ‰æœ‰æ•ˆç‰¹å¾å
        feature_names = [name for name, feats in features_dict.items()
                        if feats is not None and len(feats) > 0]

        if len(feature_names) < 2:
            print("âš ï¸  ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œæ— æ³•è®¡ç®— CKA çŸ©é˜µ")
            return None

        # åˆå§‹åŒ– CKA çŸ©é˜µ
        n_features = len(feature_names)
        cka_matrix = np.zeros((n_features, n_features))

        # è®¡ç®—æ‰€æœ‰ç‰¹å¾å¯¹çš„ CKA
        for i, name_i in enumerate(feature_names):
            for j, name_j in enumerate(feature_names):
                if i == j:
                    cka_matrix[i, j] = 1.0
                elif i < j:
                    print(f"   è®¡ç®— CKA: {name_i} vs {name_j}")
                    cka_score = centered_kernel_alignment(
                        features_dict[name_i],
                        features_dict[name_j]
                    )
                    cka_matrix[i, j] = cka_score
                    cka_matrix[j, i] = cka_score  # å¯¹ç§°çŸ©é˜µ

        # åˆ›å»º DataFrame
        cka_df = pd.DataFrame(cka_matrix,
                             index=feature_names,
                             columns=feature_names)

        # ä¿å­˜ä¸º CSV
        save_path = os.path.join(save_dir, 'cka_similarity_matrix.csv')
        cka_df.to_csv(save_path)
        print(f"\nâœ… CKA çŸ©é˜µå·²ä¿å­˜: {save_path}")
        print("\n" + cka_df.to_string())

        return cka_df

    def visualize_cka_matrix(self, cka_df, save_dir):
        """
        å¯è§†åŒ– CKA ç›¸ä¼¼åº¦çŸ©é˜µ

        Args:
            cka_df: CKA çŸ©é˜µ DataFrame
            save_dir: ä¿å­˜ç›®å½•
        """
        print("\nğŸ“Š ç”Ÿæˆ CKA ç›¸ä¼¼åº¦çƒ­å›¾...")

        if cka_df is None or len(cka_df) == 0:
            print("âš ï¸  æ²¡æœ‰å¯è§†åŒ–çš„ CKA æ•°æ®")
            return

        # åˆ›å»ºå›¾å½¢
        fig, ax = plt.subplots(figsize=(10, 8))

        # ç»˜åˆ¶çƒ­å›¾
        sns.heatmap(cka_df,
                   annot=True,  # æ˜¾ç¤ºæ•°å€¼
                   fmt='.3f',   # ä¿ç•™3ä½å°æ•°
                   cmap='RdYlGn',  # çº¢é»„ç»¿é…è‰²
                   vmin=0.0,
                   vmax=1.0,
                   center=0.5,
                   square=True,
                   linewidths=0.5,
                   cbar_kws={'label': 'CKA Similarity'},
                   ax=ax)

        ax.set_title('CKA Similarity Matrix Between Different Fusion Stages',
                    fontsize=14, fontweight='bold', pad=15)
        ax.set_xlabel('Features', fontsize=12, fontweight='bold')
        ax.set_ylabel('Features', fontsize=12, fontweight='bold')

        # æ—‹è½¬æ ‡ç­¾
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)

        plt.tight_layout()
        save_path = os.path.join(save_dir, 'cka_similarity_heatmap.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… CKA çƒ­å›¾å·²ä¿å­˜: {save_path}")
        plt.close()

        # ç”Ÿæˆ CKA åˆ†æ•°æ‘˜è¦
        self._generate_cka_summary(cka_df, save_dir)

    def _generate_cka_summary(self, cka_df, save_dir):
        """
        ç”Ÿæˆ CKA åˆ†æ•°æ‘˜è¦æŠ¥å‘Š

        Args:
            cka_df: CKA çŸ©é˜µ DataFrame
            save_dir: ä¿å­˜ç›®å½•
        """
        print("\nğŸ“ ç”Ÿæˆ CKA åˆ†æ•°æ‘˜è¦...")

        report_lines = []
        report_lines.append("=" * 70)
        report_lines.append("CKA Similarity Score Summary Report")
        report_lines.append("=" * 70)
        report_lines.append("")

        # 1. æ•´ä½“ç»Ÿè®¡
        # æå–ä¸Šä¸‰è§’ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
        n = len(cka_df)
        upper_tri_indices = np.triu_indices(n, k=1)
        upper_tri_values = cka_df.values[upper_tri_indices]

        report_lines.append("ğŸ“Š Overall Statistics:")
        report_lines.append(f"  â€¢ Mean CKA Score: {np.mean(upper_tri_values):.4f}")
        report_lines.append(f"  â€¢ Median CKA Score: {np.median(upper_tri_values):.4f}")
        report_lines.append(f"  â€¢ Min CKA Score: {np.min(upper_tri_values):.4f}")
        report_lines.append(f"  â€¢ Max CKA Score: {np.max(upper_tri_values):.4f}")
        report_lines.append(f"  â€¢ Std CKA Score: {np.std(upper_tri_values):.4f}")
        report_lines.append("")

        # 2. æœ€ç›¸ä¼¼çš„ç‰¹å¾å¯¹ï¼ˆTop 5ï¼‰
        report_lines.append("ğŸ” Top 5 Most Similar Feature Pairs:")
        similar_pairs = []
        for i in range(n):
            for j in range(i+1, n):
                similar_pairs.append((
                    cka_df.index[i],
                    cka_df.columns[j],
                    cka_df.iloc[i, j]
                ))
        similar_pairs.sort(key=lambda x: x[2], reverse=True)

        for rank, (feat1, feat2, score) in enumerate(similar_pairs[:5], 1):
            report_lines.append(f"  {rank}. {feat1} â†” {feat2}: {score:.4f}")
        report_lines.append("")

        # 3. æœ€ä¸ç›¸ä¼¼çš„ç‰¹å¾å¯¹ï¼ˆTop 5ï¼‰
        report_lines.append("ğŸ”» Top 5 Most Dissimilar Feature Pairs:")
        for rank, (feat1, feat2, score) in enumerate(similar_pairs[-5:][::-1], 1):
            report_lines.append(f"  {rank}. {feat1} â†” {feat2}: {score:.4f}")
        report_lines.append("")

        # 4. èåˆé˜¶æ®µçš„å½±å“åˆ†æ
        report_lines.append("ğŸ”¬ Fusion Stage Impact Analysis:")

        # æ£€æŸ¥ç‰¹å®šçš„èåˆé˜¶æ®µå¯¹
        stage_pairs = [
            ('graph_base', 'graph_middle', 'ä¸­æœŸèåˆçš„å½±å“'),
            ('graph_middle', 'graph_fine', 'ç»†ç²’åº¦æ³¨æ„åŠ›çš„å½±å“'),
            ('graph_fine', 'graph_cross', 'å…¨å±€æ³¨æ„åŠ›çš„å½±å“'),
            ('graph_cross', 'graph_final', 'æœ€ç»ˆèåˆçš„å½±å“'),
            ('graph_base', 'graph_final', 'æ•´ä½“èåˆæ•ˆæœ'),
            ('text_base', 'text_final', 'æ–‡æœ¬æ¨¡æ€çš„å˜åŒ–'),
        ]

        for feat1, feat2, description in stage_pairs:
            if feat1 in cka_df.index and feat2 in cka_df.columns:
                score = cka_df.loc[feat1, feat2]
                report_lines.append(f"  â€¢ {description}")
                report_lines.append(f"    {feat1} â†’ {feat2}: {score:.4f}")

                # è§£é‡Šåˆ†æ•°
                if score > 0.9:
                    interpretation = "æé«˜ç›¸ä¼¼åº¦ - èåˆå½±å“è¾ƒå°"
                elif score > 0.7:
                    interpretation = "é«˜ç›¸ä¼¼åº¦ - èåˆä¿ç•™äº†ä¸»è¦ä¿¡æ¯"
                elif score > 0.5:
                    interpretation = "ä¸­ç­‰ç›¸ä¼¼åº¦ - èåˆå¸¦æ¥äº†æ˜¾è‘—å˜åŒ–"
                else:
                    interpretation = "ä½ç›¸ä¼¼åº¦ - èåˆå¤§å¹…æ”¹å˜äº†ç‰¹å¾ç©ºé—´"
                report_lines.append(f"    è§£é‡Š: {interpretation}")
                report_lines.append("")

        # 5. å»ºè®®
        report_lines.append("ğŸ’¡ Insights and Recommendations:")
        avg_cka = np.mean(upper_tri_values)
        if avg_cka > 0.85:
            report_lines.append("  â€¢ ç‰¹å¾ç©ºé—´æ•´ä½“ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡åº¦èåˆ")
            report_lines.append("  â€¢ å»ºè®®: è€ƒè™‘å‡å°‘èåˆå±‚æ•°æˆ–è°ƒæ•´èåˆå¼ºåº¦")
        elif avg_cka > 0.65:
            report_lines.append("  â€¢ ç‰¹å¾ç©ºé—´ä¿æŒäº†é€‚åº¦çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§")
            report_lines.append("  â€¢ å»ºè®®: å½“å‰èåˆæœºåˆ¶è¾ƒä¸ºåˆç†")
        else:
            report_lines.append("  â€¢ ä¸åŒé˜¶æ®µçš„ç‰¹å¾å·®å¼‚è¾ƒå¤§")
            report_lines.append("  â€¢ å»ºè®®: åˆ†ææ˜¯å¦æœ‰è¿‡åº¦å˜æ¢å¯¼è‡´ä¿¡æ¯æŸå¤±")

        report_lines.append("")
        report_lines.append("=" * 70)

        # ä¿å­˜æŠ¥å‘Š
        report_text = "\n".join(report_lines)
        save_path = os.path.join(save_dir, 'cka_summary_report.txt')
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report_text)

        print(f"âœ… CKA æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {save_path}")
        print("\n" + report_text)


def main():
    parser = argparse.ArgumentParser(description='å¯¹æ¯”ä¸åŒèåˆæœºåˆ¶çš„æ•ˆæœ (v2)')
    parser.add_argument('--checkpoint', type=str, required=True, help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--dataset', type=str, required=True,
                        help='æ•°æ®é›†ç±»å‹ (jarvis/mp/classç­‰)')
    parser.add_argument('--property', type=str, required=True,
                        help='ç›®æ ‡å±æ€§ (å¦‚ formation_energy_peratom, bandgapç­‰)')
    parser.add_argument('--root_dir', type=str, default='/public/home/ghzhang/crysmmnet-main/dataset',
                        help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max_samples', type=int, default=500, help='æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰')
    parser.add_argument('--save_dir', type=str, default='./fusion_comparison',
                        help='ç»“æœä¿å­˜ç›®å½•')
    args = parser.parse_args()

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)

    # åŠ è½½æ¨¡å‹
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location='cpu', weights_only=False)

    if 'config' in checkpoint:
        config = checkpoint['config']
    else:
        raise ValueError("Checkpointä¸­æ²¡æœ‰æ‰¾åˆ°config")

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"   ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡å‹
    model = ALIGNN(config)
    model.load_state_dict(checkpoint['model'])
    model = model.to(device)
    model.eval()

    print(f"   æ¨¡å‹é…ç½®:")
    print(f"     - ä¸­é—´èåˆ: {model.use_middle_fusion}")
    print(f"     - ç»†ç²’åº¦æ³¨æ„åŠ›: {model.use_fine_grained_attention}")
    print(f"     - å…¨å±€æ³¨æ„åŠ›: {model.use_cross_modal_attention}")

    # åŠ è½½æ•°æ®é›†ï¼ˆæ”¯æŒæœ¬åœ°æ•°æ®ï¼‰
    print(f"\nğŸ”„ åŠ è½½æ•°æ®é›†: {args.dataset} - {args.property}")
    try:
        from train_with_cross_modal_attention import load_dataset, get_dataset_paths

        # è·å–æ•°æ®é›†è·¯å¾„
        cif_dir, id_prop_file = get_dataset_paths(args.root_dir, args.dataset, args.property)

        # åŠ è½½æ•°æ®é›†
        df = load_dataset(cif_dir, id_prop_file, args.dataset, args.property)
        print(f"âœ… åŠ è½½æ•°æ®é›†: {len(df)} æ ·æœ¬")

        # å¦‚æœè®¾ç½®äº†max_samplesï¼Œè¿›è¡Œé‡‡æ ·
        if args.max_samples and len(df) > args.max_samples:
            print(f"âš ï¸  æ•°æ®é›†è¿‡å¤§ï¼Œéšæœºé‡‡æ · {args.max_samples} æ ·æœ¬")
            import random
            random.seed(42)
            df = random.sample(df, args.max_samples)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨æœ¬åœ°æ•°æ®ï¼‰
        train_loader, val_loader, test_loader, _ = get_train_val_loaders(
            dataset='user_data',  # ä½¿ç”¨user_dataé¿å…dataseté™åˆ¶
            dataset_array=df,
            target='target',
            n_train=None,
            n_val=None,
            n_test=None,
            train_ratio=0.8,
            val_ratio=0.1,
            test_ratio=0.1,
            batch_size=args.batch_size,
            atom_features=config.atom_features if hasattr(config, 'atom_features') else 'cgcnn',
            neighbor_strategy='k-nearest',
            line_graph=config.line_graph if hasattr(config, 'line_graph') else True,
            split_seed=42,
            workers=0,
            pin_memory=False,
            save_dataloader=False,
            filename='temp_comparison',
            id_tag='jid',
            use_canonize=True,
            cutoff=8.0,
            max_neighbors=12,
            output_dir=args.save_dir
        )
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿:")
        print(f"  1. æ•°æ®é›†è·¯å¾„æ­£ç¡®: {args.root_dir}")
        print(f"  2. æ•°æ®é›†ç±»å‹æ­£ç¡®: {args.dataset}")
        print(f"  3. å±æ€§åç§°æ­£ç¡®: {args.property}")
        raise

    print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_loader.dataset)}")

    # åˆ›å»ºå¯¹æ¯”å™¨
    comparator = FusionComparator(model, device=device)

    # æå–ç‰¹å¾
    features_dict, targets, ids = comparator.extract_features_ablation(
        test_loader, max_samples=args.max_samples
    )

    # å¯è§†åŒ–
    comparator.visualize_tsne(features_dict, targets, args.save_dir)

    # è®¡ç®—èšç±»æŒ‡æ ‡
    metrics_df = comparator.compute_metrics(features_dict, targets, args.save_dir)

    # è®¡ç®—å›å½’æŒ‡æ ‡
    regression_metrics_df = comparator.compute_regression_metrics(features_dict, targets, args.save_dir)

    # è®¡ç®— CKA ç›¸ä¼¼åº¦çŸ©é˜µ
    cka_df = comparator.compute_cka_matrix(features_dict, args.save_dir)

    # å¯è§†åŒ– CKA çŸ©é˜µ
    if cka_df is not None:
        comparator.visualize_cka_matrix(cka_df, args.save_dir)

    print(f"\nğŸ‰ åˆ†æå®Œæˆ! ç»“æœä¿å­˜åœ¨: {args.save_dir}")


if __name__ == '__main__':
    main()

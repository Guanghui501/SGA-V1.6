#!/usr/bin/env python
"""
æœ¬åœ°æ•°æ®è®­ç»ƒè„šæœ¬ - ä½¿ç”¨CIFæ–‡ä»¶ + CSVæè¿°

æ•°æ®æ ¼å¼ï¼š
    - CIFç›®å½•: åŒ…å«æ‰€æœ‰æ™¶ä½“ç»“æ„CIFæ–‡ä»¶
    - CSVæ–‡ä»¶: åŒ…å«æ ·æœ¬IDã€ç›®æ ‡å€¼ã€æ–‡æœ¬æè¿°ç­‰ä¿¡æ¯

CSVæ ¼å¼è¦æ±‚ï¼š
    å¿…éœ€åˆ—:
    - id: æ ·æœ¬IDï¼ˆå¯¹åº”CIFæ–‡ä»¶åï¼Œå¦‚ sample_001.cif çš„IDä¸º sample_001ï¼‰
    - target: ç›®æ ‡å±æ€§å€¼ï¼ˆæµ®ç‚¹æ•°ï¼‰

    å¯é€‰åˆ—:
    - text_description: ææ–™æ–‡æœ¬æè¿°ï¼ˆç”¨äºå¤šæ¨¡æ€å­¦ä¹ ï¼‰
    - composition: åŒ–å­¦å¼ï¼ˆå¯é€‰ï¼‰

ä½¿ç”¨ç¤ºä¾‹:
    # åŸºç¡€è®­ç»ƒ
    python train_local_cif_csv.py \\
        --cif_dir ./my_structures/cif/ \\
        --csv_file ./my_structures/data.csv \\
        --output_dir ./results/

    # ä½¿ç”¨ DenseGNN + å¤šæ¨¡æ€
    python train_local_cif_csv.py \\
        --cif_dir ./structures/ \\
        --csv_file ./data.csv \\
        --model densegnn \\
        --use_middle_fusion \\
        --use_cross_modal \\
        --epochs 500

    # åˆ†ç±»ä»»åŠ¡
    python train_local_cif_csv.py \\
        --cif_dir ./cifs/ \\
        --csv_file ./labels.csv \\
        --classification \\
        --target_column label
"""

import os
import sys
import csv
import time
import json
import argparse
import numpy as np
from tqdm import tqdm

import torch
from jarvis.core.atoms import Atoms
from transformers import AutoTokenizer
from tokenizers.normalizers import BertNormalizer

from data import get_train_val_loaders
from train import train_dgl
from config import TrainingConfig
from models.alignn import ALIGNNConfig
from models.densegnn import DenseGNNConfig


# ==================== è¾…åŠ©å‡½æ•° ====================

def str2bool(v):
    """å­—ç¬¦ä¸²è½¬å¸ƒå°”å€¼"""
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('å¸ƒå°”å€¼åº”ä¸º yes/no, true/false, t/f, y/n, 1/0')


# ==================== æ–‡æœ¬å½’ä¸€åŒ– ====================

def setup_text_normalizer():
    """è®¾ç½®æ–‡æœ¬å½’ä¸€åŒ–å™¨"""
    # BERTå½’ä¸€åŒ–å™¨
    norm = BertNormalizer(lowercase=False, strip_accents=True,
                         clean_text=True, handle_chinese_chars=True)

    # åŠ è½½è¯æ±‡æ˜ å°„
    possible_paths = [
        'vocab_mappings.txt',
        './vocab_mappings.txt',
        os.path.join(os.path.dirname(__file__), 'vocab_mappings.txt'),
    ]

    vocab_file = None
    for path in possible_paths:
        if os.path.exists(path):
            vocab_file = path
            break

    if vocab_file is None:
        print("âš ï¸  æœªæ‰¾åˆ° vocab_mappings.txtï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬å½’ä¸€åŒ–")
        mappings = {}
    else:
        with open(vocab_file, 'r') as f:
            mappings_list = f.read().strip().split('\n')
        mappings = {m[0]: m[2:] for m in mappings_list}

    def normalize(text):
        """å½’ä¸€åŒ–æ–‡æœ¬"""
        text = [norm.normalize_str(s) for s in text.split('\n')]
        out = []
        for s in text:
            norm_s = ''
            for c in s:
                norm_s += mappings.get(c, ' ') if mappings else c
            out.append(norm_s)
        return '\n'.join(out)

    return normalize


# ==================== æ•°æ®åŠ è½½ ====================

def load_dataset_from_cif_csv(cif_dir, csv_file, target_column='target',
                               text_column='text_description', id_column='id'):
    """ä»CIFç›®å½•å’ŒCSVæ–‡ä»¶åŠ è½½æ•°æ®é›†

    Args:
        cif_dir: CIFæ–‡ä»¶ç›®å½•
        csv_file: CSVæè¿°æ–‡ä»¶è·¯å¾„
        target_column: CSVä¸­ç›®æ ‡å€¼åˆ—å
        text_column: CSVä¸­æ–‡æœ¬æè¿°åˆ—åï¼ˆå¯é€‰ï¼‰
        id_column: CSVä¸­IDåˆ—å

    Returns:
        dataset_array: æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å­—å…¸
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“‚ åŠ è½½æœ¬åœ°æ•°æ®é›†")
    print(f"{'='*80}")
    print(f"CIFç›®å½•: {cif_dir}")
    print(f"CSVæ–‡ä»¶: {csv_file}")
    print(f"ç›®æ ‡åˆ—: {target_column}")
    print(f"æ–‡æœ¬åˆ—: {text_column}")
    print(f"{'='*80}\n")

    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(cif_dir):
        raise FileNotFoundError(f"CIFç›®å½•ä¸å­˜åœ¨: {cif_dir}")
    if not os.path.exists(csv_file):
        raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")

    # è¯»å–CSVæ–‡ä»¶
    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    print(f"CSVæ–‡ä»¶å…± {len(rows)} è¡Œ")

    # æ£€æŸ¥å¿…éœ€åˆ—
    if len(rows) == 0:
        raise ValueError("CSVæ–‡ä»¶ä¸ºç©º")

    columns = rows[0].keys()
    if id_column not in columns:
        raise ValueError(f"CSVç¼ºå°‘IDåˆ—: {id_column}ã€‚å¯ç”¨åˆ—: {list(columns)}")
    if target_column not in columns:
        raise ValueError(f"CSVç¼ºå°‘ç›®æ ‡åˆ—: {target_column}ã€‚å¯ç”¨åˆ—: {list(columns)}")

    has_text = text_column in columns
    print(f"æ–‡æœ¬æè¿°: {'âœ“ åŒ…å«' if has_text else 'âœ— ä¸åŒ…å«'}")

    # è®¾ç½®æ–‡æœ¬å½’ä¸€åŒ–å™¨
    normalize_text = setup_text_normalizer()

    # åŠ è½½æ•°æ®
    dataset_array = []
    skipped = 0
    errors = []

    for row in tqdm(rows, desc="åŠ è½½æ ·æœ¬"):
        try:
            sample_id = row[id_column].strip()
            target_value = float(row[target_column])

            # æ–‡æœ¬æè¿°
            if has_text and row.get(text_column):
                text_desc = normalize_text(row[text_column])
            else:
                text_desc = f"Crystal structure {sample_id}"

            # CIFæ–‡ä»¶è·¯å¾„
            cif_file = os.path.join(cif_dir, f"{sample_id}.cif")
            if not os.path.exists(cif_file):
                skipped += 1
                if len(errors) < 5:
                    errors.append(f"CIFæ–‡ä»¶ä¸å­˜åœ¨: {cif_file}")
                continue

            # åŠ è½½ç»“æ„
            atoms = Atoms.from_cif(cif_file)

            # æ„å»ºæ ·æœ¬
            sample = {
                "atoms": atoms.to_dict(),
                "jid": sample_id,
                "target": target_value,
                "text_description": text_desc
            }

            dataset_array.append(sample)

        except Exception as e:
            skipped += 1
            if len(errors) < 5:
                errors.append(f"æ ·æœ¬ {sample_id}: {str(e)}")

    # æ‰“å°ç»Ÿè®¡
    print(f"\nâœ… æˆåŠŸåŠ è½½: {len(dataset_array)} æ ·æœ¬")
    if skipped > 0:
        print(f"âš ï¸  è·³è¿‡: {skipped} æ ·æœ¬")
        if errors:
            print("\nå‰å‡ ä¸ªé”™è¯¯:")
            for err in errors:
                print(f"   - {err}")

    if len(dataset_array) == 0:
        raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ ·æœ¬ï¼è¯·æ£€æŸ¥CIFæ–‡ä»¶è·¯å¾„å’ŒCSVæ ¼å¼ã€‚")

    # æ‰“å°æ•°æ®ç»Ÿè®¡
    targets = [d['target'] for d in dataset_array]
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   æ ·æœ¬æ•°: {len(dataset_array)}")
    print(f"   ç›®æ ‡å€¼èŒƒå›´: [{min(targets):.4f}, {max(targets):.4f}]")
    print(f"   ç›®æ ‡å€¼å‡å€¼: {np.mean(targets):.4f}")
    print(f"   ç›®æ ‡å€¼æ ‡å‡†å·®: {np.std(targets):.4f}")

    num_atoms = [len(d['atoms']['elements']) for d in dataset_array]
    print(f"   åŸå­æ•°èŒƒå›´: [{min(num_atoms)}, {max(num_atoms)}]")
    print(f"   å¹³å‡åŸå­æ•°: {np.mean(num_atoms):.1f}\n")

    return dataset_array


# ==================== é…ç½®ç”Ÿæˆ ====================

def create_config(args, dataset_array):
    """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºè®­ç»ƒé…ç½®"""

    # é€‰æ‹©æ¨¡å‹é…ç½®
    if args.model == 'densegnn':
        model_config = DenseGNNConfig(
            name="densegnn",
            densegnn_layers=args.densegnn_layers,
            atom_input_features=92,
            edge_input_features=80,
            embedding_features=64,
            hidden_features=args.hidden_features,
            output_features=1,
            graph_dropout=args.graph_dropout,
            # ä¸­æœŸèåˆ
            use_middle_fusion=args.use_middle_fusion,
            middle_fusion_layers=args.middle_fusion_layers,
            middle_fusion_hidden_dim=args.middle_fusion_hidden_dim,
            middle_fusion_num_heads=args.middle_fusion_num_heads,
            middle_fusion_dropout=args.middle_fusion_dropout,
            # æ™šæœŸèåˆ
            use_cross_modal_attention=args.use_cross_modal,
            cross_modal_hidden_dim=args.cross_modal_hidden_dim,
            cross_modal_num_heads=args.cross_modal_num_heads,
            cross_modal_dropout=args.cross_modal_dropout,
            # å¯¹æ¯”å­¦ä¹ 
            use_contrastive_loss=args.use_contrastive,
            contrastive_temperature=args.contrastive_temperature,
            contrastive_loss_weight=args.contrastive_weight,
            link="identity",
            classification=args.classification
        )
    else:  # alignn
        model_config = ALIGNNConfig(
            name="alignn",
            alignn_layers=args.alignn_layers,
            gcn_layers=args.gcn_layers,
            atom_input_features=92,
            edge_input_features=80,
            triplet_input_features=40,
            embedding_features=64,
            hidden_features=args.hidden_features,
            output_features=1,
            graph_dropout=args.graph_dropout,
            # ä¸­æœŸèåˆ
            use_middle_fusion=args.use_middle_fusion,
            middle_fusion_layers=args.middle_fusion_layers,
            middle_fusion_hidden_dim=args.middle_fusion_hidden_dim,
            middle_fusion_num_heads=args.middle_fusion_num_heads,
            middle_fusion_dropout=args.middle_fusion_dropout,
            # æ™šæœŸèåˆ
            use_cross_modal_attention=args.use_cross_modal,
            cross_modal_hidden_dim=args.cross_modal_hidden_dim,
            cross_modal_num_heads=args.cross_modal_num_heads,
            cross_modal_dropout=args.cross_modal_dropout,
            # å¯¹æ¯”å­¦ä¹ 
            use_contrastive_loss=args.use_contrastive,
            contrastive_temperature=args.contrastive_temperature,
            contrastive_loss_weight=args.contrastive_weight,
            link="identity",
            classification=args.classification
        )

    # åˆ›å»ºè®­ç»ƒé…ç½®
    config_dict = {
        "dataset": "user_data",
        "target": "target",
        "atom_features": "cgcnn",
        "neighbor_strategy": "k-nearest",
        "id_tag": "jid",

        "random_seed": args.random_seed,
        "train_ratio": args.train_ratio,
        "val_ratio": args.val_ratio,
        "test_ratio": args.test_ratio,
        "n_train": args.n_train,
        "n_val": args.n_val,
        "n_test": args.n_test,

        "epochs": args.epochs,
        "batch_size": args.batch_size,
        "learning_rate": args.learning_rate,
        "weight_decay": args.weight_decay,
        "optimizer": "adamw",
        "scheduler": "onecycle",
        "criterion": "mse",

        "n_early_stopping": args.early_stopping_patience,
        "output_dir": args.output_dir,

        "cutoff": 8.0,
        "max_neighbors": 12,
        "num_workers": args.num_workers,
        "pin_memory": True,
        "save_dataloader": False,
        "use_canonize": True,
        "keep_data_order": False,

        "classification_threshold": args.classification_threshold if args.classification else None,

        "model": model_config
    }

    return TrainingConfig(**config_dict)


# ==================== å‘½ä»¤è¡Œå‚æ•° ====================

def get_parser():
    """æ„å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='æœ¬åœ°CIF+CSVæ•°æ®è®­ç»ƒè„šæœ¬',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # æ•°æ®è·¯å¾„
    parser.add_argument('--cif_dir', type=str, required=True,
                       help='CIFæ–‡ä»¶ç›®å½•è·¯å¾„')
    parser.add_argument('--csv_file', type=str, required=True,
                       help='CSVæè¿°æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--target_column', type=str, default='target',
                       help='CSVä¸­ç›®æ ‡å€¼åˆ—å')
    parser.add_argument('--text_column', type=str, default='text_description',
                       help='CSVä¸­æ–‡æœ¬æè¿°åˆ—å')
    parser.add_argument('--id_column', type=str, default='id',
                       help='CSVä¸­æ ·æœ¬IDåˆ—å')

    # æ•°æ®åˆ’åˆ†
    parser.add_argument('--train_ratio', type=float, default=0.8,
                       help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--test_ratio', type=float, default=0.1,
                       help='æµ‹è¯•é›†æ¯”ä¾‹')
    parser.add_argument('--n_train', type=int, default=None,
                       help='è®­ç»ƒæ ·æœ¬æ•°ï¼ˆè¦†ç›–train_ratioï¼‰')
    parser.add_argument('--n_val', type=int, default=None,
                       help='éªŒè¯æ ·æœ¬æ•°')
    parser.add_argument('--n_test', type=int, default=None,
                       help='æµ‹è¯•æ ·æœ¬æ•°')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=300,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=0.0,
                       help='æƒé‡è¡°å‡')
    parser.add_argument('--early_stopping_patience', type=int, default=50,
                       help='Early stoppingè€å¿ƒå€¼')

    # æ¨¡å‹é€‰æ‹©
    parser.add_argument('--model', type=str, default='densegnn',
                       choices=['densegnn', 'alignn'],
                       help='æ¨¡å‹ç±»å‹')

    # DenseGNNå‚æ•°
    parser.add_argument('--densegnn_layers', type=int, default=4,
                       help='DenseGNNå±‚æ•°')

    # ALIGNNå‚æ•°
    parser.add_argument('--alignn_layers', type=int, default=4,
                       help='ALIGNNå±‚æ•°')
    parser.add_argument('--gcn_layers', type=int, default=4,
                       help='GCNå±‚æ•°')

    # é€šç”¨æ¨¡å‹å‚æ•°
    parser.add_argument('--hidden_features', type=int, default=256,
                       help='éšè—å±‚ç‰¹å¾ç»´åº¦')
    parser.add_argument('--graph_dropout', type=float, default=0.0,
                       help='å›¾å±‚dropoutç‡')

    # ä¸­æœŸèåˆ
    parser.add_argument('--use_middle_fusion', type=str2bool, default=True,
                       help='æ˜¯å¦ä½¿ç”¨ä¸­æœŸèåˆ')
    parser.add_argument('--middle_fusion_layers', type=str, default='1,3',
                       help='ä¸­æœŸèåˆå±‚ç´¢å¼•ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--middle_fusion_hidden_dim', type=int, default=128,
                       help='ä¸­æœŸèåˆéšè—ç»´åº¦')
    parser.add_argument('--middle_fusion_num_heads', type=int, default=2,
                       help='ä¸­æœŸèåˆæ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--middle_fusion_dropout', type=float, default=0.1,
                       help='ä¸­æœŸèåˆdropoutç‡')

    # æ™šæœŸèåˆ
    parser.add_argument('--use_cross_modal', type=str2bool, default=True,
                       help='æ˜¯å¦ä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆæ™šæœŸèåˆï¼‰')
    parser.add_argument('--cross_modal_hidden_dim', type=int, default=256,
                       help='è·¨æ¨¡æ€æ³¨æ„åŠ›éšè—ç»´åº¦')
    parser.add_argument('--cross_modal_num_heads', type=int, default=4,
                       help='è·¨æ¨¡æ€æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--cross_modal_dropout', type=float, default=0.1,
                       help='è·¨æ¨¡æ€æ³¨æ„åŠ›dropoutç‡')

    # å¯¹æ¯”å­¦ä¹ 
    parser.add_argument('--use_contrastive', type=str2bool, default=False,
                       help='æ˜¯å¦ä½¿ç”¨å¯¹æ¯”å­¦ä¹ ')
    parser.add_argument('--contrastive_weight', type=float, default=0.1,
                       help='å¯¹æ¯”å­¦ä¹ æŸå¤±æƒé‡')
    parser.add_argument('--contrastive_temperature', type=float, default=0.1,
                       help='å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°')

    # åˆ†ç±»ä»»åŠ¡
    parser.add_argument('--classification', type=str2bool, default=False,
                       help='æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡')
    parser.add_argument('--classification_threshold', type=float, default=0.5,
                       help='åˆ†ç±»é˜ˆå€¼')

    # å…¶ä»–
    parser.add_argument('--output_dir', type=str, default='./results_local_cif_csv/',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--random_seed', type=int, default=123,
                       help='éšæœºç§å­')
    parser.add_argument('--num_workers', type=int, default=4,
                       help='æ•°æ®åŠ è½½workersæ•°é‡')

    return parser


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    parser = get_parser()
    args = parser.parse_args()

    print("\n" + "="*80)
    print("ğŸš€ æœ¬åœ°CIF+CSVæ•°æ®è®­ç»ƒ")
    print("="*80)
    print(f"æ¨¡å‹: {args.model.upper()}")
    print(f"CIFç›®å½•: {args.cif_dir}")
    print(f"CSVæ–‡ä»¶: {args.csv_file}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print("="*80 + "\n")

    # 1. åŠ è½½æ•°æ®é›†
    dataset_array = load_dataset_from_cif_csv(
        cif_dir=args.cif_dir,
        csv_file=args.csv_file,
        target_column=args.target_column,
        text_column=args.text_column,
        id_column=args.id_column
    )

    # 2. åˆ›å»ºé…ç½®
    print("\nâš™ï¸  åˆ›å»ºè®­ç»ƒé…ç½®...")
    config = create_config(args, dataset_array)

    # ä¿å­˜é…ç½®
    os.makedirs(args.output_dir, exist_ok=True)
    with open(os.path.join(args.output_dir, 'config.json'), 'w') as f:
        json.dump(config.dict(), f, indent=2, default=str)
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {os.path.join(args.output_dir, 'config.json')}")

    # 3. å‡†å¤‡æ•°æ®åŠ è½½å™¨
    print("\nğŸ”„ å‡†å¤‡æ•°æ®åŠ è½½å™¨...")

    # DenseGNNä¸éœ€è¦çº¿å›¾
    use_line_graph = (args.model == 'alignn')

    train_loader, val_loader, test_loader, prepare_batch = get_train_val_loaders(
        dataset="user_data",
        dataset_array=dataset_array,
        target="target",
        atom_features="cgcnn",
        id_tag="jid",
        batch_size=args.batch_size,
        split_seed=args.random_seed,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        n_train=args.n_train,
        n_val=args.n_val,
        n_test=args.n_test,
        cutoff=8.0,
        max_neighbors=12,
        line_graph=use_line_graph,
        workers=args.num_workers,
        output_dir=args.output_dir,
        use_canonize=True,
        keep_data_order=False
    )

    print(f"\nâœ… æ•°æ®åŠ è½½å™¨å‡†å¤‡å®Œæˆ")
    print(f"   è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(test_loader.dataset)} æ ·æœ¬")

    # 4. å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print(f"   æ¨¡å‹: {args.model.upper()}")
    print(f"   ä¸­æœŸèåˆ: {'âœ“' if args.use_middle_fusion else 'âœ—'}")
    print(f"   æ™šæœŸèåˆ: {'âœ“' if args.use_cross_modal else 'âœ—'}")
    print(f"   å¯¹æ¯”å­¦ä¹ : {'âœ“' if args.use_contrastive else 'âœ—'}")
    print("="*80 + "\n")

    start_time = time.time()

    history = train_dgl(
        config=config,
        train_val_test_loaders=[train_loader, val_loader, test_loader, prepare_batch]
    )

    # 5. è®­ç»ƒå®Œæˆ
    elapsed_time = time.time() - start_time
    print("\n" + "="*80)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*80)
    print(f"æ€»è€—æ—¶: {elapsed_time/3600:.2f} å°æ—¶")
    print(f"ç»“æœä¿å­˜åœ¨: {args.output_dir}")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()

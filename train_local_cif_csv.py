#!/usr/bin/env python
"""
æœ¬åœ°æ•°æ®è®­ç»ƒè„šæœ¬ - ä½¿ç”¨CIFæ–‡ä»¶ + CSVæè¿°
ï¼ˆç…§æŠ„ train_with_cross_modal_attention.py çš„æ•°æ®é›†å¤„ç†æ–¹å¼ï¼‰

æ”¯æŒçš„æ•°æ®é›†:
    - jarvis: JARVIS-DFT æ•°æ®é›†
    - mp: Material Project æ•°æ®é›†
    - class: åˆ†ç±»æ•°æ®é›†
    - toy: ç©å…·æ•°æ®é›†ï¼ˆæµ‹è¯•ç”¨ï¼‰
    - custom: è‡ªå®šä¹‰æ•°æ®é›†

CSVæ ¼å¼ï¼ˆæŒ‰æ•°æ®é›†ç±»å‹ï¼‰:
    JARVIS/TOY: Id, Composition, prop, Description, File_Name
    MP:         id, composition, formation_energy, band_gap, description, file_name
    Class:      id, target, description
    Custom:     ç”± --target_column, --text_column, --id_column å‚æ•°æŒ‡å®š

ä½¿ç”¨ç¤ºä¾‹:
    # 1. JARVIS æ•°æ®é›†
    python train_local_cif_csv.py \\
        --root_dir ../dataset/ \\
        --dataset jarvis \\
        --property formation_energy \\
        --model densegnn \\
        --use_middle_fusion

    # 2. Material Project æ•°æ®é›†
    python train_local_cif_csv.py \\
        --root_dir ../dataset/ \\
        --dataset mp \\
        --property band_gap \\
        --model densegnn \\
        --use_cross_modal

    # 3. è‡ªå®šä¹‰æ•°æ®é›†ï¼ˆä¸ä¹‹å‰ä¸€æ ·ï¼‰
    python train_local_cif_csv.py \\
        --dataset custom \\
        --cif_dir ./my_structures/cif/ \\
        --csv_file ./my_structures/data.csv \\
        --model densegnn \\
        --use_middle_fusion \\
        --use_cross_modal

    # 4. åˆ†ç±»ä»»åŠ¡
    python train_local_cif_csv.py \\
        --dataset class \\
        --property syn \\
        --classification \\
        --num_classes 2
"""

import os
import sys
import csv
import time
import json
import argparse
import numpy as np
from tqdm import tqdm

import torch
from jarvis.core.atoms import Atoms
from transformers import AutoTokenizer
from tokenizers.normalizers import BertNormalizer

from data import get_train_val_loaders
from train import train_dgl
from config import TrainingConfig
from models.alignn import ALIGNNConfig
from models.densegnn import DenseGNNConfig


# ==================== è¾…åŠ©å‡½æ•° ====================

def str2bool(v):
    """å­—ç¬¦ä¸²è½¬å¸ƒå°”å€¼"""
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('å¸ƒå°”å€¼åº”ä¸º yes/no, true/false, t/f, y/n, 1/0')


# ==================== æ–‡æœ¬å½’ä¸€åŒ– ====================

def setup_text_normalizer():
    """è®¾ç½®æ–‡æœ¬å½’ä¸€åŒ–å™¨"""
    # BERTå½’ä¸€åŒ–å™¨
    norm = BertNormalizer(lowercase=False, strip_accents=True,
                         clean_text=True, handle_chinese_chars=True)

    # åŠ è½½è¯æ±‡æ˜ å°„
    possible_paths = [
        'vocab_mappings.txt',
        './vocab_mappings.txt',
        os.path.join(os.path.dirname(__file__), 'vocab_mappings.txt'),
    ]

    vocab_file = None
    for path in possible_paths:
        if os.path.exists(path):
            vocab_file = path
            break

    if vocab_file is None:
        print("âš ï¸  æœªæ‰¾åˆ° vocab_mappings.txtï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬å½’ä¸€åŒ–")
        mappings = {}
    else:
        with open(vocab_file, 'r') as f:
            mappings_list = f.read().strip().split('\n')
        mappings = {m[0]: m[2:] for m in mappings_list}

    def normalize(text):
        """å½’ä¸€åŒ–æ–‡æœ¬"""
        text = [norm.normalize_str(s) for s in text.split('\n')]
        out = []
        for s in text:
            norm_s = ''
            for c in s:
                norm_s += mappings.get(c, ' ') if mappings else c
            out.append(norm_s)
        return '\n'.join(out)

    return normalize


# ==================== æ•°æ®é›†è·¯å¾„é…ç½®ï¼ˆç…§æŠ„ train_with_cross_modal_attention.pyï¼‰====================

def get_dataset_paths(root_dir, dataset, property_name):
    """æ ¹æ®æ•°æ®é›†å’Œæ€§è´¨è·å–æ•°æ®è·¯å¾„

    Args:
        root_dir: æ•°æ®é›†æ ¹ç›®å½•
        dataset: æ•°æ®é›†åç§° (jarvis, mp, class, toy, custom)
        property_name: å±æ€§åç§°

    Returns:
        cif_dir, csv_file: CIFç›®å½•è·¯å¾„å’ŒCSVæ–‡ä»¶è·¯å¾„
    """
    if dataset.lower() == 'jarvis':
        # JARVIS-DFT æ•°æ®é›†
        property_map = {
            'formation_energy': 'formation_energy_peratom',
            'fe': 'formation_energy_peratom',
            'total_energy': 'optb88vdw_total_energy',
            'opt_bandgap': 'optb88vdw_bandgap',
            'mbj_bandgap': 'mbj_bandgap',
            'bulk_modulus': 'bulk_modulus_kv',
            'bulk_modulus_kv': 'bulk_modulus_kv',
            'shear_modulus': 'shear_modulus_gv',
            'shear_modulus_gv': 'shear_modulus_gv',
        }

        prop_folder = property_map.get(property_name, property_name)
        cif_dir = os.path.join(root_dir, f'jarvis/{prop_folder}/cif/')
        csv_file = os.path.join(root_dir, f'jarvis/{prop_folder}/description.csv')

    elif dataset.lower() == 'mp':
        # Material Project æ•°æ®é›†
        if property_name in ['formation_energy', 'band_gap']:
            cif_dir = os.path.join(root_dir, 'mp_2018_new/')
            csv_file = os.path.join(root_dir, 'mp_2018_new/mat_text.csv')
        elif property_name in ['bulk', 'shear', 'bulk_modulus', 'shear_modulus']:
            cif_dir = os.path.join(root_dir, 'mp_2018_small/cif/')
            csv_file = os.path.join(root_dir, 'mp_2018_small/description.csv')
        else:
            raise ValueError(f"Unsupported property for MP dataset: {property_name}")

    elif dataset.lower() == 'class':
        # åˆ†ç±»æ•°æ®é›†ï¼ˆç±»ä¼¼jarvisç»“æ„ï¼‰
        # ä¾‹å¦‚ï¼šclass/syn, class/metal_oxide, ç­‰
        cif_dir = os.path.join(root_dir, f'class/{property_name}/cif/')
        csv_file = os.path.join(root_dir, f'class/{property_name}/description.csv')

    elif dataset.lower() == 'toy':
        # ç©å…·æ•°æ®é›†ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        cif_dir = os.path.join(root_dir, 'toy/cif/')
        csv_file = os.path.join(root_dir, 'toy/description.csv')

    elif dataset.lower() == 'custom':
        # è‡ªå®šä¹‰æ•°æ®é›†ï¼šç”±å‘½ä»¤è¡Œå‚æ•°æŒ‡å®š
        # è¿™ç§æƒ…å†µä¸‹ï¼Œcif_dir å’Œ csv_file ä¼šåœ¨ main() ä¸­ç›´æ¥ä½¿ç”¨å‚æ•°å€¼
        return None, None

    else:
        raise ValueError(f"Unsupported dataset: {dataset}")

    return cif_dir, csv_file


# ==================== æ•°æ®åŠ è½½ ====================

def load_dataset_from_cif_csv(cif_dir, csv_file, dataset='custom', property_name='formation_energy',
                               target_column='target', text_column='text_description',
                               id_column='id'):
    """ä»CIFç›®å½•å’ŒCSVæ–‡ä»¶åŠ è½½æ•°æ®é›†

    Args:
        cif_dir: CIFæ–‡ä»¶ç›®å½•
        csv_file: CSVæè¿°æ–‡ä»¶è·¯å¾„
        target_column: CSVä¸­ç›®æ ‡å€¼åˆ—å
        text_column: CSVä¸­æ–‡æœ¬æè¿°åˆ—åï¼ˆå¯é€‰ï¼‰
        id_column: CSVä¸­IDåˆ—å

    Returns:
        dataset_array: æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å­—å…¸
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“‚ åŠ è½½æœ¬åœ°æ•°æ®é›†")
    print(f"{'='*80}")
    print(f"CIFç›®å½•: {cif_dir}")
    print(f"CSVæ–‡ä»¶: {csv_file}")
    print(f"ç›®æ ‡åˆ—: {target_column}")
    print(f"æ–‡æœ¬åˆ—: {text_column}")
    print(f"{'='*80}\n")

    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(cif_dir):
        raise FileNotFoundError(f"CIFç›®å½•ä¸å­˜åœ¨: {cif_dir}")
    if not os.path.exists(csv_file):
        raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")

    # è¯»å–CSVæ–‡ä»¶ï¼ˆç…§æŠ„ train_with_cross_modal_attention.py çš„æ–¹å¼ï¼‰
    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        headings = next(reader)
        data = [row for row in reader]

    print(f"CSVæ–‡ä»¶å…± {len(data)} è¡Œ")
    print(f"CSVåˆ—å: {headings}")

    # æ£€æŸ¥å¿…éœ€åˆ—
    if len(data) == 0:
        raise ValueError("CSVæ–‡ä»¶ä¸ºç©º")

    # è®¾ç½®æ–‡æœ¬å½’ä¸€åŒ–å™¨
    normalize_text = setup_text_normalizer()

    # åŠ è½½æ•°æ®
    dataset_array = []
    skipped = 0
    errors = []

    for j in tqdm(range(len(data)), desc="åŠ è½½æ ·æœ¬"):
        try:
            # æ ¹æ®ä¸åŒæ•°æ®é›†è§£æCSVè¡Œï¼ˆç…§æŠ„ train_with_cross_modal_attention.pyï¼‰
            if dataset.lower() == 'mp':
                if property_name == 'formation_energy':
                    sample_id, composition, target_value, _, crys_desc_full, _ = data[j]
                elif property_name == 'band_gap':
                    sample_id, composition, _, target_value, crys_desc_full, _ = data[j]
                elif property_name == 'shear':
                    sample_id, composition, target_value, _, crys_desc_full, _ = data[j]
                elif property_name in ['bulk', 'bulk_modulus']:
                    sample_id, composition, _, target_value, crys_desc_full, _ = data[j]
                else:
                    sample_id, composition, target_value, _, crys_desc_full, _ = data[j]

            elif dataset.lower() == 'jarvis':
                # JARVISæ ¼å¼: Id, Composition, prop, Description, File_Name
                sample_id, composition, target_value, crys_desc_full, _ = data[j]

            elif dataset.lower() == 'class':
                # åˆ†ç±»æ•°æ®é›†æ ¼å¼: id, target, description
                sample_id, target_value, crys_desc_full = data[j]
                composition = ''  # åˆ†ç±»ä»»åŠ¡ä¸éœ€è¦composition

            elif dataset.lower() == 'toy':
                sample_id, composition, target_value, crys_desc_full, _ = data[j]

            elif dataset.lower() == 'custom':
                # è‡ªå®šä¹‰æ ¼å¼ï¼šä½¿ç”¨ DictReader è§£æ
                row_dict = dict(zip(headings, data[j]))
                sample_id = row_dict[id_column].strip()
                target_value = float(row_dict[target_column])
                crys_desc_full = row_dict.get(text_column, f"Crystal structure {sample_id}")
            else:
                raise ValueError(f"Unsupported dataset: {dataset}")

            # æ ‡å‡†åŒ–æ–‡æœ¬æè¿°
            text_desc = normalize_text(crys_desc_full)

            # CIFæ–‡ä»¶è·¯å¾„
            cif_file = os.path.join(cif_dir, f"{sample_id}.cif")
            if not os.path.exists(cif_file):
                skipped += 1
                if len(errors) < 5:
                    errors.append(f"CIFæ–‡ä»¶ä¸å­˜åœ¨: {cif_file}")
                continue

            # åŠ è½½ç»“æ„
            atoms = Atoms.from_cif(cif_file)

            # æ„å»ºæ ·æœ¬ï¼ˆä¸ train_with_cross_modal_attention.py ä¸€è‡´ï¼‰
            sample = {
                "atoms": atoms.to_dict(),
                "jid": sample_id,
                "text": text_desc,  # ä½¿ç”¨ "text" è€Œä¸æ˜¯ "text_description"
                "target": float(target_value)
            }

            # MPæ•°æ®é›†çš„ç‰¹æ®Šå¤„ç†ï¼ˆå¯¹æ•°å˜æ¢ï¼‰
            if dataset.lower() == 'mp' and property_name in ['shear', 'bulk', 'bulk_modulus', 'shear_modulus']:
                sample["target"] = np.log10(float(target_value))

            dataset_array.append(sample)

        except Exception as e:
            skipped += 1
            if len(errors) < 5:
                errors.append(f"æ ·æœ¬ {j}: {str(e)}")

    # æ‰“å°ç»Ÿè®¡
    print(f"\nâœ… æˆåŠŸåŠ è½½: {len(dataset_array)} æ ·æœ¬")
    if skipped > 0:
        print(f"âš ï¸  è·³è¿‡: {skipped} æ ·æœ¬")
        if errors:
            print("\nå‰å‡ ä¸ªé”™è¯¯:")
            for err in errors:
                print(f"   - {err}")

    if len(dataset_array) == 0:
        raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ ·æœ¬ï¼è¯·æ£€æŸ¥CIFæ–‡ä»¶è·¯å¾„å’ŒCSVæ ¼å¼ã€‚")

    # æ‰“å°æ•°æ®ç»Ÿè®¡
    targets = [d['target'] for d in dataset_array]
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   æ ·æœ¬æ•°: {len(dataset_array)}")
    print(f"   ç›®æ ‡å€¼èŒƒå›´: [{min(targets):.4f}, {max(targets):.4f}]")
    print(f"   ç›®æ ‡å€¼å‡å€¼: {np.mean(targets):.4f}")
    print(f"   ç›®æ ‡å€¼æ ‡å‡†å·®: {np.std(targets):.4f}")

    num_atoms = [len(d['atoms']['elements']) for d in dataset_array]
    print(f"   åŸå­æ•°èŒƒå›´: [{min(num_atoms)}, {max(num_atoms)}]")
    print(f"   å¹³å‡åŸå­æ•°: {np.mean(num_atoms):.1f}\n")

    return dataset_array


# ==================== é…ç½®ç”Ÿæˆ ====================

def create_config(args, dataset_array):
    """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºè®­ç»ƒé…ç½®"""

    # é€‰æ‹©æ¨¡å‹é…ç½®
    if args.model == 'densegnn':
        model_config = DenseGNNConfig(
            name="densegnn",
            densegnn_layers=args.densegnn_layers,
            atom_input_features=92,
            edge_input_features=80,
            embedding_features=64,
            hidden_features=args.hidden_features,
            output_features=1,
            graph_dropout=args.graph_dropout,
            # ä¸­æœŸèåˆ
            use_middle_fusion=args.use_middle_fusion,
            middle_fusion_layers=args.middle_fusion_layers,
            middle_fusion_hidden_dim=args.middle_fusion_hidden_dim,
            middle_fusion_num_heads=args.middle_fusion_num_heads,
            middle_fusion_dropout=args.middle_fusion_dropout,
            # æ™šæœŸèåˆ
            use_cross_modal_attention=args.use_cross_modal,
            cross_modal_hidden_dim=args.cross_modal_hidden_dim,
            cross_modal_num_heads=args.cross_modal_num_heads,
            cross_modal_dropout=args.cross_modal_dropout,
            # å¯¹æ¯”å­¦ä¹ 
            use_contrastive_loss=args.use_contrastive,
            contrastive_temperature=args.contrastive_temperature,
            contrastive_loss_weight=args.contrastive_weight,
            link="identity",
            classification=args.classification
        )
    else:  # alignn
        model_config = ALIGNNConfig(
            name="alignn",
            alignn_layers=args.alignn_layers,
            gcn_layers=args.gcn_layers,
            atom_input_features=92,
            edge_input_features=80,
            triplet_input_features=40,
            embedding_features=64,
            hidden_features=args.hidden_features,
            output_features=1,
            graph_dropout=args.graph_dropout,
            # ä¸­æœŸèåˆ
            use_middle_fusion=args.use_middle_fusion,
            middle_fusion_layers=args.middle_fusion_layers,
            middle_fusion_hidden_dim=args.middle_fusion_hidden_dim,
            middle_fusion_num_heads=args.middle_fusion_num_heads,
            middle_fusion_dropout=args.middle_fusion_dropout,
            # æ™šæœŸèåˆ
            use_cross_modal_attention=args.use_cross_modal,
            cross_modal_hidden_dim=args.cross_modal_hidden_dim,
            cross_modal_num_heads=args.cross_modal_num_heads,
            cross_modal_dropout=args.cross_modal_dropout,
            # å¯¹æ¯”å­¦ä¹ 
            use_contrastive_loss=args.use_contrastive,
            contrastive_temperature=args.contrastive_temperature,
            contrastive_loss_weight=args.contrastive_weight,
            link="identity",
            classification=args.classification
        )

    # åˆ›å»ºè®­ç»ƒé…ç½®
    config_dict = {
        "dataset": "user_data",
        "target": "target",
        "atom_features": "cgcnn",
        "neighbor_strategy": "k-nearest",
        "id_tag": "jid",

        "random_seed": args.random_seed,
        "train_ratio": args.train_ratio,
        "val_ratio": args.val_ratio,
        "test_ratio": args.test_ratio,
        "n_train": args.n_train,
        "n_val": args.n_val,
        "n_test": args.n_test,

        "epochs": args.epochs,
        "batch_size": args.batch_size,
        "learning_rate": args.learning_rate,
        "weight_decay": args.weight_decay,
        "optimizer": "adamw",
        "scheduler": "onecycle",
        "criterion": "mse",

        "n_early_stopping": args.early_stopping_patience,
        "output_dir": args.output_dir,

        "cutoff": 8.0,
        "max_neighbors": 12,
        "num_workers": args.num_workers,
        "pin_memory": True,
        "save_dataloader": False,
        "use_canonize": True,
        "keep_data_order": False,

        "classification_threshold": args.classification_threshold if args.classification else None,

        "model": model_config
    }

    return TrainingConfig(**config_dict)


# ==================== å‘½ä»¤è¡Œå‚æ•° ====================

def get_parser():
    """æ„å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='æœ¬åœ°CIF+CSVæ•°æ®è®­ç»ƒè„šæœ¬',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # ========== æ•°æ®é›†è·¯å¾„å‚æ•°ï¼ˆç…§æŠ„ train_with_cross_modal_attention.pyï¼‰==========
    parser.add_argument('--root_dir', type=str, default='../dataset/',
                       help='æ•°æ®é›†æ ¹ç›®å½•ï¼ˆç›¸å¯¹äºå½“å‰ç›®å½•æˆ–ç»å¯¹è·¯å¾„ï¼‰')
    parser.add_argument('--dataset', type=str, default='custom',
                       choices=['jarvis', 'mp', 'class', 'toy', 'custom'],
                       help='æ•°æ®é›†åç§°: jarvis, mp, class (åˆ†ç±»), toy, custom (è‡ªå®šä¹‰)')
    parser.add_argument('--property', type=str, default='formation_energy',
                       help='é¢„æµ‹çš„æ€§è´¨ (å›å½’: formation_energy, band_gap; åˆ†ç±»: syn, metal_oxideç­‰)')

    # ========== è‡ªå®šä¹‰æ•°æ®è·¯å¾„ï¼ˆå½“ dataset=custom æ—¶ä½¿ç”¨ï¼‰==========
    parser.add_argument('--cif_dir', type=str, default=None,
                       help='CIFæ–‡ä»¶ç›®å½•è·¯å¾„ï¼ˆå½“dataset=customæ—¶å¿…éœ€ï¼‰')
    parser.add_argument('--csv_file', type=str, default=None,
                       help='CSVæè¿°æ–‡ä»¶è·¯å¾„ï¼ˆå½“dataset=customæ—¶å¿…éœ€ï¼‰')
    parser.add_argument('--target_column', type=str, default='target',
                       help='CSVä¸­ç›®æ ‡å€¼åˆ—å')
    parser.add_argument('--text_column', type=str, default='text_description',
                       help='CSVä¸­æ–‡æœ¬æè¿°åˆ—å')
    parser.add_argument('--id_column', type=str, default='id',
                       help='CSVä¸­æ ·æœ¬IDåˆ—å')

    # æ•°æ®åˆ’åˆ†
    parser.add_argument('--train_ratio', type=float, default=0.8,
                       help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--test_ratio', type=float, default=0.1,
                       help='æµ‹è¯•é›†æ¯”ä¾‹')
    parser.add_argument('--n_train', type=int, default=None,
                       help='è®­ç»ƒæ ·æœ¬æ•°ï¼ˆè¦†ç›–train_ratioï¼‰')
    parser.add_argument('--n_val', type=int, default=None,
                       help='éªŒè¯æ ·æœ¬æ•°')
    parser.add_argument('--n_test', type=int, default=None,
                       help='æµ‹è¯•æ ·æœ¬æ•°')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=300,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=0.0,
                       help='æƒé‡è¡°å‡')
    parser.add_argument('--early_stopping_patience', type=int, default=50,
                       help='Early stoppingè€å¿ƒå€¼')

    # æ¨¡å‹é€‰æ‹©
    parser.add_argument('--model', type=str, default='densegnn',
                       choices=['densegnn', 'alignn'],
                       help='æ¨¡å‹ç±»å‹')

    # DenseGNNå‚æ•°
    parser.add_argument('--densegnn_layers', type=int, default=4,
                       help='DenseGNNå±‚æ•°')

    # ALIGNNå‚æ•°
    parser.add_argument('--alignn_layers', type=int, default=4,
                       help='ALIGNNå±‚æ•°')
    parser.add_argument('--gcn_layers', type=int, default=4,
                       help='GCNå±‚æ•°')

    # é€šç”¨æ¨¡å‹å‚æ•°
    parser.add_argument('--hidden_features', type=int, default=256,
                       help='éšè—å±‚ç‰¹å¾ç»´åº¦')
    parser.add_argument('--graph_dropout', type=float, default=0.0,
                       help='å›¾å±‚dropoutç‡')

    # ä¸­æœŸèåˆ
    parser.add_argument('--use_middle_fusion', type=str2bool, default=True,
                       help='æ˜¯å¦ä½¿ç”¨ä¸­æœŸèåˆ')
    parser.add_argument('--middle_fusion_layers', type=str, default='1,3',
                       help='ä¸­æœŸèåˆå±‚ç´¢å¼•ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--middle_fusion_hidden_dim', type=int, default=128,
                       help='ä¸­æœŸèåˆéšè—ç»´åº¦')
    parser.add_argument('--middle_fusion_num_heads', type=int, default=2,
                       help='ä¸­æœŸèåˆæ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--middle_fusion_dropout', type=float, default=0.1,
                       help='ä¸­æœŸèåˆdropoutç‡')

    # æ™šæœŸèåˆ
    parser.add_argument('--use_cross_modal', type=str2bool, default=True,
                       help='æ˜¯å¦ä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆæ™šæœŸèåˆï¼‰')
    parser.add_argument('--cross_modal_hidden_dim', type=int, default=256,
                       help='è·¨æ¨¡æ€æ³¨æ„åŠ›éšè—ç»´åº¦')
    parser.add_argument('--cross_modal_num_heads', type=int, default=4,
                       help='è·¨æ¨¡æ€æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--cross_modal_dropout', type=float, default=0.1,
                       help='è·¨æ¨¡æ€æ³¨æ„åŠ›dropoutç‡')

    # å¯¹æ¯”å­¦ä¹ 
    parser.add_argument('--use_contrastive', type=str2bool, default=False,
                       help='æ˜¯å¦ä½¿ç”¨å¯¹æ¯”å­¦ä¹ ')
    parser.add_argument('--contrastive_weight', type=float, default=0.1,
                       help='å¯¹æ¯”å­¦ä¹ æŸå¤±æƒé‡')
    parser.add_argument('--contrastive_temperature', type=float, default=0.1,
                       help='å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°')

    # åˆ†ç±»ä»»åŠ¡
    parser.add_argument('--classification', type=str2bool, default=False,
                       help='æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡')
    parser.add_argument('--classification_threshold', type=float, default=0.5,
                       help='åˆ†ç±»é˜ˆå€¼')

    # å…¶ä»–
    parser.add_argument('--output_dir', type=str, default='./results_local_cif_csv/',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--random_seed', type=int, default=123,
                       help='éšæœºç§å­')
    parser.add_argument('--num_workers', type=int, default=4,
                       help='æ•°æ®åŠ è½½workersæ•°é‡')

    return parser


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    parser = get_parser()
    args = parser.parse_args()

    print("\n" + "="*80)
    print("ğŸš€ æœ¬åœ°CIF+CSVæ•°æ®è®­ç»ƒ")
    print("="*80)
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"å±æ€§: {args.property}")
    print(f"æ¨¡å‹: {args.model.upper()}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print("="*80 + "\n")

    # 1. ç¡®å®šæ•°æ®è·¯å¾„ï¼ˆç…§æŠ„ train_with_cross_modal_attention.py çš„æ–¹å¼ï¼‰
    if args.dataset == 'custom':
        # è‡ªå®šä¹‰æ•°æ®é›†ï¼šéœ€è¦ç”¨æˆ·æä¾›è·¯å¾„
        if args.cif_dir is None or args.csv_file is None:
            raise ValueError(
                "ä½¿ç”¨ custom æ•°æ®é›†æ—¶ï¼Œå¿…é¡»æä¾› --cif_dir å’Œ --csv_file å‚æ•°\n"
                "ç¤ºä¾‹: python train_local_cif_csv.py --dataset custom --cif_dir ./cifs/ --csv_file ./data.csv"
            )
        cif_dir = args.cif_dir
        csv_file = args.csv_file
    else:
        # æ ‡å‡†æ•°æ®é›†ï¼ˆjarvis, mp, class, toyï¼‰ï¼šä½¿ç”¨ get_dataset_paths
        cif_dir, csv_file = get_dataset_paths(args.root_dir, args.dataset, args.property)
        print(f"ğŸ“‚ ä½¿ç”¨æ ‡å‡†æ•°æ®é›†è·¯å¾„:")
        print(f"   CIFç›®å½•: {cif_dir}")
        print(f"   CSVæ–‡ä»¶: {csv_file}\n")

    # 2. åŠ è½½æ•°æ®é›†
    dataset_array = load_dataset_from_cif_csv(
        cif_dir=cif_dir,
        csv_file=csv_file,
        dataset=args.dataset,
        property_name=args.property,
        target_column=args.target_column,
        text_column=args.text_column,
        id_column=args.id_column
    )

    # 2. åˆ›å»ºé…ç½®
    print("\nâš™ï¸  åˆ›å»ºè®­ç»ƒé…ç½®...")
    config = create_config(args, dataset_array)

    # ä¿å­˜é…ç½®
    os.makedirs(args.output_dir, exist_ok=True)
    with open(os.path.join(args.output_dir, 'config.json'), 'w') as f:
        json.dump(config.dict(), f, indent=2, default=str)
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {os.path.join(args.output_dir, 'config.json')}")

    # 3. å‡†å¤‡æ•°æ®åŠ è½½å™¨
    print("\nğŸ”„ å‡†å¤‡æ•°æ®åŠ è½½å™¨...")

    # DenseGNNä¸éœ€è¦çº¿å›¾
    use_line_graph = (args.model == 'alignn')

    train_loader, val_loader, test_loader, prepare_batch = get_train_val_loaders(
        dataset="user_data",
        dataset_array=dataset_array,
        target="target",
        atom_features="cgcnn",
        id_tag="jid",
        batch_size=args.batch_size,
        split_seed=args.random_seed,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        n_train=args.n_train,
        n_val=args.n_val,
        n_test=args.n_test,
        cutoff=8.0,
        max_neighbors=12,
        line_graph=use_line_graph,
        workers=args.num_workers,
        output_dir=args.output_dir,
        use_canonize=True,
        keep_data_order=False
    )

    print(f"\nâœ… æ•°æ®åŠ è½½å™¨å‡†å¤‡å®Œæˆ")
    print(f"   è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(test_loader.dataset)} æ ·æœ¬")

    # 4. å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print(f"   æ¨¡å‹: {args.model.upper()}")
    print(f"   ä¸­æœŸèåˆ: {'âœ“' if args.use_middle_fusion else 'âœ—'}")
    print(f"   æ™šæœŸèåˆ: {'âœ“' if args.use_cross_modal else 'âœ—'}")
    print(f"   å¯¹æ¯”å­¦ä¹ : {'âœ“' if args.use_contrastive else 'âœ—'}")
    print("="*80 + "\n")

    start_time = time.time()

    history = train_dgl(
        config=config,
        train_val_test_loaders=[train_loader, val_loader, test_loader, prepare_batch]
    )

    # 5. è®­ç»ƒå®Œæˆ
    elapsed_time = time.time() - start_time
    print("\n" + "="*80)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*80)
    print(f"æ€»è€—æ—¶: {elapsed_time/3600:.2f} å°æ—¶")
    print(f"ç»“æœä¿å­˜åœ¨: {args.output_dir}")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()

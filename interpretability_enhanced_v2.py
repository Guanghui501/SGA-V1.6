"""
å¢å¼ºå¯è§£é‡Šæ€§åˆ†ææ¨¡å— v2.0

åŠŸèƒ½ç‰¹æ€§ï¼š
1. å¤šå±‚æ¬¡æ³¨æ„åŠ›åˆ†æï¼ˆå…¨å±€ã€ç»†ç²’åº¦ã€å¤šå¤´ï¼‰
2. åŸå­é‡è¦æ€§å½’å› ï¼ˆæ¢¯åº¦ã€ç§¯åˆ†æ¢¯åº¦ã€SHAPè¿‘ä¼¼ï¼‰
3. æ–‡æœ¬Tokené‡è¦æ€§åˆ†æ
4. è·¨æ¨¡æ€äº¤äº’å¯è§†åŒ–
5. ç‰©ç†åŒ–å­¦ç‰¹å¾å…³è”åˆ†æ
6. é¢„æµ‹ç½®ä¿¡åº¦ä¸ä¸ç¡®å®šæ€§ä¼°è®¡
7. æ‰¹é‡å¯è§£é‡Šæ€§æŠ¥å‘Šç”Ÿæˆ
8. ç¬¦å·å›å½’åˆ†æï¼ˆä»ç¥ç»ç½‘ç»œç‰¹å¾ä¸­å‘ç°å¯è§£é‡Šçš„æ•°å­¦å…¬å¼ï¼‰

ä½œè€…: Enhanced Interpretability Module v2
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import LinearSegmentedColormap
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Union, Any
import pandas as pd
from pathlib import Path
import json
from collections import defaultdict
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False


def safe_to_float(value):
    """
    å®‰å…¨åœ°å°†å„ç§æ ¼å¼çš„æ•°å€¼è½¬æ¢ä¸ºPython float
    
    å¤„ç†ï¼šæ ‡é‡ã€0ç»´æ•°ç»„ã€1ç»´æ•°ç»„ã€tensorç­‰
    """
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, np.ndarray):
        if value.ndim == 0:
            return float(value.item())
        else:
            return float(value.flat[0])
    if torch.is_tensor(value):
        return float(value.detach().cpu().item()) if value.numel() == 1 else float(value.detach().cpu().flatten()[0].item())
    if hasattr(value, 'item'):
        return float(value.item())
    return float(value)


class AtomImportanceAnalyzer:
    """åŸå­é‡è¦æ€§åˆ†æå™¨ - å¤šç§å½’å› æ–¹æ³•"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.model.eval()
        
    def gradient_importance(self, g, lg, text, target_output=None):
        """
        æ¢¯åº¦æ³•è®¡ç®—åŸå­é‡è¦æ€§
        
        Args:
            g: DGL graph
            lg: Line graph
            text: æ–‡æœ¬åˆ—è¡¨
            target_output: ç›®æ ‡è¾“å‡ºç´¢å¼•ï¼ˆç”¨äºå¤šè¾“å‡ºä»»åŠ¡ï¼‰
            
        Returns:
            importance: [num_atoms] åŸå­é‡è¦æ€§åˆ†æ•°
            gradients: åŸå§‹æ¢¯åº¦
        """
        g = g.to(self.device)
        lg = lg.to(self.device)
        
        # è·å–åŸå­ç‰¹å¾å¹¶å¯ç”¨æ¢¯åº¦
        atom_features = g.ndata['atom_features'].clone().detach().requires_grad_(True)
        original_features = g.ndata['atom_features']
        g.ndata['atom_features'] = atom_features
        
        # å‰å‘ä¼ æ’­
        self.model.train()  # å¯ç”¨æ¢¯åº¦è®¡ç®—
        output = self.model([g, lg, text])
        
        if isinstance(output, dict):
            prediction = output['predictions']
        else:
            prediction = output
            
        # é€‰æ‹©ç›®æ ‡è¾“å‡º
        if target_output is not None and prediction.dim() > 1:
            prediction = prediction[:, target_output]
            
        # åå‘ä¼ æ’­
        loss = prediction.sum()
        loss.backward()
        
        # è®¡ç®—é‡è¦æ€§ï¼ˆæ¢¯åº¦çš„L2èŒƒæ•°ï¼‰
        gradients = atom_features.grad.detach()
        importance = torch.norm(gradients, dim=1).cpu().numpy()
        
        # æ¢å¤æ¨¡å‹çŠ¶æ€
        self.model.eval()
        g.ndata['atom_features'] = original_features
        
        return importance, gradients.cpu().numpy()
    
    def integrated_gradients(self, g, lg, text, steps=50, baseline='zero'):
        """
        ç§¯åˆ†æ¢¯åº¦æ³• - æ›´å¯é çš„å½’å› æ–¹æ³•
        
        Args:
            g, lg, text: è¾“å…¥æ•°æ®
            steps: ç§¯åˆ†æ­¥æ•°
            baseline: åŸºçº¿ç±»å‹ ('zero', 'random', 'mean')
            
        Returns:
            importance: åŸå­é‡è¦æ€§åˆ†æ•°
            attributions: è¯¦ç»†å½’å› 
        """
        g = g.to(self.device)
        lg = lg.to(self.device)
        
        original_features = g.ndata['atom_features'].clone()
        
        # åˆ›å»ºåŸºçº¿
        if baseline == 'zero':
            baseline_features = torch.zeros_like(original_features)
        elif baseline == 'random':
            baseline_features = torch.randn_like(original_features) * 0.1
        elif baseline == 'mean':
            baseline_features = original_features.mean(dim=0, keepdim=True).expand_as(original_features)
        else:
            baseline_features = torch.zeros_like(original_features)
            
        # ç§¯åˆ†è·¯å¾„
        integrated_grads = torch.zeros_like(original_features)
        
        self.model.train()
        
        for alpha in torch.linspace(0, 1, steps):
            # æ’å€¼ç‰¹å¾
            interpolated = baseline_features + alpha * (original_features - baseline_features)
            interpolated = interpolated.clone().detach().requires_grad_(True)
            g.ndata['atom_features'] = interpolated
            
            # å‰å‘ä¼ æ’­
            output = self.model([g, lg, text])
            if isinstance(output, dict):
                prediction = output['predictions']
            else:
                prediction = output
                
            loss = prediction.sum()
            loss.backward()
            
            integrated_grads += interpolated.grad
            
        # å¹³å‡å¹¶ç¼©æ”¾
        integrated_grads = integrated_grads / steps
        attributions = integrated_grads * (original_features - baseline_features)
        importance = torch.norm(attributions, dim=1).cpu().numpy()
        
        # æ¢å¤
        self.model.eval()
        g.ndata['atom_features'] = original_features
        
        return importance, attributions.cpu().numpy()
    
    def layer_wise_relevance(self, g, lg, text):
        """
        Layer-wise Relevance Propagation (LRP) è¿‘ä¼¼
        é€šè¿‡åˆ†æä¸­é—´å±‚æ¿€æ´»æ¥ç†è§£è´¡çŒ®
        """
        g = g.to(self.device)
        lg = lg.to(self.device)
        
        # æ³¨å†Œé’©å­æ”¶é›†ä¸­é—´æ¿€æ´»
        activations = {}
        hooks = []
        
        def save_activation(name):
            def hook(module, input, output):
                if isinstance(output, tuple):
                    activations[name] = output[0].detach()
                else:
                    activations[name] = output.detach()
            return hook
        
        # åœ¨å…³é”®å±‚æ³¨å†Œé’©å­
        for name, module in self.model.named_modules():
            if 'alignn_layers' in name or 'gcn_layers' in name:
                hooks.append(module.register_forward_hook(save_activation(name)))
                
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output = self.model([g, lg, text])
            
        # ç§»é™¤é’©å­
        for hook in hooks:
            hook.remove()
            
        # åˆ†ææ¿€æ´»å¼ºåº¦ä½œä¸ºé‡è¦æ€§ä»£ç†
        layer_importance = {}
        for name, act in activations.items():
            if act.dim() >= 2:
                layer_importance[name] = torch.norm(act, dim=-1).cpu().numpy()
                
        return layer_importance, activations


class TextTokenAnalyzer:
    """æ–‡æœ¬Tokené‡è¦æ€§åˆ†æå™¨"""
    
    def __init__(self, model, tokenizer, device='cuda'):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
    def analyze_token_importance(self, g, lg, text, method='attention'):
        """
        åˆ†ææ–‡æœ¬tokençš„é‡è¦æ€§
        
        Args:
            g, lg, text: è¾“å…¥æ•°æ®
            method: åˆ†ææ–¹æ³• ('attention', 'gradient', 'occlusion')
            
        Returns:
            token_importance: {token: importance_score}
        """
        if method == 'attention':
            return self._attention_based_importance(g, lg, text)
        elif method == 'gradient':
            return self._gradient_based_importance(g, lg, text)
        elif method == 'occlusion':
            return self._occlusion_based_importance(g, lg, text)
        else:
            raise ValueError(f"Unknown method: {method}")
            
    def _attention_based_importance(self, g, lg, text):
        """åŸºäºæ³¨æ„åŠ›æƒé‡çš„tokené‡è¦æ€§"""
        g = g.to(self.device)
        lg = lg.to(self.device)
        
        with torch.no_grad():
            output = self.model(
                [g, lg, text],
                return_features=True,
                return_attention=True
            )
            
        # è·å–ç»†ç²’åº¦æ³¨æ„åŠ›
        if 'fine_grained_attention_weights' in output:
            fg_attn = output['fine_grained_attention_weights']
            
            # text_to_atom: [batch, heads, seq_len, num_atoms]
            t2a = fg_attn.get('text_to_atom', None)
            
            if t2a is not None:
                # å¹³å‡è·¨å¤´å’ŒåŸå­ï¼Œå¾—åˆ°æ¯ä¸ªtokençš„é‡è¦æ€§
                token_importance = t2a[0].mean(dim=0).mean(dim=1).cpu().numpy()
                
                # è·å–tokenæ–‡æœ¬
                tokens = self.tokenizer.tokenize(text[0])
                tokens = ['[CLS]'] + tokens + ['[SEP]']
                
                # æˆªæ–­æˆ–å¡«å……åˆ°åŒ¹é…é•¿åº¦
                seq_len = len(token_importance)
                if len(tokens) > seq_len:
                    tokens = tokens[:seq_len]
                elif len(tokens) < seq_len:
                    tokens = tokens + ['[PAD]'] * (seq_len - len(tokens))
                    
                return dict(zip(tokens, token_importance))
                
        return {}
    
    def _gradient_based_importance(self, g, lg, text):
        """åŸºäºæ¢¯åº¦çš„tokené‡è¦æ€§"""
        # éœ€è¦è®¿é—®æ–‡æœ¬ç¼–ç å™¨çš„åµŒå…¥å±‚
        # è¿™é‡Œæä¾›æ¡†æ¶ï¼Œå…·ä½“å®ç°å–å†³äºæ¨¡å‹ç»“æ„
        pass
    
    def _occlusion_based_importance(self, g, lg, text, batch_size=10):
        """
        é®æŒ¡æ³•ï¼šé€ä¸ªmask tokenè§‚å¯Ÿé¢„æµ‹å˜åŒ–
        """
        g = g.to(self.device)
        lg = lg.to(self.device)
        
        # åŸºå‡†é¢„æµ‹
        with torch.no_grad():
            base_output = self.model([g, lg, text])
            if isinstance(base_output, dict):
                base_pred = base_output['predictions'].item()
            else:
                base_pred = base_output.item()
                
        # è·å–tokens
        tokens = self.tokenizer.tokenize(text[0])
        tokens = ['[CLS]'] + tokens + ['[SEP]']
        
        token_importance = {}
        
        for i, token in enumerate(tokens):
            if token in ['[CLS]', '[SEP]', '[PAD]']:
                token_importance[f"{token}_{i}"] = 0.0
                continue
                
            # åˆ›å»ºmaskæ–‡æœ¬
            masked_tokens = tokens.copy()
            masked_tokens[i] = '[MASK]'
            masked_text = [self.tokenizer.convert_tokens_to_string(masked_tokens[1:-1])]
            
            # é¢„æµ‹
            with torch.no_grad():
                masked_output = self.model([g, lg, masked_text])
                if isinstance(masked_output, dict):
                    masked_pred = masked_output['predictions'].item()
                else:
                    masked_pred = masked_output.item()
                    
            # é‡è¦æ€§ = é¢„æµ‹å˜åŒ–çš„ç»å¯¹å€¼
            importance = abs(base_pred - masked_pred)
            token_importance[f"{token}_{i}"] = importance
            
        return token_importance


class CrossModalInteractionAnalyzer:
    """è·¨æ¨¡æ€äº¤äº’åˆ†æå™¨"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        
    def analyze_modal_contribution(self, g, lg, text):
        """
        åˆ†æå„æ¨¡æ€å¯¹é¢„æµ‹çš„è´¡çŒ®
        
        Returns:
            contributions: {
                'graph_only': prediction,
                'text_only': prediction,
                'combined': prediction,
                'graph_contribution': float,
                'text_contribution': float,
                'synergy': float  # ååŒæ•ˆåº”
            }
        """
        g = g.to(self.device)
        lg = lg.to(self.device)
        
        contributions = {}
        
        # å®Œæ•´é¢„æµ‹
        with torch.no_grad():
            full_output = self.model([g, lg, text], return_features=True)
            if isinstance(full_output, dict):
                contributions['combined'] = safe_to_float(full_output['predictions'].cpu().numpy())
                graph_feat = full_output.get('graph_features', None)
                text_feat = full_output.get('text_features', None)
            else:
                contributions['combined'] = safe_to_float(full_output.cpu().numpy())
                
        # ç‰¹å¾èŒƒæ•°ä½œä¸ºè´¡çŒ®åº¦ä»£ç†
        if graph_feat is not None and text_feat is not None:
            graph_norm = torch.norm(graph_feat).item()
            text_norm = torch.norm(text_feat).item()
            total_norm = graph_norm + text_norm
            
            contributions['graph_contribution'] = float(graph_norm / total_norm)
            contributions['text_contribution'] = float(text_norm / total_norm)
            
            # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦ä½œä¸ºååŒåº¦é‡
            if graph_feat.shape == text_feat.shape:
                similarity = F.cosine_similarity(
                    graph_feat.flatten().unsqueeze(0),
                    text_feat.flatten().unsqueeze(0)
                ).item()
                contributions['feature_alignment'] = float(similarity)
                
        return contributions
    
    def attention_flow_analysis(self, g, lg, text):
        """
        æ³¨æ„åŠ›æµåˆ†æï¼šè¿½è¸ªä¿¡æ¯å¦‚ä½•åœ¨æ¨¡æ€é—´æµåŠ¨
        """
        g = g.to(self.device)
        lg = lg.to(self.device)
        
        with torch.no_grad():
            output = self.model(
                [g, lg, text],
                return_features=True,
                return_attention=True,
                return_intermediate_features=True
            )
            
        flow_analysis = {}
        
        # æ”¶é›†å„é˜¶æ®µçš„ç‰¹å¾
        if 'graph_base' in output:
            flow_analysis['graph_base_norm'] = float(torch.norm(output['graph_base']).item())
        if 'text_base' in output:
            flow_analysis['text_base_norm'] = float(torch.norm(output['text_base']).item())
        if 'graph_cross' in output:
            flow_analysis['graph_after_cross_norm'] = float(torch.norm(output['graph_cross']).item())
        if 'text_cross' in output:
            flow_analysis['text_after_cross_norm'] = float(torch.norm(output['text_cross']).item())
            
        # è®¡ç®—ç‰¹å¾å˜åŒ–
        if 'graph_base' in output and 'graph_cross' in output:
            change = output['graph_cross'] - output['graph_base']
            flow_analysis['graph_change_from_text'] = float(torch.norm(change).item())
            
        if 'text_base' in output and 'text_cross' in output:
            change = output['text_cross'] - output['text_base']
            flow_analysis['text_change_from_graph'] = float(torch.norm(change).item())
            
        return flow_analysis


class PhysicsCorrelationAnalyzer:
    """ç‰©ç†åŒ–å­¦ç‰¹å¾å…³è”åˆ†æå™¨"""
    
    def __init__(self):
        # å…ƒç´ ç‰©ç†åŒ–å­¦æ•°æ®
        self.element_data = self._load_element_data()
        
    def _load_element_data(self):
        """åŠ è½½å…ƒç´ ç‰©ç†åŒ–å­¦æ•°æ®"""
        # åŸºç¡€å…ƒç´ æ•°æ®ï¼ˆå¯æ‰©å±•ï¼‰
        data = {
            'H': {'electronegativity': 2.20, 'atomic_radius': 53, 'group': 1, 'period': 1},
            'Li': {'electronegativity': 0.98, 'atomic_radius': 167, 'group': 1, 'period': 2},
            'Be': {'electronegativity': 1.57, 'atomic_radius': 112, 'group': 2, 'period': 2},
            'B': {'electronegativity': 2.04, 'atomic_radius': 87, 'group': 13, 'period': 2},
            'C': {'electronegativity': 2.55, 'atomic_radius': 67, 'group': 14, 'period': 2},
            'N': {'electronegativity': 3.04, 'atomic_radius': 56, 'group': 15, 'period': 2},
            'O': {'electronegativity': 3.44, 'atomic_radius': 48, 'group': 16, 'period': 2},
            'F': {'electronegativity': 3.98, 'atomic_radius': 42, 'group': 17, 'period': 2},
            'Na': {'electronegativity': 0.93, 'atomic_radius': 190, 'group': 1, 'period': 3},
            'Mg': {'electronegativity': 1.31, 'atomic_radius': 145, 'group': 2, 'period': 3},
            'Al': {'electronegativity': 1.61, 'atomic_radius': 118, 'group': 13, 'period': 3},
            'Si': {'electronegativity': 1.90, 'atomic_radius': 111, 'group': 14, 'period': 3},
            'P': {'electronegativity': 2.19, 'atomic_radius': 98, 'group': 15, 'period': 3},
            'S': {'electronegativity': 2.58, 'atomic_radius': 88, 'group': 16, 'period': 3},
            'Cl': {'electronegativity': 3.16, 'atomic_radius': 79, 'group': 17, 'period': 3},
            'K': {'electronegativity': 0.82, 'atomic_radius': 243, 'group': 1, 'period': 4},
            'Ca': {'electronegativity': 1.00, 'atomic_radius': 194, 'group': 2, 'period': 4},
            'Ti': {'electronegativity': 1.54, 'atomic_radius': 176, 'group': 4, 'period': 4},
            'V': {'electronegativity': 1.63, 'atomic_radius': 171, 'group': 5, 'period': 4},
            'Cr': {'electronegativity': 1.66, 'atomic_radius': 166, 'group': 6, 'period': 4},
            'Mn': {'electronegativity': 1.55, 'atomic_radius': 161, 'group': 7, 'period': 4},
            'Fe': {'electronegativity': 1.83, 'atomic_radius': 156, 'group': 8, 'period': 4},
            'Co': {'electronegativity': 1.88, 'atomic_radius': 152, 'group': 9, 'period': 4},
            'Ni': {'electronegativity': 1.91, 'atomic_radius': 149, 'group': 10, 'period': 4},
            'Cu': {'electronegativity': 1.90, 'atomic_radius': 145, 'group': 11, 'period': 4},
            'Zn': {'electronegativity': 1.65, 'atomic_radius': 142, 'group': 12, 'period': 4},
            'Ga': {'electronegativity': 1.81, 'atomic_radius': 136, 'group': 13, 'period': 4},
            'Ge': {'electronegativity': 2.01, 'atomic_radius': 125, 'group': 14, 'period': 4},
            'As': {'electronegativity': 2.18, 'atomic_radius': 114, 'group': 15, 'period': 4},
            'Se': {'electronegativity': 2.55, 'atomic_radius': 103, 'group': 16, 'period': 4},
            'Br': {'electronegativity': 2.96, 'atomic_radius': 94, 'group': 17, 'period': 4},
            'Sr': {'electronegativity': 0.95, 'atomic_radius': 219, 'group': 2, 'period': 5},
            'Y': {'electronegativity': 1.22, 'atomic_radius': 212, 'group': 3, 'period': 5},
            'Zr': {'electronegativity': 1.33, 'atomic_radius': 206, 'group': 4, 'period': 5},
            'Nb': {'electronegativity': 1.60, 'atomic_radius': 198, 'group': 5, 'period': 5},
            'Mo': {'electronegativity': 2.16, 'atomic_radius': 190, 'group': 6, 'period': 5},
            'Ag': {'electronegativity': 1.93, 'atomic_radius': 165, 'group': 11, 'period': 5},
            'Cd': {'electronegativity': 1.69, 'atomic_radius': 161, 'group': 12, 'period': 5},
            'In': {'electronegativity': 1.78, 'atomic_radius': 156, 'group': 13, 'period': 5},
            'Sn': {'electronegativity': 1.96, 'atomic_radius': 145, 'group': 14, 'period': 5},
            'Sb': {'electronegativity': 2.05, 'atomic_radius': 133, 'group': 15, 'period': 5},
            'Te': {'electronegativity': 2.10, 'atomic_radius': 123, 'group': 16, 'period': 5},
            'I': {'electronegativity': 2.66, 'atomic_radius': 115, 'group': 17, 'period': 5},
            'Ba': {'electronegativity': 0.89, 'atomic_radius': 253, 'group': 2, 'period': 6},
            'La': {'electronegativity': 1.10, 'atomic_radius': 195, 'group': 3, 'period': 6},
            'Pb': {'electronegativity': 2.33, 'atomic_radius': 154, 'group': 14, 'period': 6},
            'Bi': {'electronegativity': 2.02, 'atomic_radius': 143, 'group': 15, 'period': 6},
        }
        return data
    
    def correlate_importance_with_physics(self, elements, importance_scores):
        """
        åˆ†æåŸå­é‡è¦æ€§ä¸ç‰©ç†åŒ–å­¦ç‰¹å¾çš„å…³è”
        
        Args:
            elements: å…ƒç´ åˆ—è¡¨
            importance_scores: é‡è¦æ€§åˆ†æ•°
            
        Returns:
            correlations: ç‰©ç†é‡ä¸é‡è¦æ€§çš„ç›¸å…³æ€§
        """
        # æ”¶é›†ç‰©ç†åŒ–å­¦ç‰¹å¾
        physics_features = defaultdict(list)
        valid_importance = []
        
        for elem, imp in zip(elements, importance_scores):
            if elem in self.element_data:
                for key, value in self.element_data[elem].items():
                    physics_features[key].append(value)
                valid_importance.append(imp)
                
        if len(valid_importance) < 3:
            return {}
            
        # è®¡ç®—ç›¸å…³æ€§
        correlations = {}
        for key, values in physics_features.items():
            if len(values) == len(valid_importance):
                corr = np.corrcoef(values, valid_importance)[0, 1]
                correlations[key] = float(corr) if not np.isnan(corr) else 0.0
                
        return correlations
    
    def analyze_structure_property_relation(self, atoms_object, prediction, importance_scores):
        """
        åˆ†æç»“æ„-æ€§è´¨å…³ç³»
        
        Args:
            atoms_object: JARVIS Atomså¯¹è±¡
            prediction: æ¨¡å‹é¢„æµ‹å€¼
            importance_scores: åŸå­é‡è¦æ€§
            
        Returns:
            analysis: ç»“æ„-æ€§è´¨åˆ†æç»“æœ
        """
        analysis = {}
        
        elements = list(atoms_object.elements)
        coords = atoms_object.cart_coords
        
        # å…ƒç´ ç»Ÿè®¡
        unique_elements = list(set(elements))
        element_importance = {}
        for elem in unique_elements:
            indices = [i for i, e in enumerate(elements) if e == elem]
            element_importance[elem] = float(np.mean([importance_scores[i] for i in indices]))
        analysis['element_importance'] = element_importance
        
        # æŒ‰é‡è¦æ€§æ’åºçš„å…ƒç´ 
        sorted_elements = sorted(element_importance.items(), key=lambda x: x[1], reverse=True)
        analysis['most_important_element'] = list(sorted_elements[0]) if sorted_elements else None
        
        # ç©ºé—´åˆ†å¸ƒåˆ†æ
        if len(coords) > 0:
            # é‡å¿ƒ
            center = coords.mean(axis=0)
            
            # é‡è¦åŸå­çš„ç©ºé—´åˆ†å¸ƒ
            top_k = min(5, len(importance_scores))
            top_indices = np.argsort(importance_scores)[-top_k:]
            top_coords = coords[top_indices]
            
            # é‡è¦åŸå­æ˜¯å¦é›†ä¸­åœ¨æŸåŒºåŸŸ
            if len(top_coords) > 1:
                spread = np.std(top_coords, axis=0).mean()
                analysis['important_atoms_spread'] = float(spread)
                
        # é…ä½ç¯å¢ƒåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        analysis['num_atoms'] = len(elements)
        analysis['num_elements'] = len(unique_elements)
        analysis['prediction'] = safe_to_float(prediction)
        
        return analysis


class UncertaintyEstimator:
    """ä¸ç¡®å®šæ€§ä¼°è®¡å™¨"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        
    def mc_dropout_uncertainty(self, g, lg, text, n_samples=30):
        """
        Monte Carlo Dropout ä¸ç¡®å®šæ€§ä¼°è®¡
        
        Args:
            g, lg, text: è¾“å…¥
            n_samples: MCé‡‡æ ·æ¬¡æ•°
            
        Returns:
            mean: é¢„æµ‹å‡å€¼
            std: é¢„æµ‹æ ‡å‡†å·®ï¼ˆä¸ç¡®å®šæ€§ï¼‰
            samples: æ‰€æœ‰é‡‡æ ·é¢„æµ‹
        """
        g = g.to(self.device)
        lg = lg.to(self.device)
        
        # å¯ç”¨dropoutï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
        self.model.train()
        
        predictions = []
        for _ in range(n_samples):
            with torch.no_grad():
                output = self.model([g, lg, text])
                if isinstance(output, dict):
                    pred = output['predictions'].cpu().numpy()
                else:
                    pred = output.cpu().numpy()
                predictions.append(pred)
                
        # æ¢å¤è¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        predictions = np.array(predictions)
        mean = predictions.mean(axis=0)
        std = predictions.std(axis=0)
        
        return mean, std, predictions
    
    def feature_space_uncertainty(self, g, lg, text, reference_features=None):
        """
        åŸºäºç‰¹å¾ç©ºé—´çš„ä¸ç¡®å®šæ€§ä¼°è®¡
        æ£€æµ‹è¾“å…¥æ˜¯å¦åœ¨è®­ç»ƒåˆ†å¸ƒå†…
        """
        g = g.to(self.device)
        lg = lg.to(self.device)
        
        with torch.no_grad():
            output = self.model([g, lg, text], return_features=True)
            
        if isinstance(output, dict):
            graph_feat = output.get('graph_features', None)
            text_feat = output.get('text_features', None)
            
            if graph_feat is not None:
                feature_norm = float(torch.norm(graph_feat).item())
                
                # å¦‚æœæœ‰å‚è€ƒç‰¹å¾åˆ†å¸ƒï¼Œè®¡ç®—é©¬æ°è·ç¦»
                if reference_features is not None:
                    # ç®€åŒ–ï¼šä½¿ç”¨æ¬§æ°è·ç¦»åˆ°å‚è€ƒä¸­å¿ƒ
                    ref_center = reference_features.mean(dim=0)
                    distance = float(torch.norm(graph_feat - ref_center).item())
                    return {'feature_norm': feature_norm, 'distance_to_center': distance}
                    
                return {'feature_norm': feature_norm}
                
        return {}


class InterpretabilityVisualizer:
    """å¯è§£é‡Šæ€§å¯è§†åŒ–å·¥å…·"""
    
    @staticmethod
    def plot_atom_importance_3d(atoms_object, importance_scores, save_path=None, 
                                 title="Atom Importance Visualization"):
        """
        3DåŸå­é‡è¦æ€§å¯è§†åŒ–
        """
        from mpl_toolkits.mplot3d import Axes3D
        
        coords = atoms_object.cart_coords
        elements = list(atoms_object.elements)
        
        # å½’ä¸€åŒ–é‡è¦æ€§
        imp_norm = (importance_scores - importance_scores.min()) / \
                   (importance_scores.max() - importance_scores.min() + 1e-8)
        
        fig = plt.figure(figsize=(12, 10))
        ax = fig.add_subplot(111, projection='3d')
        
        # é¢œè‰²æ˜ å°„
        colors = plt.cm.YlOrRd(imp_norm)
        
        # ç»˜åˆ¶åŸå­
        scatter = ax.scatter(
            coords[:, 0], coords[:, 1], coords[:, 2],
            c=imp_norm, cmap='YlOrRd', s=500, alpha=0.8,
            edgecolors='black', linewidth=1
        )
        
        # æ ‡æ³¨å…ƒç´ ç¬¦å·
        for i, (coord, elem) in enumerate(zip(coords, elements)):
            ax.text(coord[0], coord[1], coord[2], elem, 
                   fontsize=10, fontweight='bold', ha='center', va='center')
            
        ax.set_xlabel('X (Ã…)', fontsize=12)
        ax.set_ylabel('Y (Ã…)', fontsize=12)
        ax.set_zlabel('Z (Ã…)', fontsize=12)
        ax.set_title(title, fontsize=14, fontweight='bold')
        
        # é¢œè‰²æ¡
        cbar = plt.colorbar(scatter, ax=ax, shrink=0.6, label='Importance')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… 3Då¯è§†åŒ–å·²ä¿å­˜: {save_path}")
            
        plt.close()
        
    @staticmethod
    def plot_attention_heatmap(attention_weights, row_labels, col_labels, 
                               title="Attention Heatmap", save_path=None):
        """
        æ³¨æ„åŠ›çƒ­å›¾å¯è§†åŒ–
        """
        fig, ax = plt.subplots(figsize=(14, 10))
        
        # ç¡®ä¿æ•°æ®æ˜¯2D
        if attention_weights.ndim > 2:
            attention_weights = attention_weights.mean(axis=tuple(range(attention_weights.ndim - 2)))
            
        sns.heatmap(
            attention_weights,
            xticklabels=col_labels,
            yticklabels=row_labels,
            cmap='YlOrRd',
            ax=ax,
            cbar_kws={'label': 'Attention Weight'}
        )
        
        ax.set_title(title, fontsize=14, fontweight='bold')
        plt.xticks(rotation=45, ha='right', fontsize=8)
        plt.yticks(fontsize=8)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… çƒ­å›¾å·²ä¿å­˜: {save_path}")
            
        plt.close()
        
    @staticmethod
    def plot_modal_contribution(contributions, save_path=None):
        """
        æ¨¡æ€è´¡çŒ®é¥¼å›¾
        """
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # å·¦å›¾ï¼šè´¡çŒ®æ¯”ä¾‹
        if 'graph_contribution' in contributions and 'text_contribution' in contributions:
            ax1 = axes[0]
            sizes = [contributions['graph_contribution'], contributions['text_contribution']]
            labels = ['Graph\n(Structure)', 'Text\n(Description)']
            colors = ['#3498db', '#e74c3c']
            explode = (0.05, 0.05)
            
            wedges, texts, autotexts = ax1.pie(
                sizes, explode=explode, labels=labels, colors=colors,
                autopct='%1.1f%%', shadow=True, startangle=90,
                textprops={'fontsize': 12}
            )
            ax1.set_title('Modal Contribution', fontsize=14, fontweight='bold')
            
        # å³å›¾ï¼šç‰¹å¾å¯¹é½åº¦
        ax2 = axes[1]
        metrics = ['Feature\nAlignment', 'Graph\nNorm', 'Text\nNorm']
        values = [
            contributions.get('feature_alignment', 0),
            contributions.get('graph_contribution', 0),
            contributions.get('text_contribution', 0)
        ]
        colors = ['#2ecc71', '#3498db', '#e74c3c']
        
        bars = ax2.bar(metrics, values, color=colors, edgecolor='black', linewidth=1.5)
        ax2.set_ylabel('Value', fontsize=12)
        ax2.set_title('Feature Metrics', fontsize=14, fontweight='bold')
        ax2.set_ylim(0, 1.1)
        
        for bar, val in zip(bars, values):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                    f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')
            
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… æ¨¡æ€è´¡çŒ®å›¾å·²ä¿å­˜: {save_path}")
            
        plt.close()
        
    @staticmethod
    def plot_physics_correlation(correlations, save_path=None):
        """
        ç‰©ç†ç‰¹å¾ä¸é‡è¦æ€§çš„ç›¸å…³æ€§
        """
        if not correlations:
            print("âš ï¸ æ²¡æœ‰ç›¸å…³æ€§æ•°æ®")
            return
            
        fig, ax = plt.subplots(figsize=(10, 6))
        
        properties = list(correlations.keys())
        values = list(correlations.values())
        colors = ['#e74c3c' if v < 0 else '#2ecc71' for v in values]
        
        bars = ax.barh(properties, values, color=colors, edgecolor='black', linewidth=1)
        
        ax.axvline(x=0, color='black', linewidth=1)
        ax.set_xlabel('Correlation with Importance', fontsize=12)
        ax.set_title('Physics-Importance Correlation', fontsize=14, fontweight='bold')
        ax.set_xlim(-1, 1)
        
        for bar, val in zip(bars, values):
            ax.text(val + 0.02 if val >= 0 else val - 0.02,
                   bar.get_y() + bar.get_height()/2,
                   f'{val:.3f}', ha='left' if val >= 0 else 'right',
                   va='center', fontsize=10)
            
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… ç‰©ç†ç›¸å…³æ€§å›¾å·²ä¿å­˜: {save_path}")
            
        plt.close()


class ComprehensiveExplainer:
    """ç»¼åˆè§£é‡Šå™¨ - æ•´åˆæ‰€æœ‰åˆ†æåŠŸèƒ½"""
    
    def __init__(self, model, tokenizer=None, device='cuda'):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
        # åˆå§‹åŒ–å„åˆ†æå™¨
        self.atom_analyzer = AtomImportanceAnalyzer(model, device)
        self.text_analyzer = TextTokenAnalyzer(model, tokenizer, device) if tokenizer else None
        self.cross_modal_analyzer = CrossModalInteractionAnalyzer(model, device)
        self.physics_analyzer = PhysicsCorrelationAnalyzer()
        self.uncertainty_estimator = UncertaintyEstimator(model, device)
        self.visualizer = InterpretabilityVisualizer()
        
        # å°è¯•å¯¼å…¥å±€éƒ¨ç¯å¢ƒåˆ†æå™¨
        try:
            from local_environment_analyzer import (
                LocalEnvironmentAnalyzer, 
                LocalEnvironmentVisualizer,
                EnhancedAttentionVisualizer
            )
            self.local_env_analyzer = LocalEnvironmentAnalyzer()
            self.local_env_visualizer = LocalEnvironmentVisualizer()
            self.enhanced_attn_visualizer = EnhancedAttentionVisualizer()
            self._has_local_env = True
        except ImportError:
            self._has_local_env = False
            print("âš ï¸ å±€éƒ¨ç¯å¢ƒåˆ†ææ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡å±€éƒ¨ç¯å¢ƒåˆ†æ")
        
    def explain_prediction(self, g, lg, text, atoms_object, 
                          true_value=None, save_dir=None, sample_id='sample'):
        """
        ä¸ºå•ä¸ªé¢„æµ‹ç”Ÿæˆå®Œæ•´è§£é‡ŠæŠ¥å‘Š
        
        Args:
            g, lg, text: æ¨¡å‹è¾“å…¥
            atoms_object: JARVIS Atomså¯¹è±¡
            true_value: çœŸå®å€¼ï¼ˆå¯é€‰ï¼‰
            save_dir: ä¿å­˜ç›®å½•
            sample_id: æ ·æœ¬ID
            
        Returns:
            explanation: å®Œæ•´è§£é‡Šå­—å…¸
        """
        if save_dir:
            save_dir = Path(save_dir)
            save_dir.mkdir(parents=True, exist_ok=True)
            
        print(f"\n{'='*80}")
        print(f"ğŸ”¬ æ ·æœ¬ {sample_id} çš„ç»¼åˆå¯è§£é‡Šæ€§åˆ†æ")
        print(f"{'='*80}")
        
        explanation = {'sample_id': sample_id}
        
        # ==================== 1. åŸºç¡€é¢„æµ‹ ====================
        print("\nğŸ“Š [1/7] è·å–é¢„æµ‹ç»“æœ...")
        
        g_device = g.to(self.device)
        lg_device = lg.to(self.device)
        
        with torch.no_grad():
            output = self.model(
                [g_device, lg_device, text],
                return_features=True,
                return_attention=True,
                return_intermediate_features=True
            )
            
        if isinstance(output, dict):
            prediction = output['predictions'].cpu().numpy()
        else:
            prediction = output.cpu().numpy()
        
        pred_value = safe_to_float(prediction)
        explanation['prediction'] = pred_value
        explanation['true_value'] = float(true_value) if true_value is not None else None
        
        if true_value is not None:
            error = abs(pred_value - float(true_value))
            explanation['error'] = error
            print(f"   é¢„æµ‹å€¼: {pred_value:.4f}")
            print(f"   çœŸå®å€¼: {true_value:.4f}")
            print(f"   è¯¯å·®: {error:.4f}")
        else:
            print(f"   é¢„æµ‹å€¼: {pred_value:.4f}")
            
        # ==================== 2. åŸå­é‡è¦æ€§åˆ†æ ====================
        print("\nâš›ï¸  [2/7] è®¡ç®—åŸå­é‡è¦æ€§...")
        
        # æ¢¯åº¦æ³•
        importance_grad, gradients = self.atom_analyzer.gradient_importance(g, lg, text)
        explanation['atom_importance_gradient'] = importance_grad.tolist()
        
        # ç§¯åˆ†æ¢¯åº¦æ³•
        importance_ig, attributions = self.atom_analyzer.integrated_gradients(g, lg, text, steps=30)
        explanation['atom_importance_integrated_gradients'] = importance_ig.tolist()
        
        # é€‰æ‹©ç§¯åˆ†æ¢¯åº¦ä½œä¸ºä¸»è¦é‡è¦æ€§
        primary_importance = importance_ig
        
        # å¯è§†åŒ–
        elements = list(atoms_object.elements)
        print(f"   åˆ†æäº† {len(elements)} ä¸ªåŸå­")
        
        # æ˜¾ç¤ºtop-5é‡è¦åŸå­
        top_k = min(5, len(elements))
        top_indices = np.argsort(primary_importance)[-top_k:][::-1]
        print(f"   Top-{top_k} é‡è¦åŸå­:")
        for rank, idx in enumerate(top_indices, 1):
            print(f"     {rank}. {elements[idx]} (index {idx}): {primary_importance[idx]:.4f}")
            
        if save_dir:
            # 2Då¯è§†åŒ–
            self._plot_atom_importance_2d(
                atoms_object, primary_importance,
                save_path=save_dir / f'{sample_id}_atom_importance.png'
            )
            
        # ==================== 3. ç‰©ç†åŒ–å­¦å…³è”åˆ†æ ====================
        print("\nğŸ§ª [3/7] åˆ†æç‰©ç†åŒ–å­¦å…³è”...")
        
        correlations = self.physics_analyzer.correlate_importance_with_physics(
            elements, primary_importance
        )
        explanation['physics_correlations'] = correlations
        
        if correlations:
            print("   é‡è¦æ€§ä¸ç‰©ç†é‡çš„ç›¸å…³æ€§:")
            for prop, corr in sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True):
                print(f"     {prop}: {corr:+.3f}")
                
            if save_dir:
                self.visualizer.plot_physics_correlation(
                    correlations,
                    save_path=save_dir / f'{sample_id}_physics_correlation.png'
                )
                
        # ç»“æ„-æ€§è´¨åˆ†æ
        struct_analysis = self.physics_analyzer.analyze_structure_property_relation(
            atoms_object, pred_value, primary_importance
        )
        explanation['structure_analysis'] = struct_analysis
        
        # ==================== 3.5 å±€éƒ¨åŒ–å­¦ç¯å¢ƒåˆ†æ ====================
        if self._has_local_env:
            print("\nğŸ”¬ [3.5/7] åˆ†æå±€éƒ¨åŒ–å­¦ç¯å¢ƒ...")
            
            local_env = self.local_env_analyzer.analyze_local_environment(
                atoms_object, primary_importance
            )
            explanation['local_environment'] = local_env
            
            coord_data = local_env.get('coordination', {})
            bond_data = local_env.get('bonds', {})
            
            print(f"   å¹³å‡é…ä½æ•°: {coord_data.get('mean_coordination', 0):.2f}")
            print(f"   é”®ç±»å‹æ•°: {len(bond_data.get('bond_types', []))}")
            print(f"   æ€»æˆé”®æ•°: {bond_data.get('total_bonds', 0)}")
            
            # é…ä½æ•°ä¸é‡è¦æ€§ç›¸å…³æ€§
            env_corr = local_env.get('environment_importance_correlation', {})
            if 'coordination_importance_correlation' in env_corr:
                print(f"   é…ä½æ•°-é‡è¦æ€§ç›¸å…³æ€§: {env_corr['coordination_importance_correlation']:+.3f}")
                
            if save_dir:
                self.local_env_visualizer.plot_coordination_analysis(
                    local_env,
                    primary_importance,
                    save_path=save_dir / f'{sample_id}_local_environment.png'
                )
        
        # ==================== 4. è·¨æ¨¡æ€äº¤äº’åˆ†æ ====================
        print("\nğŸ”— [4/7] åˆ†æè·¨æ¨¡æ€äº¤äº’...")
        
        modal_contribution = self.cross_modal_analyzer.analyze_modal_contribution(g, lg, text)
        explanation['modal_contribution'] = modal_contribution
        
        print(f"   å›¾æ¨¡æ€è´¡çŒ®: {modal_contribution.get('graph_contribution', 0):.1%}")
        print(f"   æ–‡æœ¬æ¨¡æ€è´¡çŒ®: {modal_contribution.get('text_contribution', 0):.1%}")
        if 'feature_alignment' in modal_contribution:
            print(f"   ç‰¹å¾å¯¹é½åº¦: {modal_contribution['feature_alignment']:.3f}")
            
        if save_dir:
            self.visualizer.plot_modal_contribution(
                modal_contribution,
                save_path=save_dir / f'{sample_id}_modal_contribution.png'
            )
            
        # æ³¨æ„åŠ›æµåˆ†æ
        flow_analysis = self.cross_modal_analyzer.attention_flow_analysis(g, lg, text)
        explanation['attention_flow'] = flow_analysis
        
        # ==================== 5. æ³¨æ„åŠ›æƒé‡åˆ†æ ====================
        print("\nğŸ‘ï¸  [5/7] åˆ†ææ³¨æ„åŠ›æƒé‡...")
        
        if isinstance(output, dict):
            # å…¨å±€æ³¨æ„åŠ›
            if 'attention_weights' in output and output['attention_weights']:
                attn = output['attention_weights']
                explanation['global_attention'] = {
                    k: v.cpu().numpy().tolist() if v is not None else None
                    for k, v in attn.items()
                }
                print("   âœ“ å…¨å±€è·¨æ¨¡æ€æ³¨æ„åŠ›å·²æå–")
                
            # ç»†ç²’åº¦æ³¨æ„åŠ›
            if 'fine_grained_attention_weights' in output and output['fine_grained_attention_weights']:
                fg_attn = output['fine_grained_attention_weights']
                
                # åˆ†æåŸå­-tokenæ³¨æ„åŠ›
                if 'atom_to_text' in fg_attn and fg_attn['atom_to_text'] is not None:
                    a2t = fg_attn['atom_to_text'][0].cpu().numpy()  # [heads, atoms, tokens]
                    
                    # ä¿å­˜åŸå§‹å¤šå¤´æ³¨æ„åŠ›
                    explanation['atom_to_text_attention'] = a2t.tolist()
                    
                    # å¹³å‡è·¨å¤´
                    a2t_mean = a2t.mean(axis=0)  # [atoms, tokens]
                    
                    # æ¯ä¸ªåŸå­æœ€å…³æ³¨çš„tokenä½ç½®
                    atom_focus = a2t_mean.argmax(axis=1)
                    explanation['atom_text_focus'] = atom_focus.tolist()
                    
                    print(f"   âœ“ ç»†ç²’åº¦æ³¨æ„åŠ›å·²æå– (shape: {a2t.shape})")
                    
                    if save_dir:
                        # ä½¿ç”¨å¢å¼ºçš„æ³¨æ„åŠ›å¯è§†åŒ–
                        if self._has_local_env:
                            # å‡†å¤‡tokenæ ‡ç­¾
                            if self.tokenizer and len(text) > 0:
                                try:
                                    tokens = self.tokenizer.tokenize(text[0])[:a2t.shape[2]]
                                    token_labels = tokens
                                except:
                                    token_labels = [f'T{i}' for i in range(a2t.shape[2])]
                            else:
                                token_labels = [f'T{i}' for i in range(a2t.shape[2])]
                            
                            # å¢å¼ºçš„æ³¨æ„åŠ›çƒ­å›¾
                            self.enhanced_attn_visualizer.plot_atom_text_attention_enhanced(
                                a2t,  # å¤šå¤´æ³¨æ„åŠ›
                                atom_labels=[f"{elements[i]}" for i in range(len(elements))],
                                token_labels=token_labels,
                                importance_scores=primary_importance,
                                save_path=save_dir / f'{sample_id}_atom_text_attention_enhanced.png'
                            )
                            
                            # å¤šå¤´æ³¨æ„åŠ›åˆ†è§£å›¾
                            if a2t.shape[0] > 1:  # å¦‚æœæœ‰å¤šå¤´
                                self.enhanced_attn_visualizer.plot_multi_head_attention(
                                    a2t,
                                    atom_labels=[f"{elements[i]}" for i in range(len(elements))],
                                    token_labels=token_labels,
                                    save_path=save_dir / f'{sample_id}_multi_head_attention.png'
                                )
                                
                            # æ³¨æ„åŠ›æµå›¾
                            self.enhanced_attn_visualizer.plot_attention_flow_sankey(
                                a2t_mean,
                                atom_labels=[f"{elements[i]}_{i}" for i in range(len(elements))],
                                token_labels=token_labels,
                                save_path=save_dir / f'{sample_id}_attention_flow.png'
                            )
                        else:
                            # ä½¿ç”¨åŸå§‹çš„ç®€åŒ–çƒ­å›¾
                            top_atoms = min(10, len(elements))
                            top_atom_indices = np.argsort(primary_importance)[-top_atoms:]
                            
                            self.visualizer.plot_attention_heatmap(
                                a2t_mean[top_atom_indices],
                                row_labels=[f"{elements[i]}_{i}" for i in top_atom_indices],
                                col_labels=[f"T{i}" for i in range(a2t_mean.shape[1])],
                                title="Atom-to-Text Attention (Top Atoms)",
                                save_path=save_dir / f'{sample_id}_atom_text_attention.png'
                            )
                        
        # ==================== 6. ä¸ç¡®å®šæ€§ä¼°è®¡ ====================
        print("\nğŸ“‰ [6/7] ä¼°è®¡é¢„æµ‹ä¸ç¡®å®šæ€§...")
        
        mean_pred, std_pred, mc_samples = self.uncertainty_estimator.mc_dropout_uncertainty(
            g, lg, text, n_samples=20
        )
        
        mean_val = safe_to_float(mean_pred)
        std_val = safe_to_float(std_pred)
        
        explanation['uncertainty'] = {
            'mean': mean_val,
            'std': std_val,
            'confidence_interval_95': [
                mean_val - 1.96 * std_val,
                mean_val + 1.96 * std_val
            ] if mean_val is not None else None
        }
        
        print(f"   é¢„æµ‹å‡å€¼: {mean_val:.4f}")
        print(f"   é¢„æµ‹æ ‡å‡†å·®: {std_val:.4f}")
        print(f"   95%ç½®ä¿¡åŒºé—´: [{mean_val - 1.96*std_val:.4f}, "
              f"{mean_val + 1.96*std_val:.4f}]")
        
        # ==================== 7. ç”ŸæˆæŠ¥å‘Š ====================
        print("\nğŸ“ [7/7] ç”Ÿæˆè§£é‡ŠæŠ¥å‘Š...")
        
        if save_dir:
            # ä¿å­˜JSONæŠ¥å‘Š
            report_path = save_dir / f'{sample_id}_explanation.json'
            
            # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ï¼ˆå¢å¼ºç‰ˆï¼Œå¤„ç†æ‰€æœ‰numpyç±»å‹ï¼‰
            def convert_to_serializable(obj):
                if obj is None:
                    return None
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, (np.floating, np.float32, np.float64)):
                    return float(obj)
                elif isinstance(obj, (np.integer, np.int32, np.int64)):
                    return int(obj)
                elif isinstance(obj, np.bool_):
                    return bool(obj)
                elif isinstance(obj, dict):
                    return {k: convert_to_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, (list, tuple)):
                    return [convert_to_serializable(i) for i in obj]
                elif isinstance(obj, (int, float, str, bool)):
                    return obj
                elif hasattr(obj, 'item'):  # å¤„ç†0ç»´æ•°ç»„æˆ–å•å…ƒç´ tensor
                    return obj.item()
                else:
                    try:
                        return float(obj)
                    except (TypeError, ValueError):
                        return str(obj)
                    
            serializable_explanation = convert_to_serializable(explanation)
            
            with open(report_path, 'w') as f:
                json.dump(serializable_explanation, f, indent=2)
            print(f"   âœ“ JSONæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
            # ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
            summary_path = save_dir / f'{sample_id}_summary.txt'
            self._generate_text_summary(explanation, summary_path)
            print(f"   âœ“ æ–‡æœ¬æ‘˜è¦å·²ä¿å­˜: {summary_path}")
            
        print(f"\n{'='*80}")
        print(f"âœ… å¯è§£é‡Šæ€§åˆ†æå®Œæˆ!")
        print(f"{'='*80}\n")
        
        return explanation
    
    def _plot_atom_importance_2d(self, atoms_object, importance_scores, save_path=None):
        """2DåŸå­é‡è¦æ€§å¯è§†åŒ–"""
        coords = atoms_object.cart_coords
        elements = list(atoms_object.elements)
        
        # å½’ä¸€åŒ–
        imp_norm = (importance_scores - importance_scores.min()) / \
                   (importance_scores.max() - importance_scores.min() + 1e-8)
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        projections = [
            (0, 1, 'X', 'Y', 'X-Y Projection'),
            (0, 2, 'X', 'Z', 'X-Z Projection'),
            (1, 2, 'Y', 'Z', 'Y-Z Projection')
        ]
        
        for ax, (i, j, xlabel, ylabel, title) in zip(axes, projections):
            scatter = ax.scatter(
                coords[:, i], coords[:, j],
                c=imp_norm, cmap='YlOrRd', s=300, alpha=0.8,
                edgecolors='black', linewidth=1.5
            )
            
            # æ ‡æ³¨å…ƒç´ 
            for k, (coord, elem) in enumerate(zip(coords, elements)):
                ax.annotate(
                    f"{elem}",
                    (coord[i], coord[j]),
                    fontsize=9, fontweight='bold',
                    ha='center', va='center'
                )
                
            ax.set_xlabel(f'{xlabel} (Ã…)', fontsize=11)
            ax.set_ylabel(f'{ylabel} (Ã…)', fontsize=11)
            ax.set_title(title, fontsize=12, fontweight='bold')
            ax.grid(True, alpha=0.3, linestyle='--')
            
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(scatter, ax=axes, shrink=0.8, label='Importance')
        
        plt.suptitle('Atom Importance - Spatial Distribution', fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            
        plt.close()
        
    def _generate_text_summary(self, explanation, save_path):
        """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦"""
        lines = []
        lines.append("=" * 80)
        lines.append("å¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š")
        lines.append("=" * 80)
        lines.append("")
        
        # é¢„æµ‹ä¿¡æ¯
        lines.append("ã€é¢„æµ‹ç»“æœã€‘")
        lines.append(f"  é¢„æµ‹å€¼: {explanation.get('prediction', 'N/A')}")
        if explanation.get('true_value') is not None:
            lines.append(f"  çœŸå®å€¼: {explanation['true_value']}")
            lines.append(f"  è¯¯å·®: {explanation.get('error', 'N/A')}")
        lines.append("")
        
        # ä¸ç¡®å®šæ€§
        if 'uncertainty' in explanation:
            lines.append("ã€ä¸ç¡®å®šæ€§ä¼°è®¡ã€‘")
            unc = explanation['uncertainty']
            lines.append(f"  é¢„æµ‹å‡å€¼: {unc.get('mean', 'N/A')}")
            lines.append(f"  æ ‡å‡†å·®: {unc.get('std', 'N/A')}")
            if unc.get('confidence_interval_95'):
                ci = unc['confidence_interval_95']
                lines.append(f"  95%ç½®ä¿¡åŒºé—´: [{ci[0]:.4f}, {ci[1]:.4f}]")
            lines.append("")
            
        # æ¨¡æ€è´¡çŒ®
        if 'modal_contribution' in explanation:
            lines.append("ã€æ¨¡æ€è´¡çŒ®ã€‘")
            mc = explanation['modal_contribution']
            lines.append(f"  å›¾æ¨¡æ€: {mc.get('graph_contribution', 0):.1%}")
            lines.append(f"  æ–‡æœ¬æ¨¡æ€: {mc.get('text_contribution', 0):.1%}")
            if 'feature_alignment' in mc:
                lines.append(f"  ç‰¹å¾å¯¹é½åº¦: {mc['feature_alignment']:.3f}")
            lines.append("")
            
        # ç‰©ç†å…³è”
        if 'physics_correlations' in explanation and explanation['physics_correlations']:
            lines.append("ã€ç‰©ç†åŒ–å­¦å…³è”ã€‘")
            for prop, corr in sorted(explanation['physics_correlations'].items(), 
                                    key=lambda x: abs(x[1]), reverse=True):
                lines.append(f"  {prop}: {corr:+.3f}")
            lines.append("")
            
        # ç»“æ„åˆ†æ
        if 'structure_analysis' in explanation:
            lines.append("ã€ç»“æ„åˆ†æã€‘")
            sa = explanation['structure_analysis']
            lines.append(f"  åŸå­æ•°: {sa.get('num_atoms', 'N/A')}")
            lines.append(f"  å…ƒç´ ç§ç±»: {sa.get('num_elements', 'N/A')}")
            if sa.get('most_important_element'):
                elem, imp = sa['most_important_element']
                lines.append(f"  æœ€é‡è¦å…ƒç´ : {elem} (é‡è¦æ€§: {imp:.4f})")
            lines.append("")
            
        lines.append("=" * 80)
        
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

    def extract_symbolic_features(self, test_loader, save_dir=None, max_samples=None):
        """
        æå–ç‰¹å¾ç”¨äºç¬¦å·å›å½’åˆ†æ

        ä½¿ç”¨PySRåº“ä»æ¨¡å‹ç‰¹å¾ä¸­å‘ç°å¯è§£é‡Šçš„ç¬¦å·å…¬å¼ï¼Œå°†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„
        é»‘ç›’é¢„æµ‹è½¬åŒ–ä¸ºå¯ç†è§£çš„æ•°å­¦è¡¨è¾¾å¼ã€‚

        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            save_dir: ä¿å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æ ·æœ¬ï¼‰

        Returns:
            model_sr: è®­ç»ƒå¥½çš„PySRå›å½’å™¨
            results: åŒ…å«å…¬å¼å’Œè¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        try:
            import pysr
        except ImportError:
            print("âš ï¸ PySRæœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install pysr")
            print("   æ³¨æ„: PySRéœ€è¦Juliaç¯å¢ƒã€‚è¯¦è§: https://github.com/MilesCranmer/PySR")
            return None, None

        print("\n" + "="*80)
        print("ğŸ”¬ ç¬¦å·å›å½’åˆ†æ - ä»ç¥ç»ç½‘ç»œç‰¹å¾ä¸­å‘ç°æ•°å­¦å…¬å¼")
        print("="*80)

        all_features = []
        all_targets = []
        all_predictions = []

        print("\nğŸ“Š [1/3] æå–æ¨¡å‹ç‰¹å¾...")

        self.model.eval()
        sample_count = 0

        with torch.no_grad():
            for batch in tqdm(test_loader, desc="æå–ç‰¹å¾"):
                if max_samples is not None and sample_count >= max_samples:
                    break

                g, lg, text, target = batch

                # å‰å‘ä¼ æ’­ï¼Œè·å–ç‰¹å¾
                output = self.model(
                    [g.to(self.device), lg.to(self.device), text],
                    return_features=True
                )

                # æå–å›¾ç‰¹å¾ï¼ˆæœ€å…·ä»£è¡¨æ€§çš„ç»“æ„ç‰¹å¾ï¼‰
                if isinstance(output, dict):
                    # ä¼˜å…ˆä½¿ç”¨èåˆåçš„å›¾ç‰¹å¾
                    if 'graph_features' in output and output['graph_features'] is not None:
                        features = output['graph_features'].cpu().numpy()
                    elif 'graph_cross' in output and output['graph_cross'] is not None:
                        features = output['graph_cross'].cpu().numpy()
                    else:
                        print("âš ï¸ æœªæ‰¾åˆ°å›¾ç‰¹å¾ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue

                    # è·å–é¢„æµ‹
                    pred = output['predictions'].cpu().numpy()
                else:
                    print("âš ï¸ æ¨¡å‹è¾“å‡ºæ ¼å¼ä¸æ”¯æŒç‰¹å¾æå–")
                    continue

                all_features.append(features)
                all_targets.append(target.numpy())
                all_predictions.append(pred)
                sample_count += len(target)

        if len(all_features) == 0:
            print("âŒ æœªæå–åˆ°ä»»ä½•ç‰¹å¾ï¼")
            return None, None

        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        X = np.vstack(all_features)
        y = np.concatenate(all_targets)
        y_pred_nn = np.concatenate(all_predictions)

        print(f"   âœ“ æå–äº† {len(y)} ä¸ªæ ·æœ¬")
        print(f"   âœ“ ç‰¹å¾ç»´åº¦: {X.shape[1]}")

        # ==================== ç¬¦å·å›å½’ ====================
        print("\nğŸ§® [2/3] è¿è¡Œç¬¦å·å›å½’...")
        print("   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")

        # é…ç½®PySR
        model_sr = pysr.PySRRegressor(
            niterations=100,  # è¿­ä»£æ¬¡æ•°
            binary_operators=["+", "-", "*", "/", "^"],  # äºŒå…ƒè¿ç®—ç¬¦
            unary_operators=[
                "exp",   # æŒ‡æ•°
                "log",   # å¯¹æ•°
                "sqrt",  # å¹³æ–¹æ ¹
                "abs",   # ç»å¯¹å€¼
            ],
            maxsize=20,  # æœ€å¤§å…¬å¼å¤æ‚åº¦
            populations=15,  # ç§ç¾¤æ•°é‡
            population_size=33,  # æ¯ä¸ªç§ç¾¤çš„å¤§å°
            ncyclesperiteration=550,  # æ¯æ¬¡è¿­ä»£çš„å¾ªç¯æ•°
            # æŸå¤±å‡½æ•°ï¼šå¹³è¡¡å‡†ç¡®æ€§å’Œå¤æ‚åº¦
            parsimony=0.0032,  # ç®€æ´æ€§æƒ©ç½š
            # ç‰¹å¾é€‰æ‹©
            select_k_features=min(10, X.shape[1]),  # è‡ªåŠ¨é€‰æ‹©æœ€é‡è¦çš„kä¸ªç‰¹å¾
            # è¾“å‡ºè®¾ç½®
            verbosity=1,  # æ˜¾ç¤ºè¿›åº¦
            progress=True,  # æ˜¾ç¤ºè¿›åº¦æ¡
            # æ€§èƒ½ä¼˜åŒ–
            turbo=True,  # åŠ é€Ÿæ¨¡å¼
            precision=32,  # ä½¿ç”¨32ä½ç²¾åº¦
        )

        # æ‹Ÿåˆç¬¦å·å›å½’æ¨¡å‹
        try:
            model_sr.fit(X, y)

            print("\n" + "="*80)
            print("ğŸ“ å‘ç°çš„ç¬¦å·å…¬å¼:")
            print("="*80)

            # è·å–æœ€ä½³å…¬å¼
            equations = model_sr.equations_

            # æ˜¾ç¤ºå‰5ä¸ªæœ€ä½³å…¬å¼
            print("\nå‰5ä¸ªå€™é€‰å…¬å¼ï¼ˆæŒ‰å¤æ‚åº¦-å‡†ç¡®åº¦æƒè¡¡æ’åºï¼‰:")
            print("-"*80)

            for i, row in equations.head(5).iterrows():
                print(f"\nå…¬å¼ {i+1}:")
                print(f"  è¡¨è¾¾å¼: {row['equation']}")
                print(f"  å¤æ‚åº¦: {row['complexity']}")
                print(f"  æŸå¤±: {row['loss']:.6f}")
                if 'score' in row:
                    print(f"  è¯„åˆ†: {row['score']:.6f}")

            # ä½¿ç”¨sympyæ˜¾ç¤ºæœ€ä½³å…¬å¼
            print("\n" + "="*80)
            print("ğŸ¯ æœ€ä½³å…¬å¼ (SymPyæ ¼å¼):")
            print("="*80)
            try:
                best_formula = model_sr.sympy()
                print(f"\n{best_formula}\n")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è½¬æ¢ä¸ºSymPyæ ¼å¼: {e}")

            # ==================== è¯„ä¼°ç¬¦å·å›å½’æ¨¡å‹ ====================
            print("\nğŸ“Š [3/3] è¯„ä¼°ç¬¦å·å›å½’æ¨¡å‹...")

            # ä½¿ç”¨ç¬¦å·å›å½’æ¨¡å‹é¢„æµ‹
            y_pred_sr = model_sr.predict(X)

            # è®¡ç®—æŒ‡æ ‡
            from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

            mae = mean_absolute_error(y, y_pred_sr)
            mse = mean_squared_error(y, y_pred_sr)
            rmse = np.sqrt(mse)
            r2 = r2_score(y, y_pred_sr)

            # ä¸ç¥ç»ç½‘ç»œæ¯”è¾ƒ
            mae_nn = mean_absolute_error(y, y_pred_nn)
            r2_nn = r2_score(y, y_pred_nn)

            print("\nç¬¦å·å›å½’æ¨¡å‹æ€§èƒ½:")
            print(f"  MAE:  {mae:.4f}")
            print(f"  RMSE: {rmse:.4f}")
            print(f"  RÂ²:   {r2:.4f}")

            print("\nä¸ç¥ç»ç½‘ç»œå¯¹æ¯”:")
            print(f"  ç¥ç»ç½‘ç»œ MAE: {mae_nn:.4f}")
            print(f"  ç¥ç»ç½‘ç»œ RÂ²:  {r2_nn:.4f}")
            print(f"  MAE æ¯”ç‡:     {mae/mae_nn:.2%} (è¶Šå°è¶Šå¥½)")
            print(f"  RÂ² å·®è·:      {r2 - r2_nn:+.4f}")

            # ç»„è£…ç»“æœ
            results = {
                'best_formula': str(best_formula) if 'best_formula' in locals() else None,
                'equations_df': equations.to_dict('records') if equations is not None else None,
                'metrics': {
                    'mae': float(mae),
                    'rmse': float(rmse),
                    'r2': float(r2),
                },
                'nn_comparison': {
                    'mae_nn': float(mae_nn),
                    'r2_nn': float(r2_nn),
                    'mae_ratio': float(mae/mae_nn),
                    'r2_diff': float(r2 - r2_nn),
                },
                'feature_dim': int(X.shape[1]),
                'num_samples': int(len(y)),
            }

            # ==================== ä¿å­˜ç»“æœ ====================
            if save_dir:
                save_dir = Path(save_dir)
                save_dir.mkdir(parents=True, exist_ok=True)

                # ä¿å­˜å…¬å¼
                formula_path = save_dir / 'symbolic_regression_formulas.txt'
                with open(formula_path, 'w') as f:
                    f.write("="*80 + "\n")
                    f.write("ç¬¦å·å›å½’å‘ç°çš„å…¬å¼\n")
                    f.write("="*80 + "\n\n")

                    if 'best_formula' in locals():
                        f.write(f"æœ€ä½³å…¬å¼:\n{best_formula}\n\n")

                    f.write("æ‰€æœ‰å€™é€‰å…¬å¼:\n")
                    f.write("-"*80 + "\n")
                    for i, row in equations.iterrows():
                        f.write(f"\nå…¬å¼ {i+1}:\n")
                        f.write(f"  {row['equation']}\n")
                        f.write(f"  å¤æ‚åº¦: {row['complexity']}, æŸå¤±: {row['loss']:.6f}\n")

                print(f"\n   âœ“ å…¬å¼å·²ä¿å­˜: {formula_path}")

                # ä¿å­˜è¯¦ç»†ç»“æœ
                results_path = save_dir / 'symbolic_regression_results.json'
                with open(results_path, 'w') as f:
                    json.dump(results, f, indent=2, default=str)

                print(f"   âœ“ ç»“æœå·²ä¿å­˜: {results_path}")

                # ä¿å­˜æ¨¡å‹
                try:
                    model_path = save_dir / 'symbolic_regression_model.pkl'
                    import pickle
                    with open(model_path, 'wb') as f:
                        pickle.dump(model_sr, f)
                    print(f"   âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
                except Exception as e:
                    print(f"   âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")

            print("\n" + "="*80)
            print("âœ… ç¬¦å·å›å½’åˆ†æå®Œæˆ!")
            print("="*80 + "\n")

            return model_sr, results

        except Exception as e:
            print(f"\nâŒ ç¬¦å·å›å½’å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None

    def batch_explain(self, data_loader, atoms_list, save_dir, max_samples=50):
        """
        æ‰¹é‡è§£é‡Š
        
        Args:
            data_loader: æ•°æ®åŠ è½½å™¨
            atoms_list: Atomså¯¹è±¡åˆ—è¡¨
            save_dir: ä¿å­˜ç›®å½•
            max_samples: æœ€å¤§æ ·æœ¬æ•°
        """
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        all_explanations = []
        
        for i, (batch, atoms) in enumerate(tqdm(zip(data_loader, atoms_list), 
                                                  total=min(max_samples, len(atoms_list)),
                                                  desc="æ‰¹é‡åˆ†æ")):
            if i >= max_samples:
                break
                
            g, lg, text, target = batch
            
            explanation = self.explain_prediction(
                g, lg, text, atoms,
                true_value=target.item() if target.numel() == 1 else None,
                save_dir=save_dir / f'sample_{i}',
                sample_id=f'sample_{i}'
            )
            
            all_explanations.append(explanation)
            
        # ä¿å­˜æ±‡æ€»
        summary_path = save_dir / 'batch_summary.json'
        with open(summary_path, 'w') as f:
            json.dump({
                'num_samples': len(all_explanations),
                'explanations': all_explanations
            }, f, indent=2, default=str)
            
        print(f"\nâœ… æ‰¹é‡åˆ†æå®Œæˆ! å…± {len(all_explanations)} ä¸ªæ ·æœ¬")
        print(f"   ç»“æœä¿å­˜åœ¨: {save_dir}")
        
        return all_explanations


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

def demo_usage():
    """æ¼”ç¤ºç”¨æ³•"""
    print("""
    =========================================================
    å¢å¼ºå¯è§£é‡Šæ€§åˆ†ææ¨¡å— v2.0 ä½¿ç”¨ç¤ºä¾‹
    =========================================================
    
    # åˆå§‹åŒ–
    from interpretability_enhanced_v2 import ComprehensiveExplainer
    
    explainer = ComprehensiveExplainer(
        model=trained_model,
        tokenizer=tokenizer,  # å¯é€‰
        device='cuda'
    )
    
    # å•æ ·æœ¬åˆ†æ
    explanation = explainer.explain_prediction(
        g=graph,
        lg=line_graph,
        text=["Material description..."],
        atoms_object=atoms,
        true_value=1.5,
        save_dir='./explanations',
        sample_id='sample_001'
    )
    
    # æ‰¹é‡åˆ†æ
    explanations = explainer.batch_explain(
        data_loader=test_loader,
        atoms_list=test_atoms,
        save_dir='./batch_explanations',
        max_samples=100
    )
    
    # å•ç‹¬ä½¿ç”¨å„åˆ†æå™¨
    
    ## åŸå­é‡è¦æ€§
    atom_analyzer = AtomImportanceAnalyzer(model)
    importance, gradients = atom_analyzer.integrated_gradients(g, lg, text)
    
    ## è·¨æ¨¡æ€åˆ†æ
    cross_modal = CrossModalInteractionAnalyzer(model)
    contributions = cross_modal.analyze_modal_contribution(g, lg, text)
    
    ## ç‰©ç†å…³è”åˆ†æ
    physics = PhysicsCorrelationAnalyzer()
    correlations = physics.correlate_importance_with_physics(elements, importance)
    
    ## ä¸ç¡®å®šæ€§ä¼°è®¡
    uncertainty = UncertaintyEstimator(model)
    mean, std, samples = uncertainty.mc_dropout_uncertainty(g, lg, text)

    ## ç¬¦å·å›å½’åˆ†æ (æ–°åŠŸèƒ½!)
    # ä»ç¥ç»ç½‘ç»œç‰¹å¾ä¸­å‘ç°å¯è§£é‡Šçš„æ•°å­¦å…¬å¼
    model_sr, results = explainer.extract_symbolic_features(
        test_loader=test_loader,
        save_dir='./symbolic_regression',
        max_samples=500  # å¯é€‰ï¼šé™åˆ¶æ ·æœ¬æ•°ä»¥åŠ å¿«é€Ÿåº¦
    )

    # ä½¿ç”¨å‘ç°çš„ç¬¦å·å…¬å¼è¿›è¡Œé¢„æµ‹
    if model_sr is not None:
        # æå–æ–°æ ·æœ¬çš„ç‰¹å¾
        new_features = model.get_features(new_g, new_lg, new_text)
        # ä½¿ç”¨ç¬¦å·å…¬å¼é¢„æµ‹
        symbolic_prediction = model_sr.predict(new_features)
        print(f"ç¬¦å·å…¬å¼é¢„æµ‹: {symbolic_prediction}")

    =========================================================
    """)


if __name__ == "__main__":
    demo_usage()

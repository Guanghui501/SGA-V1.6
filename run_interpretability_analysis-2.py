#!/usr/bin/env python
"""
å¯è§£é‡Šæ€§åˆ†æžä½¿ç”¨ç¤ºä¾‹å’Œé›†æˆæŒ‡å—

æœ¬æ–‡ä»¶å±•ç¤ºå¦‚ä½•å°†å¢žå¼ºçš„å¯è§£é‡Šæ€§æ¨¡å—é›†æˆåˆ° SGA-Net è®­ç»ƒå’ŒæŽ¨ç†æµç¨‹ä¸­ã€‚

ç”¨æ³•:
    python run_interpretability_analysis.py \
        --model_path ./output/best_val_model.pt \
        --data_dir ./dataset/jarvis/formation_energy_peratom \
        --output_dir ./interpretability_results \
        --num_samples 10
"""

import os
import sys
import argparse
import json
import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥å¯è§£é‡Šæ€§æ¨¡å—
from interpretability_enhanced_v2 import (
    ComprehensiveExplainer,
    AtomImportanceAnalyzer,
    CrossModalInteractionAnalyzer,
    PhysicsCorrelationAnalyzer,
    UncertaintyEstimator,
)
from advanced_visualization import AdvancedVisualizer, quick_visualize


def load_model_and_config(model_path, device='cuda'):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡åž‹å’Œé…ç½®
    
    Args:
        model_path: æ¨¡åž‹checkpointè·¯å¾„
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        model: åŠ è½½å¥½çš„æ¨¡åž‹
        config: æ¨¡åž‹é…ç½®
    """
    print(f"\nðŸ“¦ åŠ è½½æ¨¡åž‹: {model_path}")
    
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    # èŽ·å–é…ç½®
    config = checkpoint.get('config', None)
    
    if config is None:
        print("âš ï¸  checkpointä¸­æ²¡æœ‰æ‰¾åˆ°é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        # ä½¿ç”¨é»˜è®¤é…ç½®
        from models.alignn import ALIGNNConfig
        config = ALIGNNConfig(
            name="alignn",
            alignn_layers=4,
            gcn_layers=4,
            atom_input_features=92,
            hidden_features=256,
            output_features=1,
            use_cross_modal_attention=True,
            use_fine_grained_attention=True,
        )
        
    # åˆ›å»ºæ¨¡åž‹
    from models.alignn import ALIGNN
    model = ALIGNN(config)
    
    # åŠ è½½æƒé‡
    model.load_state_dict(checkpoint['model'])
    model = model.to(device)
    model.eval()
    
    print(f"âœ… æ¨¡åž‹åŠ è½½æˆåŠŸ")
    print(f"   - ALIGNNå±‚æ•°: {config.alignn_layers}")
    print(f"   - GCNå±‚æ•°: {config.gcn_layers}")
    print(f"   - è·¨æ¨¡æ€æ³¨æ„åŠ›: {config.use_cross_modal_attention}")
    print(f"   - ç»†ç²’åº¦æ³¨æ„åŠ›: {getattr(config, 'use_fine_grained_attention', False)}")
    
    return model, config


def load_test_samples(data_dir, num_samples=10):
    """
    åŠ è½½æµ‹è¯•æ ·æœ¬
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        num_samples: æ ·æœ¬æ•°é‡
        
    Returns:
        samples: [(g, lg, text, target, atoms_object), ...]
    """
    import csv
    from jarvis.core.atoms import Atoms
    from graphs import Graph
    
    print(f"\nðŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {data_dir}")
    
    cif_dir = os.path.join(data_dir, 'cif')
    desc_file = os.path.join(data_dir, 'description.csv')
    
    # è¯»å–æè¿°æ–‡ä»¶
    with open(desc_file, 'r') as f:
        reader = csv.reader(f)
        header = next(reader)
        data = list(reader)[:num_samples]
        
    samples = []
    
    for row in tqdm(data, desc="åŠ è½½æ ·æœ¬"):
        try:
            jid = row[0]
            target = float(row[2])
            text = row[3]
            
            # è¯»å–CIF
            cif_path = os.path.join(cif_dir, f'{jid}.cif')
            atoms = Atoms.from_cif(cif_path)
            
            # æž„å»ºå›¾
            g, lg = Graph.atom_dgl_multigraph(
                atoms=atoms,
                cutoff=8.0,
                max_neighbors=12,
                atom_features="cgcnn",
                compute_line_graph=True,
                use_canonize=True
            )
            
            samples.append({
                'jid': jid,
                'g': g,
                'lg': lg,
                'text': [text],
                'target': target,
                'atoms': atoms
            })
            
        except Exception as e:
            print(f"âš ï¸  è·³è¿‡æ ·æœ¬ {row[0]}: {e}")
            
    print(f"âœ… åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
    return samples


def run_single_sample_analysis(explainer, sample, output_dir, visualizer=None):
    """
    å¯¹å•ä¸ªæ ·æœ¬è¿è¡Œå®Œæ•´åˆ†æž
    
    Args:
        explainer: ComprehensiveExplainerå®žä¾‹
        sample: æ ·æœ¬å­—å…¸
        output_dir: è¾“å‡ºç›®å½•
        visualizer: AdvancedVisualizerå®žä¾‹ï¼ˆå¯é€‰ï¼‰
    """
    sample_dir = Path(output_dir) / sample['jid']
    sample_dir.mkdir(parents=True, exist_ok=True)
    
    # è¿è¡Œç»¼åˆåˆ†æž
    explanation = explainer.explain_prediction(
        g=sample['g'],
        lg=sample['lg'],
        text=sample['text'],
        atoms_object=sample['atoms'],
        true_value=sample['target'],
        save_dir=sample_dir,
        sample_id=sample['jid']
    )
    
    # é«˜çº§å¯è§†åŒ–
    if visualizer:
        visualizer.create_comprehensive_report(
            explanation, sample['atoms'],
            save_path=sample_dir / 'comprehensive_report.png'
        )
        
        # å…ƒç´ é‡è¦æ€§å‘¨æœŸè¡¨
        importance = explanation.get('atom_importance_integrated_gradients', [])
        if len(importance) > 0:
            from collections import defaultdict
            elements = list(sample['atoms'].elements)
            elem_imp = defaultdict(list)
            for elem, imp in zip(elements, importance):
                elem_imp[elem].append(imp)
            elem_mean = {k: np.mean(v) for k, v in elem_imp.items()}
            
            visualizer.plot_periodic_table_importance(
                elem_mean,
                save_path=sample_dir / 'periodic_table_importance.png'
            )
            
        # HTMLæŠ¥å‘Š
        image_paths = {
            'Atom Importance': 'atom_importance.png',
            'Modal Contribution': 'modal_contribution.png',
            'Physics Correlation': 'physics_correlation.png',
        }
        # åªåŒ…å«å­˜åœ¨çš„å›¾åƒ
        image_paths = {k: str(sample_dir / v) for k, v in image_paths.items() 
                      if (sample_dir / v).exists()}
        
        visualizer.generate_html_report(
            explanation, sample['atoms'],
            image_paths,
            save_path=str(sample_dir / 'report.html')
        )
        
    return explanation


def run_batch_analysis(explainer, samples, output_dir, visualizer=None):
    """
    æ‰¹é‡åˆ†æž
    
    Args:
        explainer: ComprehensiveExplainerå®žä¾‹
        samples: æ ·æœ¬åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        visualizer: AdvancedVisualizerå®žä¾‹
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    all_explanations = []
    
    # ç»Ÿè®¡æ•°æ®
    all_errors = []
    all_uncertainties = []
    all_graph_contrib = []
    all_text_contrib = []
    element_importance_agg = {}
    
    for sample in tqdm(samples, desc="æ‰¹é‡åˆ†æž"):
        explanation = run_single_sample_analysis(
            explainer, sample, output_dir, visualizer
        )
        all_explanations.append(explanation)
        
        # æ”¶é›†ç»Ÿè®¡æ•°æ®
        if explanation.get('error') is not None:
            all_errors.append(explanation['error'])
        if explanation.get('uncertainty', {}).get('std') is not None:
            all_uncertainties.append(explanation['uncertainty']['std'])
            
        mc = explanation.get('modal_contribution', {})
        if mc.get('graph_contribution') is not None:
            all_graph_contrib.append(mc['graph_contribution'])
        if mc.get('text_contribution') is not None:
            all_text_contrib.append(mc['text_contribution'])
            
        # èšåˆå…ƒç´ é‡è¦æ€§
        sa = explanation.get('structure_analysis', {})
        elem_imp = sa.get('element_importance', {})
        for elem, imp in elem_imp.items():
            if elem not in element_importance_agg:
                element_importance_agg[elem] = []
            element_importance_agg[elem].append(imp)
            
    # ç”Ÿæˆæ‰¹é‡ç»Ÿè®¡æŠ¥å‘Š
    summary = {
        'num_samples': len(samples),
        'error_statistics': {
            'mean': float(np.mean(all_errors)) if all_errors else None,
            'std': float(np.std(all_errors)) if all_errors else None,
            'min': float(np.min(all_errors)) if all_errors else None,
            'max': float(np.max(all_errors)) if all_errors else None,
        },
        'uncertainty_statistics': {
            'mean': float(np.mean(all_uncertainties)) if all_uncertainties else None,
            'std': float(np.std(all_uncertainties)) if all_uncertainties else None,
        },
        'modal_contribution': {
            'graph_mean': float(np.mean(all_graph_contrib)) if all_graph_contrib else None,
            'text_mean': float(np.mean(all_text_contrib)) if all_text_contrib else None,
        },
        'element_importance': {
            elem: {
                'mean': float(np.mean(imps)),
                'std': float(np.std(imps)),
                'count': len(imps)
            }
            for elem, imps in element_importance_agg.items()
        }
    }
    
    # ä¿å­˜æ±‡æ€»
    with open(output_dir / 'batch_summary.json', 'w') as f:
        json.dump(summary, f, indent=2)
        
    # ç”Ÿæˆæ±‡æ€»å¯è§†åŒ–
    if visualizer and element_importance_agg:
        elem_mean = {k: np.mean(v) for k, v in element_importance_agg.items()}
        visualizer.plot_periodic_table_importance(
            elem_mean,
            save_path=output_dir / 'batch_periodic_table_importance.png'
        )
        
    print(f"\n{'='*80}")
    print("ðŸ“Š æ‰¹é‡åˆ†æžç»Ÿè®¡")
    print(f"{'='*80}")
    print(f"æ ·æœ¬æ•°: {len(samples)}")
    if all_errors:
        print(f"å¹³å‡è¯¯å·®: {np.mean(all_errors):.4f} Â± {np.std(all_errors):.4f}")
    if all_uncertainties:
        print(f"å¹³å‡ä¸ç¡®å®šæ€§: {np.mean(all_uncertainties):.4f}")
    if all_graph_contrib:
        print(f"å›¾æ¨¡æ€å¹³å‡è´¡çŒ®: {np.mean(all_graph_contrib):.1%}")
        print(f"æ–‡æœ¬æ¨¡æ€å¹³å‡è´¡çŒ®: {np.mean(all_text_contrib):.1%}")
    print(f"\nç»“æžœä¿å­˜åœ¨: {output_dir}")
    print(f"{'='*80}\n")
    
    return all_explanations, summary


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¿è¡Œå¯è§£é‡Šæ€§åˆ†æž')
    parser.add_argument('--model_path', type=str, required=True,
                        help='æ¨¡åž‹checkpointè·¯å¾„')
    parser.add_argument('--data_dir', type=str, required=True,
                        help='æ•°æ®ç›®å½•ï¼ˆåŒ…å«cif/å’Œdescription.csvï¼‰')
    parser.add_argument('--output_dir', type=str, default='./interpretability_results',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--num_samples', type=int, default=10,
                        help='åˆ†æžæ ·æœ¬æ•°')
    parser.add_argument('--device', type=str, default='cuda',
                        help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--style', type=str, default='publication',
                        choices=['publication', 'presentation', 'report'],
                        help='å¯è§†åŒ–é£Žæ ¼')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("ðŸ”¬ SGA-Net å¯è§£é‡Šæ€§åˆ†æž")
    print("="*80)
    
    # è®¾ç½®è®¾å¤‡
    device = args.device if torch.cuda.is_available() else 'cpu'
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡åž‹
    model, config = load_model_and_config(args.model_path, device)
    
    # åŠ è½½æ•°æ®
    samples = load_test_samples(args.data_dir, args.num_samples)
    
    if not samples:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ ·æœ¬!")
        return
        
    # åˆå§‹åŒ–åˆ†æžå™¨
    print("\nðŸ”§ åˆå§‹åŒ–åˆ†æžå™¨...")
    
    # å°è¯•åŠ è½½tokenizerï¼ˆå¯é€‰ï¼‰
    try:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained('m3rg-iitd/matscibert')
    except:
        tokenizer = None
        print("âš ï¸  æœªèƒ½åŠ è½½tokenizerï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")
        
    explainer = ComprehensiveExplainer(
        model=model,
        tokenizer=tokenizer,
        device=device
    )
    
    visualizer = AdvancedVisualizer(style=args.style)
    
    # è¿è¡Œåˆ†æž
    print(f"\nðŸš€ å¼€å§‹åˆ†æž {len(samples)} ä¸ªæ ·æœ¬...")
    
    explanations, summary = run_batch_analysis(
        explainer, samples, args.output_dir, visualizer
    )
    
    print("\nâœ… åˆ†æžå®Œæˆ!")


# ==================== å¿«æ·ä½¿ç”¨æŽ¥å£ ====================

class QuickAnalyzer:
    """
    å¿«æ·åˆ†æžæŽ¥å£ - ç®€åŒ–ä½¿ç”¨æµç¨‹
    
    ç”¨æ³•:
        analyzer = QuickAnalyzer(model)
        result = analyzer.analyze(g, lg, text, atoms)
        analyzer.visualize(result, atoms, './output')
    """
    
    def __init__(self, model, device='cuda'):
        """
        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡åž‹
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model
        self.device = device
        self.model.eval()
        self.model.to(device)
        
        # åˆå§‹åŒ–åˆ†æžå™¨
        self.explainer = ComprehensiveExplainer(model, device=device)
        self.visualizer = AdvancedVisualizer()
        
    def analyze(self, g, lg, text, atoms_object, true_value=None):
        """
        å¿«é€Ÿåˆ†æž
        
        Args:
            g: DGL graph
            lg: Line graph
            text: æ–‡æœ¬åˆ—è¡¨
            atoms_object: JARVIS Atomså¯¹è±¡
            true_value: çœŸå®žå€¼ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            explanation: è§£é‡Šå­—å…¸
        """
        return self.explainer.explain_prediction(
            g, lg, text, atoms_object,
            true_value=true_value,
            save_dir=None  # ä¸ä¿å­˜ï¼Œåªè¿”å›ž
        )
        
    def visualize(self, explanation, atoms_object, save_dir, sample_id='sample'):
        """
        å¯è§†åŒ–åˆ†æžç»“æžœ
        
        Args:
            explanation: è§£é‡Šå­—å…¸
            atoms_object: Atomså¯¹è±¡
            save_dir: ä¿å­˜ç›®å½•
            sample_id: æ ·æœ¬ID
        """
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»¼åˆæŠ¥å‘Š
        self.visualizer.create_comprehensive_report(
            explanation, atoms_object,
            save_path=save_dir / f'{sample_id}_report.png'
        )
        
        # å‘¨æœŸè¡¨
        importance = explanation.get('atom_importance_integrated_gradients', [])
        if len(importance) > 0:
            from collections import defaultdict
            elements = list(atoms_object.elements)
            elem_imp = defaultdict(list)
            for elem, imp in zip(elements, importance):
                elem_imp[elem].append(imp)
            elem_mean = {k: np.mean(v) for k, v in elem_imp.items()}
            
            self.visualizer.plot_periodic_table_importance(
                elem_mean,
                save_path=save_dir / f'{sample_id}_periodic_table.png'
            )
            
        print(f"âœ… å¯è§†åŒ–ç»“æžœå·²ä¿å­˜åˆ°: {save_dir}")
        
    def get_atom_importance(self, g, lg, text, method='integrated_gradients'):
        """
        å¿«é€ŸèŽ·å–åŽŸå­é‡è¦æ€§
        
        Args:
            g, lg, text: æ¨¡åž‹è¾“å…¥
            method: 'gradient' æˆ– 'integrated_gradients'
            
        Returns:
            importance: numpyæ•°ç»„
        """
        atom_analyzer = AtomImportanceAnalyzer(self.model, self.device)
        
        if method == 'gradient':
            importance, _ = atom_analyzer.gradient_importance(g, lg, text)
        else:
            importance, _ = atom_analyzer.integrated_gradients(g, lg, text)
            
        return importance
        
    def get_uncertainty(self, g, lg, text, n_samples=30):
        """
        å¿«é€ŸèŽ·å–é¢„æµ‹ä¸ç¡®å®šæ€§
        
        Args:
            g, lg, text: æ¨¡åž‹è¾“å…¥
            n_samples: MCé‡‡æ ·æ•°
            
        Returns:
            mean, std: é¢„æµ‹å‡å€¼å’Œæ ‡å‡†å·®
        """
        uncertainty_estimator = UncertaintyEstimator(self.model, self.device)
        mean, std, _ = uncertainty_estimator.mc_dropout_uncertainty(g, lg, text, n_samples)
        return mean, std


# ==================== æ¼”ç¤ºå‡½æ•° ====================

def demo():
    """æ¼”ç¤ºç”¨æ³•"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    SGA-Net å¢žå¼ºå¯è§£é‡Šæ€§åˆ†æžæ¨¡å—ä½¿ç”¨æŒ‡å—                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                                â•‘
â•‘  1. å‘½ä»¤è¡Œä½¿ç”¨:                                                                â•‘
â•‘     python run_interpretability_analysis.py \\                                 â•‘
â•‘         --model_path ./output/best_val_model.pt \\                             â•‘
â•‘         --data_dir ./dataset/jarvis/formation_energy_peratom \\                â•‘
â•‘         --output_dir ./interpretability_results \\                             â•‘
â•‘         --num_samples 10                                                       â•‘
â•‘                                                                                â•‘
â•‘  2. Pythonä»£ç ä½¿ç”¨:                                                            â•‘
â•‘                                                                                â•‘
â•‘     # æ–¹å¼ä¸€ï¼šä½¿ç”¨å¿«æ·åˆ†æžå™¨                                                    â•‘
â•‘     from run_interpretability_analysis import QuickAnalyzer                    â•‘
â•‘                                                                                â•‘
â•‘     analyzer = QuickAnalyzer(model)                                            â•‘
â•‘     explanation = analyzer.analyze(g, lg, text, atoms)                         â•‘
â•‘     analyzer.visualize(explanation, atoms, './output')                         â•‘
â•‘                                                                                â•‘
â•‘     # æ–¹å¼äºŒï¼šä½¿ç”¨ç»¼åˆè§£é‡Šå™¨                                                    â•‘
â•‘     from interpretability_enhanced_v2 import ComprehensiveExplainer            â•‘
â•‘                                                                                â•‘
â•‘     explainer = ComprehensiveExplainer(model, tokenizer, device='cuda')        â•‘
â•‘     explanation = explainer.explain_prediction(                                â•‘
â•‘         g, lg, text, atoms,                                                    â•‘
â•‘         true_value=1.5,                                                        â•‘
â•‘         save_dir='./results',                                                  â•‘
â•‘         sample_id='sample_001'                                                 â•‘
â•‘     )                                                                          â•‘
â•‘                                                                                â•‘
â•‘     # æ–¹å¼ä¸‰ï¼šå•ç‹¬ä½¿ç”¨å„åˆ†æžå™¨                                                  â•‘
â•‘     from interpretability_enhanced_v2 import (                                 â•‘
â•‘         AtomImportanceAnalyzer,                                                â•‘
â•‘         CrossModalInteractionAnalyzer,                                         â•‘
â•‘         PhysicsCorrelationAnalyzer,                                            â•‘
â•‘         UncertaintyEstimator                                                   â•‘
â•‘     )                                                                          â•‘
â•‘                                                                                â•‘
â•‘     # åŽŸå­é‡è¦æ€§                                                                â•‘
â•‘     atom_analyzer = AtomImportanceAnalyzer(model)                              â•‘
â•‘     importance, gradients = atom_analyzer.integrated_gradients(g, lg, text)    â•‘
â•‘                                                                                â•‘
â•‘     # è·¨æ¨¡æ€åˆ†æž                                                                â•‘
â•‘     cross_modal = CrossModalInteractionAnalyzer(model)                         â•‘
â•‘     contributions = cross_modal.analyze_modal_contribution(g, lg, text)        â•‘
â•‘                                                                                â•‘
â•‘     # ç‰©ç†å…³è”                                                                  â•‘
â•‘     physics = PhysicsCorrelationAnalyzer()                                     â•‘
â•‘     correlations = physics.correlate_importance_with_physics(elements, imp)    â•‘
â•‘                                                                                â•‘
â•‘     # ä¸ç¡®å®šæ€§ä¼°è®¡                                                              â•‘
â•‘     uncertainty = UncertaintyEstimator(model)                                  â•‘
â•‘     mean, std, samples = uncertainty.mc_dropout_uncertainty(g, lg, text)       â•‘
â•‘                                                                                â•‘
â•‘  3. è¾“å‡ºè¯´æ˜Ž:                                                                  â•‘
â•‘     - *_explanation.json: å®Œæ•´è§£é‡Šæ•°æ®                                         â•‘
â•‘     - *_summary.txt: æ–‡æœ¬æ‘˜è¦                                                  â•‘
â•‘     - *_comprehensive_report.png: ç»¼åˆæŠ¥å‘Šå›¾                                   â•‘
â•‘     - *_periodic_table.png: å…ƒç´ å‘¨æœŸè¡¨é‡è¦æ€§å›¾                                 â•‘
â•‘     - *_report.html: äº¤äº’å¼HTMLæŠ¥å‘Š                                            â•‘
â•‘                                                                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)


if __name__ == "__main__":
    if len(sys.argv) > 1:
        main()
    else:
        demo()

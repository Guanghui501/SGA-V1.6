"""
å¢å¼ºçš„å¯è§£é‡Šæ€§åˆ†æå·¥å…· - å®Œæ•´æ”¯æŒè·¨æ¨¡æ€æ³¨æ„åŠ›å¯è§†åŒ–

ä¸»è¦åŠŸèƒ½ï¼š
1. è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡æå–å’Œå¯è§†åŒ–
2. åŸå­é‡è¦æ€§åˆ†æï¼ˆæ¢¯åº¦æ³•ã€ç§¯åˆ†æ¢¯åº¦æ³•ã€Layer-wise Relevance Propagationï¼‰
3. æ–‡æœ¬Tokené‡è¦æ€§åˆ†æ
4. ç‰¹å¾ç©ºé—´å¯¹é½å¯è§†åŒ–
5. å•æ ·æœ¬å®Œæ•´è§£é‡ŠæŠ¥å‘Š
6. æ‰¹é‡å¯è§£é‡Šæ€§åˆ†æ

ä½œè€…: Enhanced Interpretability Module
æ—¥æœŸ: 2025
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Union
import pandas as pd
from pathlib import Path
import json
from tqdm import tqdm


class EnhancedInterpretabilityAnalyzer:
    """å¢å¼ºç‰ˆå¯è§£é‡Šæ€§åˆ†æå™¨ - æ”¯æŒå®Œæ•´çš„è·¨æ¨¡æ€æ³¨æ„åŠ›åˆ†æ"""

    # åœç”¨è¯å°†ä»æ–‡ä»¶åŠ è½½
    _STOPWORDS = None

    @classmethod
    def load_stopwords(cls, stopwords_dir=None):
        """ä»æ–‡ä»¶åŠ è½½åœç”¨è¯

        Args:
            stopwords_dir: åœç”¨è¯ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º ./stopwords/en/

        Returns:
            åœç”¨è¯é›†åˆ
        """
        if cls._STOPWORDS is not None:
            return cls._STOPWORDS

        stopwords = set()

        # é»˜è®¤åœç”¨è¯ç›®å½•
        if stopwords_dir is None:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
            possible_dirs = [
                Path(__file__).parent / 'stopwords' / 'en',
                Path('./stopwords/en'),
                Path('../stopwords/en'),
            ]
            for d in possible_dirs:
                if d.exists():
                    stopwords_dir = d
                    break

        if stopwords_dir is None or not Path(stopwords_dir).exists():
            print(f"âš ï¸ åœç”¨è¯ç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤åœç”¨è¯")
            # ä½¿ç”¨åŸºæœ¬åœç”¨è¯
            stopwords = {
                'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'as',
                'and', 'or', 'but', 'not', 'no', 'if', 'that', 'this', 'it', 'its',
            }
        else:
            # ä»æ‰€æœ‰txtæ–‡ä»¶åŠ è½½åœç”¨è¯
            stopwords_path = Path(stopwords_dir)
            txt_files = list(stopwords_path.glob('*.txt'))

            for txt_file in txt_files:
                try:
                    with open(txt_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            word = line.strip().lower()
                            if word and not word.startswith('#'):  # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                                stopwords.add(word)
                except Exception as e:
                    pass  # é™é»˜å¤„ç†è¯»å–é”™è¯¯

            print(f"âœ… ä» {len(txt_files)} ä¸ªæ–‡ä»¶åŠ è½½äº† {len(stopwords)} ä¸ªåœç”¨è¯")

        # æ·»åŠ  BERT ç‰¹æ®Š token å’Œæ ‡ç‚¹ç¬¦å·
        special_tokens = {
            # BERT ç‰¹æ®Š tokens
            '[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASK]',
            # è‹±æ–‡æ ‡ç‚¹ç¬¦å·ï¼ˆæ‰€æœ‰å¸¸è§æ ‡ç‚¹ï¼‰
            '.', ',', '!', '?', ';', ':',
            '(', ')', '[', ']', '{', '}', '<', '>',
            '"', "'", '`',
            '-', 'â€“', 'â€”', '_',
            '/', '\\', '|',
            '+', '=', '*', '&', '%', '$', '#', '@', '^', '~',
            # ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
            'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼š',
            'ï¼ˆ', 'ï¼‰', 'ã€', 'ã€‘', 'ã€Š', 'ã€‹', 'ã€ˆ', 'ã€‰',
            '"', '"', ''', ''', 'ã€', 'Â·',
            'â€¦â€¦', 'â€”', 'ï½',
            # WordPiece è¯ç¼€
            '##s', '##ed', '##ing', '##ly', '##er', '##est', '##tion', '##ment',
        }
        stopwords.update(special_tokens)

        cls._STOPWORDS = stopwords
        return stopwords

    @property
    def STOPWORDS(self):
        """è·å–åœç”¨è¯é›†åˆï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._STOPWORDS is None:
            self.load_stopwords()
        return self._STOPWORDS

    def __init__(self, model, tokenizer=None, device='cuda'):
        """
        Args:
            model: è®­ç»ƒå¥½çš„ ALIGNN æ¨¡å‹
            tokenizer: æ–‡æœ¬tokenizerï¼ˆå¯é€‰ï¼Œç”¨äºtokençº§åˆ«åˆ†æï¼‰
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.model.eval()

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæ³¨æ„åŠ›æå–
        self.has_cross_modal = hasattr(model, 'use_cross_modal_attention') and \
                              model.use_cross_modal_attention
        self.has_middle_fusion = hasattr(model, 'use_middle_fusion') and \
                                model.use_middle_fusion

        # åŠ è½½åœç”¨è¯
        self.load_stopwords()

        print(f"\nğŸ” å¯è§£é‡Šæ€§åˆ†æå™¨åˆå§‹åŒ–:")
        print(f"  - è·¨æ¨¡æ€æ³¨æ„åŠ›: {'âœ… æ”¯æŒ' if self.has_cross_modal else 'âŒ æœªå¯ç”¨'}")
        print(f"  - ä¸­æœŸèåˆ: {'âœ… æ”¯æŒ' if self.has_middle_fusion else 'âŒ æœªå¯ç”¨'}")
        print(f"  - è®¾å¤‡: {device}")
        print(f"  - åœç”¨è¯è¿‡æ»¤: âœ… å·²å¯ç”¨ ({len(self.STOPWORDS)} ä¸ªåœç”¨è¯)\n")

    @staticmethod
    def _merge_tokens_and_weights(tokens, weights):
        """Merge WordPiece tokens and their corresponding attention weights.

        Handles space groups (F-43m, P63/mmc), N-coordinate (12-coordinate),
        atom labels with site numbers (Ba(1)), and element symbols with digits (Ba4).

        Args:
            tokens: list of token strings
            weights: numpy array of shape [..., seq_len]

        Returns:
            merged_tokens: list of merged token strings
            merged_weights: numpy array with merged weights
            token_mapping: list mapping merged index to original indices
        """
        import numpy as np

        # Space group starting letters (Bravais lattice symbols)
        space_group_starters = {'P', 'I', 'F', 'R', 'C', 'A', 'B'}
        # Characters that are part of space group notation
        space_group_chars = {'-', '/', 'm', 'n', 'c', 'a', 'b', 'd', 'e'}
        # Common atom symbols (1-2 letters)
        atom_symbols = {'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne',
                       'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca',
                       'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn',
                       'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr',
                       'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn',
                       'Sb', 'Te', 'I', 'Xe', 'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd',
                       'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb',
                       'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg',
                       'Tl', 'Pb', 'Bi', 'Po', 'At', 'Rn', 'Fr', 'Ra', 'Ac', 'Th',
                       'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es', 'Fm'}

        merged_tokens = []
        token_mapping = []  # Each element is a list of original indices
        current_token = ""
        current_indices = []
        in_space_group = False  # Track if we're in a space group symbol
        in_coordinate = False  # Track if we're in a N-coordinate pattern
        in_parentheses = False  # Track if we're inside parentheses like Ba(1)
        after_closing_paren = False  # Track if we just closed a parenthesis (for chemical formulas like Ca(1)O6)
        in_hyphenated_word = False  # Track if we're in a hyphenated compound word like "see-saw-like"

        for i, token in enumerate(tokens):
            if token.startswith("##"):
                # Continue previous token (WordPiece continuation)
                current_token += token[2:]
                current_indices.append(i)
            elif token == '-' and current_token:
                # Check if this is part of space group or N-coordinate pattern
                # Use upper() for case-insensitive space group detection (BERT lowercases)
                if current_token[0].upper() in space_group_starters and len(current_token) <= 4:
                    # Space group pattern (F-43m, P-1, f-43m)
                    current_token += token
                    current_indices.append(i)
                    in_space_group = True
                elif current_token.isdigit():
                    # N-coordinate pattern (12-coordinate)
                    current_token += token
                    current_indices.append(i)
                    in_coordinate = True
                else:
                    # Check if this is part of a hyphenated compound word (like "see-saw-like")
                    # Merge hyphen if current token is alphabetic (or already hyphenated) and has reasonable length
                    # This keeps compound words together while avoiding merging with formulas
                    token_without_hyphens = current_token.replace('##', '').replace('-', '')
                    is_compound_word = (token_without_hyphens.isalpha() and
                                       len(current_token) >= 2 and
                                       not (current_token in atom_symbols or current_token.capitalize() in atom_symbols))
                    if is_compound_word:
                        # This looks like a hyphenated compound word
                        current_token += token
                        current_indices.append(i)
                        in_hyphenated_word = True  # Continue merging the next word
                    else:
                        # Don't merge "-" with atom symbols or other patterns
                        if current_token:
                            merged_tokens.append(current_token)
                            token_mapping.append(current_indices)
                            in_space_group = False
                            in_coordinate = False
                            after_closing_paren = False
                            in_hyphenated_word = False
                        current_token = token
                        current_indices = [i]
            elif token == '/' and current_token:
                # Merge "/" for space groups (I4/mmm, P63/mmc)
                # Use upper() for case-insensitive detection
                if current_token[0].upper() in space_group_starters:
                    current_token += token
                    current_indices.append(i)
                    in_space_group = True
                else:
                    if current_token:
                        merged_tokens.append(current_token)
                        token_mapping.append(current_indices)
                        after_closing_paren = False
                        in_hyphenated_word = False
                    current_token = token
                    current_indices = [i]
            elif in_coordinate and token.isalpha():
                # Continue N-coordinate pattern (12-coordinate)
                current_token += token
                current_indices.append(i)
                in_coordinate = False  # End after the word
            elif in_space_group:
                # Check if this token should be part of the space group
                # Continue only for digits and specific space group characters
                if token.isdigit() or (token.lower() in space_group_chars):
                    # Continue space group: merge numbers, m/n/c/a/b/d letters
                    current_token += token
                    current_indices.append(i)
                else:
                    # End space group - don't merge this token
                    if current_token:
                        merged_tokens.append(current_token)
                        token_mapping.append(current_indices)
                    in_space_group = False
                    after_closing_paren = False
                    in_hyphenated_word = False
                    current_token = token
                    current_indices = [i]
            elif token == '(' and current_token:
                # Merge opening parentheses (e.g., for atom labels like Li(1))
                current_token += token
                current_indices.append(i)
                in_parentheses = True
            elif token == ')' and current_token and in_parentheses:
                # Merge closing parentheses
                current_token += token
                current_indices.append(i)
                in_parentheses = False
                # Set flag to continue merging if followed by element symbols/digits (like Ca(1)O6)
                after_closing_paren = True
            elif in_parentheses and (token.isdigit() or token.isalpha()):
                # Merge content inside parentheses (digits or letters)
                current_token += token
                current_indices.append(i)
            elif after_closing_paren and (token in atom_symbols or token.capitalize() in atom_symbols or token.isdigit()):
                # Continue merging after closing parenthesis for chemical formulas (Ca(1)O6)
                # Merge element symbols or digits immediately following ")"
                current_token += token
                current_indices.append(i)
                # Keep the flag set if it's an element symbol (to continue with digits like O6)
                # Reset the flag if it's a digit (we're done with this formula unit)
                if token.isdigit():
                    after_closing_paren = False
            elif token.isdigit() and current_token and not current_token[-1].isdigit():
                # Only merge numbers with atom symbols or short space group starters
                # NOT with regular words like "bonded"
                # Check both original case and capitalized for atom symbols
                # For space group starters, only merge if token is short (<=2 chars, like P, I, F, Pm)
                is_atom = current_token in atom_symbols or current_token.capitalize() in atom_symbols
                is_short_space_group = (len(current_token) <= 2 and current_token[0].upper() in space_group_starters)
                if is_atom or is_short_space_group:
                    current_token += token
                    current_indices.append(i)
                else:
                    # Save previous and start new with the digit
                    if current_token:
                        merged_tokens.append(current_token)
                        token_mapping.append(current_indices)
                        in_space_group = False
                        after_closing_paren = False
                        in_hyphenated_word = False
                    current_token = token
                    current_indices = [i]
            elif token == '.' and current_token:
                # Only merge decimal points if followed by digits (e.g., 3.14)
                # Don't merge period after space groups or between different entities
                # Check if the current token looks like it should accept a decimal
                # (e.g., a number, not a space group or atom label)
                is_numeric = current_token.replace('-', '').replace('/', '').isdigit()
                # Don't merge if current_token ends with ')' (like "Ca(1).")
                ends_with_paren = current_token.endswith(')')
                # Don't merge if current token is a space group (short and has special chars)
                is_space_group = len(current_token) <= 6 and any(c in current_token for c in ['-', '/'])

                if is_numeric and not is_space_group and not ends_with_paren:
                    # This looks like a decimal number
                    current_token += token
                    current_indices.append(i)
                else:
                    # Save previous token and start new with period
                    if current_token:
                        merged_tokens.append(current_token)
                        token_mapping.append(current_indices)
                        in_space_group = False
                        in_coordinate = False
                        in_parentheses = False
                        after_closing_paren = False
                        in_hyphenated_word = False
                    current_token = token
                    current_indices = [i]
            elif in_hyphenated_word and token.replace('##', '').isalpha():
                # Continue merging hyphenated compound word (like "saw" in "see-saw-like")
                current_token += token
                current_indices.append(i)
                # Reset flag after merging the word part
                # The next hyphen will be checked again to see if it should continue the compound
                in_hyphenated_word = False
            else:
                # Save previous token if exists (unless we're continuing a hyphenated word)
                if current_token and not in_hyphenated_word:
                    merged_tokens.append(current_token)
                    token_mapping.append(current_indices)
                    in_space_group = False
                    in_coordinate = False
                    in_parentheses = False
                    after_closing_paren = False
                    # Start new token
                    current_token = token
                    current_indices = [i]
                elif current_token and in_hyphenated_word:
                    # Reset hyphenated word flag if next token is not alphabetic or hyphen
                    # This handles the end of the hyphenated word
                    merged_tokens.append(current_token)
                    token_mapping.append(current_indices)
                    in_space_group = False
                    in_coordinate = False
                    in_parentheses = False
                    after_closing_paren = False
                    in_hyphenated_word = False
                    current_token = token
                    current_indices = [i]
                else:
                    current_token = token
                    current_indices = [i]
                # Check if starting a space group (case-insensitive)
                if token.upper() in space_group_starters:
                    in_space_group = True
                else:
                    in_space_group = False

        # Don't forget the last token
        if current_token:
            merged_tokens.append(current_token)
            token_mapping.append(current_indices)

        # Merge weights by taking maximum over grouped indices
        # Using max preserves the strongest attention signal in merged tokens
        if weights is not None and len(weights.shape) >= 1:
            # Handle different weight shapes
            if len(weights.shape) == 1:
                # [seq_len]
                merged_weights = np.array([weights[indices].max() for indices in token_mapping])
            elif len(weights.shape) == 2:
                # [num_atoms, seq_len] or [seq_len, num_atoms]
                if weights.shape[-1] == len(tokens):
                    # Last dim is seq_len
                    merged_weights = np.array([[weights[i, indices].max() for indices in token_mapping]
                                               for i in range(weights.shape[0])])
                else:
                    # First dim is seq_len
                    merged_weights = np.array([[weights[indices, i].max() for indices in token_mapping]
                                               for i in range(weights.shape[1])]).T
            else:
                merged_weights = weights  # Don't merge for complex shapes
        else:
            merged_weights = weights

        return merged_tokens, merged_weights, token_mapping

    def is_stopword(self, word):
        """æ£€æŸ¥æ˜¯å¦ä¸ºåœç”¨è¯ï¼ˆåŒ…æ‹¬æ‰€æœ‰æ ‡ç‚¹ç¬¦å·ï¼‰"""
        word_lower = word.lower().strip()

        # ç©ºå­—ç¬¦ä¸²
        if not word_lower:
            return True

        # æ£€æŸ¥æ˜¯å¦åœ¨åœç”¨è¯åˆ—è¡¨ä¸­
        if word_lower in self.STOPWORDS or word in self.STOPWORDS:
            return True

        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ ‡ç‚¹ç¬¦å·å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ï¼š"...", "---", "!!!"ï¼‰
        # å®šä¹‰æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å­—ç¬¦ï¼ˆåŒ…æ‹¬ä¸­è‹±æ–‡ï¼‰
        punctuation_chars = set('.,:;!?()[]{}"\'-â€”â€“_/\\|+*&%$#@^~`<>ï¼Œã€‚ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€ã€‘ã€Šã€‹""''ã€Â·â€¦â€¦ï½')
        if all(c in punctuation_chars for c in word):
            return True

        # æ£€æŸ¥æ˜¯å¦ä¸ºWordPieceç¢ç‰‡ï¼ˆä»¥##å¼€å¤´ä¸”é•¿åº¦å°äº5ï¼‰
        if word.startswith('##') and len(word) < 5:
            return True

        # æ£€æŸ¥æ˜¯å¦ä¸ºå•ä¸ªå­—ç¬¦ï¼ˆé™¤äº†å¤§å†™å­—æ¯å¦‚å…ƒç´ ç¬¦å·O/N/Cï¼Œæˆ–å¸Œè…Šå­—æ¯Î±/Î²/Î³ï¼‰
        if len(word_lower) == 1:
            # ä¿ç•™ASCIIå¤§å†™å­—æ¯ï¼ˆå…ƒç´ ç¬¦å·ï¼‰
            if word.isupper():
                return False
            # ä¿ç•™å¸Œè…Šå­—æ¯ (U+03B1-U+03C9 å°å†™, U+0391-U+03A9 å¤§å†™)
            char_code = ord(word_lower)
            if 0x03B1 <= char_code <= 0x03C9 or 0x0391 <= char_code <= 0x03A9:
                return False
            # å…¶ä»–å•å­—ç¬¦éƒ½è¿‡æ»¤
            return True

        return False

    def filter_stopwords_from_analysis(self, word_importance_pairs):
        """è¿‡æ»¤åˆ†æç»“æœä¸­çš„åœç”¨è¯

        Args:
            word_importance_pairs: [(word, importance), ...] åˆ—è¡¨

        Returns:
            è¿‡æ»¤åçš„åˆ—è¡¨
        """
        return [(word, imp) for word, imp in word_importance_pairs
                if not self.is_stopword(word)]

    def extract_attention_weights(self, g, lg, text, return_prediction=True):
        """
        æå–è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡ï¼ˆå®Œæ•´ç‰ˆï¼‰

        Args:
            g: DGL graph
            lg: Line graph
            text: æ–‡æœ¬åˆ—è¡¨
            return_prediction: æ˜¯å¦è¿”å›é¢„æµ‹å€¼

        Returns:
            DictåŒ…å«:
                - attention_weights: æ³¨æ„åŠ›æƒé‡å­—å…¸
                - prediction: é¢„æµ‹å€¼
                - graph_features: å›¾ç‰¹å¾
                - text_features: æ–‡æœ¬ç‰¹å¾
        """
        self.model.eval()

        with torch.no_grad():
            # ğŸ”‘ å…³é”®ï¼šä½¿ç”¨ return_attention=True
            output = self.model(
                [g.to(self.device), lg.to(self.device), text],
                return_features=True,
                return_attention=True  # è¿”å›æ³¨æ„åŠ›æƒé‡
            )

        result = {}

        if isinstance(output, dict):
            result['prediction'] = output['predictions'].cpu().numpy()
            result['graph_features'] = output.get('graph_features', None)
            result['text_features'] = output.get('text_features', None)

            # æå–å…¨å±€æ³¨æ„åŠ›æƒé‡ï¼ˆå‘åå…¼å®¹ï¼‰
            if 'attention_weights' in output:
                attn = output['attention_weights']
                result['attention_weights'] = {
                    'graph_to_text': attn.get('graph_to_text', None),
                    'text_to_graph': attn.get('text_to_graph', None)
                }
            else:
                result['attention_weights'] = None

            # æå–ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡ï¼ˆæ–°å¢ï¼‰
            if 'fine_grained_attention_weights' in output:
                fg_attn = output['fine_grained_attention_weights']
                result['fine_grained_attention_weights'] = {
                    'atom_to_text': fg_attn.get('atom_to_text', None),  # [batch, heads, num_atoms, seq_len]
                    'text_to_atom': fg_attn.get('text_to_atom', None)   # [batch, heads, seq_len, num_atoms]
                }
            else:
                result['fine_grained_attention_weights'] = None
        else:
            result['prediction'] = output.cpu().numpy()
            result['attention_weights'] = None

        return result

    def compute_atom_importance(self, g, lg, text, method='gradient', target_class=None):
        """
        è®¡ç®—åŸå­é‡è¦æ€§åˆ†æ•°

        Args:
            g: DGL graph
            lg: Line graph
            text: æ–‡æœ¬
            method: 'gradient' æˆ– 'integrated_gradients'
            target_class: ç›®æ ‡ç±»åˆ«ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰

        Returns:
            importance_scores: [num_atoms] é‡è¦æ€§åˆ†æ•°
        """
        self.model.eval()

        if method == 'gradient':
            return self._gradient_importance(g, lg, text, target_class)
        elif method == 'integrated_gradients':
            return self._integrated_gradients(g, lg, text, target_class)
        else:
            raise ValueError(f"Unknown method: {method}")

    def _gradient_importance(self, g, lg, text, target_class=None):
        """æ¢¯åº¦æ³•è®¡ç®—åŸå­é‡è¦æ€§"""
        g = g.to(self.device)
        lg = lg.to(self.device)

        # å¯ç”¨æ¢¯åº¦
        node_features = g.ndata['atom_features'].clone().detach().requires_grad_(True)
        original_features = g.ndata['atom_features']
        g.ndata['atom_features'] = node_features

        # Forward
        output = self.model([g, lg, text])

        if isinstance(output, dict):
            prediction = output['predictions']
        else:
            prediction = output

        # é€‰æ‹©ç›®æ ‡è¾“å‡º
        if target_class is not None:
            if prediction.dim() > 1:
                prediction = prediction[:, target_class]

        # å¯¹é¢„æµ‹æ±‚å’Œï¼ˆå¦‚æœæ˜¯batchï¼‰
        loss = prediction.sum()

        # Backward
        loss.backward()

        # è®¡ç®—é‡è¦æ€§ï¼ˆæ¢¯åº¦L2èŒƒæ•°ï¼‰
        gradients = node_features.grad
        importance = torch.norm(gradients, dim=1).cpu().numpy()

        # æ¢å¤
        g.ndata['atom_features'] = original_features

        return importance

    def _integrated_gradients(self, g, lg, text, target_class=None, steps=50):
        """ç§¯åˆ†æ¢¯åº¦æ³•è®¡ç®—åŸå­é‡è¦æ€§"""
        g = g.to(self.device)
        lg = lg.to(self.device)

        original_features = g.ndata['atom_features'].clone()
        baseline = torch.zeros_like(original_features)

        integrated_grads = torch.zeros_like(original_features)

        for alpha in torch.linspace(0, 1, steps):
            # æ’å€¼
            interpolated = baseline + alpha * (original_features - baseline)
            interpolated = interpolated.clone().detach().requires_grad_(True)
            g.ndata['atom_features'] = interpolated

            # Forward
            output = self.model([g, lg, text])
            if isinstance(output, dict):
                prediction = output['predictions']
            else:
                prediction = output

            if target_class is not None:
                if prediction.dim() > 1:
                    prediction = prediction[:, target_class]

            loss = prediction.sum()
            loss.backward()

            integrated_grads += interpolated.grad

        # å¹³å‡å¹¶ç¼©æ”¾
        integrated_grads = integrated_grads / steps
        importance = torch.norm(integrated_grads * (original_features - baseline), dim=1)

        # æ¢å¤
        g.ndata['atom_features'] = original_features

        return importance.cpu().numpy()

    def visualize_cross_modal_attention(
        self,
        attention_weights,
        atom_symbols=None,
        text_tokens=None,
        save_path=None,
        figsize=(14, 10)
    ):
        """
        å¯è§†åŒ–è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡ï¼ˆå¢å¼ºç‰ˆï¼‰

        Args:
            attention_weights: æ³¨æ„åŠ›æƒé‡å­—å…¸
            atom_symbols: åŸå­ç¬¦å·åˆ—è¡¨
            text_tokens: æ–‡æœ¬tokenåˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„
            figsize: å›¾åƒå¤§å°
        """
        if attention_weights is None:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„æ³¨æ„åŠ›æƒé‡ï¼ˆæ¨¡å‹å¯èƒ½æœªå¯ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼‰")
            return

        fig = plt.figure(figsize=figsize)

        # 1. Graph-to-Text Attention
        if 'graph_to_text' in attention_weights and attention_weights['graph_to_text'] is not None:
            ax1 = fig.add_subplot(211)

            g2t_attn = attention_weights['graph_to_text']
            # [batch, heads, 1, 1] -> å–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œå¹³å‡æ‰€æœ‰å¤´
            if g2t_attn.dim() == 4:
                g2t_attn = g2t_attn[0].mean(dim=0).cpu().numpy()  # [1, 1]
            else:
                g2t_attn = g2t_attn.cpu().numpy()

            sns.heatmap(
                g2t_attn,
                cmap='YlOrRd',
                annot=True,
                fmt='.3f',
                cbar_kws={'label': 'Attention Weight'},
                ax=ax1
            )
            ax1.set_title('Graph-to-Text Attention\n(å›¾å…³æ³¨æ–‡æœ¬çš„å¼ºåº¦)', fontsize=12, fontweight='bold')
            ax1.set_xlabel('Text Features')
            ax1.set_ylabel('Graph Features')

        # 2. Text-to-Graph Attention
        if 'text_to_graph' in attention_weights and attention_weights['text_to_graph'] is not None:
            ax2 = fig.add_subplot(212)

            t2g_attn = attention_weights['text_to_graph']
            if t2g_attn.dim() == 4:
                t2g_attn = t2g_attn[0].mean(dim=0).cpu().numpy()
            else:
                t2g_attn = t2g_attn.cpu().numpy()

            sns.heatmap(
                t2g_attn,
                cmap='YlOrRd',
                annot=True,
                fmt='.3f',
                cbar_kws={'label': 'Attention Weight'},
                ax=ax2
            )
            ax2.set_title('Text-to-Graph Attention\n(æ–‡æœ¬å…³æ³¨å›¾çš„å¼ºåº¦)', fontsize=12, fontweight='bold')
            ax2.set_xlabel('Graph Features')
            ax2.set_ylabel('Text Features')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        else:
            plt.show()

        plt.close()

    def visualize_attention_by_heads(
        self,
        attention_weights,
        save_path=None,
        figsize=(16, 4)
    ):
        """
        æŒ‰æ³¨æ„åŠ›å¤´åˆ†åˆ«å¯è§†åŒ–

        Args:
            attention_weights: æ³¨æ„åŠ›æƒé‡å­—å…¸
            save_path: ä¿å­˜è·¯å¾„
            figsize: å›¾åƒå¤§å°
        """
        if attention_weights is None or 'graph_to_text' not in attention_weights:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„å¤šå¤´æ³¨æ„åŠ›æƒé‡")
            return

        g2t = attention_weights['graph_to_text']  # [batch, heads, 1, 1]

        if g2t.dim() != 4:
            print("âš ï¸  æ³¨æ„åŠ›æƒé‡ç»´åº¦ä¸ç¬¦åˆå¤šå¤´æ ¼å¼")
            return

        num_heads = g2t.shape[1]
        g2t = g2t[0].cpu().numpy()  # [heads, 1, 1]

        fig, axes = plt.subplots(1, num_heads, figsize=figsize)

        for i in range(num_heads):
            ax = axes[i] if num_heads > 1 else axes

            sns.heatmap(
                g2t[i],
                cmap='YlOrRd',
                annot=True,
                fmt='.3f',
                cbar=False,
                ax=ax
            )
            ax.set_title(f'Head {i+1}', fontweight='bold')
            ax.set_xlabel('Text')
            ax.set_ylabel('Graph')

        plt.suptitle('Multi-Head Attention Weights (Graph â†’ Text)',
                    fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… å¤šå¤´æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        else:
            plt.show()

        plt.close()

    def visualize_atom_importance(
        self,
        atoms_object,
        importance_scores,
        save_path=None,
        top_k=10,
        figsize=(16, 5)
    ):
        """
        å¯è§†åŒ–åŸå­é‡è¦æ€§ï¼ˆå¢å¼ºç‰ˆï¼‰

        Args:
            atoms_object: jarvis.core.atoms.Atomså¯¹è±¡
            importance_scores: é‡è¦æ€§åˆ†æ•°
            save_path: ä¿å­˜è·¯å¾„
            top_k: æ˜¾ç¤ºtop-ké‡è¦åŸå­
            figsize: å›¾åƒå¤§å°

        Returns:
            df: åŒ…å«åŸå­ä¿¡æ¯å’Œé‡è¦æ€§çš„DataFrame
        """
        # å½’ä¸€åŒ–
        importance_scores = (importance_scores - importance_scores.min()) / \
                          (importance_scores.max() - importance_scores.min() + 1e-8)

        # åˆ›å»ºDataFrame
        elements = list(atoms_object.elements)
        coords = atoms_object.cart_coords

        df = pd.DataFrame({
            'Index': range(len(elements)),
            'Element': elements,
            'X': coords[:, 0],
            'Y': coords[:, 1],
            'Z': coords[:, 2],
            'Importance': importance_scores
        })

        df = df.sort_values('Importance', ascending=False).reset_index(drop=True)

        # æ‰“å°Top-k
        print(f"\n{'='*70}")
        print(f"Top {top_k} Most Important Atoms")
        print(f"{'='*70}")
        print(df.head(top_k)[['Index', 'Element', 'Importance']].to_string(index=False))
        print(f"{'='*70}\n")

        # å¯è§†åŒ–
        fig = plt.figure(figsize=figsize)

        # 1. é‡è¦æ€§åˆ†å¸ƒ
        ax1 = fig.add_subplot(131)
        bars = ax1.bar(range(len(importance_scores)), importance_scores,
                      color=plt.cm.YlOrRd(importance_scores))
        ax1.set_xlabel('Atom Index', fontsize=11)
        ax1.set_ylabel('Importance Score', fontsize=11)
        ax1.set_title('Atom Importance Distribution', fontsize=12, fontweight='bold')
        ax1.grid(True, alpha=0.3, linestyle='--')

        # é«˜äº®top-k
        top_indices = df.head(top_k)['Index'].values
        for idx in top_indices:
            ax1.axvline(idx, color='red', alpha=0.3, linestyle='--', linewidth=1)

        # 2. æŒ‰å…ƒç´ ç±»å‹ç»Ÿè®¡
        ax2 = fig.add_subplot(132)
        element_stats = df.groupby('Element')['Importance'].agg(['mean', 'std', 'count'])
        element_stats = element_stats.sort_values('mean', ascending=False)

        x_pos = np.arange(len(element_stats))
        ax2.barh(x_pos, element_stats['mean'].values,
                color=plt.cm.viridis(np.linspace(0, 1, len(element_stats))))
        ax2.set_yticks(x_pos)
        ax2.set_yticklabels([f"{elem} (n={int(count)})"
                             for elem, count in zip(element_stats.index, element_stats['count'])])
        ax2.set_xlabel('Average Importance', fontsize=11)
        ax2.set_title('Importance by Element Type', fontsize=12, fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='x', linestyle='--')

        # 3. ç©ºé—´åˆ†å¸ƒï¼ˆ2DæŠ•å½±ï¼‰
        ax3 = fig.add_subplot(133)
        scatter = ax3.scatter(
            coords[:, 0], coords[:, 1],
            c=importance_scores,
            s=300,
            cmap='YlOrRd',
            alpha=0.7,
            edgecolors='black',
            linewidth=1.5
        )

        # æ ‡æ³¨top-kåŸå­
        for idx in top_indices[:min(top_k, 5)]:  # åªæ ‡æ³¨å‰5ä¸ªé¿å…æ‹¥æŒ¤
            ax3.annotate(
                f"{elements[idx]}",
                (coords[idx, 0], coords[idx, 1]),
                fontsize=10,
                fontweight='bold',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7)
            )

        ax3.set_xlabel('X Coordinate (Ã…)', fontsize=11)
        ax3.set_ylabel('Y Coordinate (Ã…)', fontsize=11)
        ax3.set_title('Spatial Distribution (X-Y Projection)', fontsize=12, fontweight='bold')
        cbar = plt.colorbar(scatter, ax=ax3, label='Importance Score')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… åŸå­é‡è¦æ€§å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        else:
            plt.show()

        plt.close()

        return df

    def visualize_feature_space(
        self,
        graph_features,
        text_features,
        labels=None,
        predictions=None,
        method='tsne',
        save_path=None,
        figsize=(14, 6)
    ):
        """
        å¯è§†åŒ–ç‰¹å¾ç©ºé—´ï¼ˆå¢å¼ºç‰ˆï¼‰

        Args:
            graph_features: å›¾ç‰¹å¾ [N, D]
            text_features: æ–‡æœ¬ç‰¹å¾ [N, D]
            labels: çœŸå®æ ‡ç­¾
            predictions: é¢„æµ‹å€¼
            method: 'tsne' æˆ– 'pca'
            save_path: ä¿å­˜è·¯å¾„
            figsize: å›¾åƒå¤§å°
        """
        from sklearn.manifold import TSNE
        from sklearn.decomposition import PCA

        # è½¬æ¢ä¸ºnumpy
        if isinstance(graph_features, torch.Tensor):
            graph_features = graph_features.cpu().numpy()
        if isinstance(text_features, torch.Tensor):
            text_features = text_features.cpu().numpy()

        # åˆå¹¶ç‰¹å¾
        all_features = np.vstack([graph_features, text_features])

        # é™ç»´
        print(f"\nğŸ”„ ä½¿ç”¨ {method.upper()} è¿›è¡Œé™ç»´...")
        if method == 'tsne':
            reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_features)-1))
        else:
            reducer = PCA(n_components=2)

        embedded = reducer.fit_transform(all_features)

        # åˆ†ç¦»
        n = len(graph_features)
        graph_emb = embedded[:n]
        text_emb = embedded[n:]

        # å¯è§†åŒ–
        fig = plt.figure(figsize=figsize)

        # å·¦å›¾ï¼šæ¨¡æ€åˆ†ç¦»
        ax1 = fig.add_subplot(121)
        ax1.scatter(graph_emb[:, 0], graph_emb[:, 1],
                   c='#3498db', alpha=0.6, s=80, label='Graph',
                   marker='o', edgecolors='black', linewidth=0.5)
        ax1.scatter(text_emb[:, 0], text_emb[:, 1],
                   c='#e74c3c', alpha=0.6, s=80, label='Text',
                   marker='^', edgecolors='black', linewidth=0.5)

        # ç»˜åˆ¶é…å¯¹è¿çº¿ï¼ˆé‡‡æ ·é¿å…è¿‡å¯†ï¼‰
        sample_size = min(50, n)
        sample_indices = np.random.choice(n, sample_size, replace=False)
        for i in sample_indices:
            ax1.plot([graph_emb[i, 0], text_emb[i, 0]],
                    [graph_emb[i, 1], text_emb[i, 1]],
                    'gray', alpha=0.2, linewidth=0.8)

        ax1.set_xlabel(f'{method.upper()} Component 1', fontsize=11)
        ax1.set_ylabel(f'{method.upper()} Component 2', fontsize=11)
        ax1.set_title('Graph-Text Feature Alignment', fontsize=12, fontweight='bold')
        ax1.legend(fontsize=10, loc='best')
        ax1.grid(True, alpha=0.3, linestyle='--')

        # å³å›¾ï¼šæŒ‰æ ‡ç­¾ç€è‰²
        ax2 = fig.add_subplot(122)
        if labels is not None:
            if isinstance(labels, torch.Tensor):
                labels = labels.cpu().numpy()

            scatter1 = ax2.scatter(graph_emb[:, 0], graph_emb[:, 1],
                                  c=labels, cmap='viridis', alpha=0.6, s=80,
                                  marker='o', edgecolors='black', linewidth=0.5)
            scatter2 = ax2.scatter(text_emb[:, 0], text_emb[:, 1],
                                  c=labels, cmap='viridis', alpha=0.6, s=80,
                                  marker='^', edgecolors='black', linewidth=0.5)
            cbar = plt.colorbar(scatter1, ax=ax2, label='Target Value')
            ax2.set_title('Features Colored by Target', fontsize=12, fontweight='bold')
        else:
            ax2.scatter(graph_emb[:, 0], graph_emb[:, 1],
                       c='#3498db', alpha=0.6, s=80, label='Graph', marker='o')
            ax2.scatter(text_emb[:, 0], text_emb[:, 1],
                       c='#e74c3c', alpha=0.6, s=80, label='Text', marker='^')
            ax2.set_title('Feature Distribution', fontsize=12, fontweight='bold')
            ax2.legend()

        ax2.set_xlabel(f'{method.upper()} Component 1', fontsize=11)
        ax2.set_ylabel(f'{method.upper()} Component 2', fontsize=11)
        ax2.grid(True, alpha=0.3, linestyle='--')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… ç‰¹å¾ç©ºé—´å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        else:
            plt.show()

        plt.close()

    def explain_single_prediction(
        self,
        g, lg, text,
        atoms_object,
        true_value=None,
        save_dir=None,
        sample_id='sample'
    ):
        """
        ä¸ºå•ä¸ªæ ·æœ¬ç”Ÿæˆå®Œæ•´è§£é‡ŠæŠ¥å‘Š

        Args:
            g, lg, text: æ¨¡å‹è¾“å…¥
            atoms_object: Atomså¯¹è±¡
            true_value: çœŸå®å€¼
            save_dir: ä¿å­˜ç›®å½•
            sample_id: æ ·æœ¬ID

        Returns:
            explanation: è§£é‡Šå­—å…¸
        """
        if save_dir:
            save_dir = Path(save_dir)
            save_dir.mkdir(parents=True, exist_ok=True)

        print(f"\n{'='*80}")
        print(f"ğŸ” æ ·æœ¬ {sample_id} çš„å¯è§£é‡Šæ€§åˆ†æ")
        print(f"{'='*80}")

        explanation = {}

        # 1. æå–æ³¨æ„åŠ›å’Œé¢„æµ‹
        print("\n1ï¸âƒ£  æå–æ³¨æ„åŠ›æƒé‡å’Œé¢„æµ‹...")
        result = self.extract_attention_weights(g, lg, text, return_prediction=True)

        prediction = result['prediction'][0] if len(result['prediction'].shape) > 0 else result['prediction']
        explanation['prediction'] = float(prediction)
        explanation['true_value'] = float(true_value) if true_value is not None else None

        if true_value is not None:
            error = abs(prediction - true_value)
            explanation['error'] = float(error)
            print(f"   é¢„æµ‹å€¼: {prediction:.4f}")
            print(f"   çœŸå®å€¼: {true_value:.4f}")
            print(f"   è¯¯å·®: {error:.4f}")
        else:
            print(f"   é¢„æµ‹å€¼: {prediction:.4f}")

        # 2. å¯è§†åŒ–æ³¨æ„åŠ›
        if result['attention_weights'] is not None:
            print("\n2ï¸âƒ£  å¯è§†åŒ–è·¨æ¨¡æ€æ³¨æ„åŠ›...")
            attn_path = save_dir / f'{sample_id}_attention.png' if save_dir else None
            self.visualize_cross_modal_attention(
                result['attention_weights'],
                save_path=attn_path
            )

            # å¤šå¤´æ³¨æ„åŠ›
            heads_path = save_dir / f'{sample_id}_attention_heads.png' if save_dir else None
            self.visualize_attention_by_heads(
                result['attention_weights'],
                save_path=heads_path
            )
        else:
            print("\n2ï¸âƒ£  âš ï¸  è·¨æ¨¡æ€æ³¨æ„åŠ›æœªå¯ç”¨")

        # 3. è®¡ç®—åŸå­é‡è¦æ€§
        print("\n3ï¸âƒ£  è®¡ç®—åŸå­é‡è¦æ€§...")
        atom_importance_grad = self.compute_atom_importance(g, lg, text, method='gradient')
        explanation['atom_importance'] = atom_importance_grad.tolist()

        # å¯è§†åŒ–
        atom_path = save_dir / f'{sample_id}_atom_importance.png' if save_dir else None
        atom_df = self.visualize_atom_importance(
            atoms_object,
            atom_importance_grad,
            save_path=atom_path,
            top_k=10
        )

        # 4. ä¿å­˜è§£é‡Š
        if save_dir:
            with open(save_dir / f'{sample_id}_explanation.json', 'w') as f:
                json.dump(explanation, f, indent=2)
            print(f"\nâœ… è§£é‡ŠæŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_dir}")

        print(f"\n{'='*80}\n")

        return explanation

    def visualize_fine_grained_attention(
        self,
        attention_weights,
        atoms_object,
        text_tokens,
        save_path=None,
        top_k_atoms=10,
        top_k_words=15,
        show_all_heads=False,
        filter_stopwords=True,
        merge_wordpiece=True
    ):
        """
        å¯è§†åŒ–ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡ï¼ˆåŸå­-æ–‡æœ¬tokençº§åˆ«ï¼‰

        Args:
            attention_weights: ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡å­—å…¸
                - 'atom_to_text': [batch, heads, num_atoms, seq_len]
                - 'text_to_atom': [batch, heads, seq_len, num_atoms]
            atoms_object: Atomså¯¹è±¡ï¼ˆJARVISï¼‰
            text_tokens: æ–‡æœ¬tokensåˆ—è¡¨ï¼ˆè§£ç åçš„è¯è¯­ï¼‰
            save_path: ä¿å­˜è·¯å¾„
            top_k_atoms: æ˜¾ç¤ºtop-ké‡è¦çš„åŸå­
            top_k_words: æ˜¾ç¤ºtop-ké‡è¦çš„è¯è¯­
            show_all_heads: æ˜¯å¦æ˜¾ç¤ºæ‰€æœ‰æ³¨æ„åŠ›å¤´
            filter_stopwords: æ˜¯å¦è¿‡æ»¤åœç”¨è¯ï¼ˆé»˜è®¤Trueï¼‰
            merge_wordpiece: æ˜¯å¦åˆå¹¶WordPiece tokensç”¨äºæ˜¾ç¤ºï¼ˆå¦‚F-43mï¼‰

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        import matplotlib.pyplot as plt
        import seaborn as sns
        import numpy as np

        if attention_weights is None:
            print("âš ï¸  æ²¡æœ‰ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡")
            return None

        atom_to_text = attention_weights.get('atom_to_text', None)
        text_to_atom = attention_weights.get('text_to_atom', None)

        if atom_to_text is None and text_to_atom is None:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡")
            return None

        # Convert to numpy
        if atom_to_text is not None:
            atom_to_text = atom_to_text.cpu().numpy()  # [batch, heads, num_atoms, seq_len]
        if text_to_atom is not None:
            text_to_atom = text_to_atom.cpu().numpy()  # [batch, heads, seq_len, num_atoms]

        # For single sample (batch_size=1)
        if atom_to_text is not None:
            atom_to_text = atom_to_text[0]  # [heads, num_atoms, seq_len]
        if text_to_atom is not None:
            text_to_atom = text_to_atom[0]  # [heads, seq_len, num_atoms]

        num_heads = atom_to_text.shape[0] if atom_to_text is not None else text_to_atom.shape[0]
        num_atoms = atom_to_text.shape[1] if atom_to_text is not None else text_to_atom.shape[2]
        seq_len = atom_to_text.shape[2] if atom_to_text is not None else text_to_atom.shape[1]

        # Get atom elements
        elements = [str(atoms_object.elements[i]) for i in range(num_atoms)]

        # Average over heads
        atom_to_text_avg = atom_to_text.mean(axis=0) if atom_to_text is not None else None  # [num_atoms, seq_len]
        text_to_atom_avg = text_to_atom.mean(axis=0) if text_to_atom is not None else None  # [seq_len, num_atoms]

        # Merge WordPiece tokens if requested
        original_tokens = text_tokens.copy() if isinstance(text_tokens, list) else list(text_tokens)
        if merge_wordpiece and atom_to_text_avg is not None:
            text_tokens, atom_to_text_avg, token_mapping = self._merge_tokens_and_weights(original_tokens, atom_to_text_avg)
            if text_to_atom_avg is not None:
                # text_to_atom_avg is [seq_len, num_atoms], merge along first dim
                merged_t2a = []
                for indices in token_mapping:
                    merged_t2a.append(text_to_atom_avg[indices, :].max(axis=0))
                text_to_atom_avg = np.array(merged_t2a)
            seq_len = len(text_tokens)
            print(f"   - Merged tokens: {seq_len} (from {len(original_tokens)} original)")

        # Create visualization
        if show_all_heads:
            # Show each head separately
            fig, axes = plt.subplots(2, num_heads//2 + num_heads%2, figsize=(20, 8))
            axes = axes.flatten()

            for head in range(num_heads):
                sns.heatmap(
                    atom_to_text[head],
                    xticklabels=text_tokens[:seq_len],
                    yticklabels=elements,
                    cmap='YlOrRd',
                    ax=axes[head],
                    cbar=True
                )
                axes[head].set_title(f'Head {head+1}')
                axes[head].set_xlabel('Text Tokens')
                axes[head].set_ylabel('Atoms')

            plt.tight_layout()
        else:
            # Show averaged attention with only top-k atoms and words
            fig, axes = plt.subplots(1, 2, figsize=(16, 10))

            # Select top-k atoms and words based on importance
            if atom_to_text_avg is not None:
                # Calculate importance scores
                atom_importance = atom_to_text_avg.mean(axis=1)  # [num_atoms]
                word_importance = atom_to_text_avg.mean(axis=0)  # [seq_len]

                # Get top-k indices
                top_atom_indices = atom_importance.argsort()[-top_k_atoms:][::-1]

                # Filter word indices to exclude stopwords/punctuation
                valid_word_indices = []
                for idx in range(len(word_importance)):
                    if idx < len(text_tokens) and not self.is_stopword(text_tokens[idx]):
                        valid_word_indices.append(idx)

                # Select top-k from valid (non-stopword) words
                if valid_word_indices:
                    valid_word_scores = [(idx, word_importance[idx]) for idx in valid_word_indices]
                    valid_word_scores.sort(key=lambda x: x[1], reverse=True)
                    top_word_indices = [idx for idx, _ in valid_word_scores[:top_k_words]]
                else:
                    # Fallback: if all words are stopwords, use original logic
                    top_word_indices = word_importance.argsort()[-top_k_words:][::-1].tolist()

                # Sort indices for better visualization
                top_atom_indices = np.sort(top_atom_indices)
                top_word_indices = np.sort(top_word_indices)

                # Extract sub-matrix
                atom_to_text_sub = atom_to_text_avg[np.ix_(top_atom_indices, top_word_indices)]

                # Get labels for selected items
                selected_atoms = [f"{elements[i]}-{i}" for i in top_atom_indices]
                selected_words = [text_tokens[i] if i < len(text_tokens) else f"[{i}]" for i in top_word_indices]

                # Atom-to-Text attention heatmap (filtered)
                sns.heatmap(
                    atom_to_text_sub,
                    xticklabels=selected_words,
                    yticklabels=selected_atoms,
                    cmap='YlOrRd',
                    ax=axes[0],
                    cbar=True,
                    annot=top_k_atoms <= 15 and top_k_words <= 15,
                    fmt='.2f' if top_k_atoms <= 15 and top_k_words <= 15 else None
                )
                axes[0].set_title(f'Atom â†’ Text Attention\n(Top {top_k_atoms} atoms Ã— Top {top_k_words} words)', fontsize=12)
                axes[0].set_xlabel('Text Tokens (High Importance)', fontsize=10)
                axes[0].set_ylabel('Atoms (High Importance)', fontsize=10)
                plt.setp(axes[0].get_xticklabels(), rotation=45, ha='right', fontsize=9)
                plt.setp(axes[0].get_yticklabels(), fontsize=9)

            # Text-to-Atom attention heatmap (filtered)
            if text_to_atom_avg is not None:
                # Calculate importance scores
                word_importance_t2a = text_to_atom_avg.mean(axis=1)  # [seq_len]
                atom_importance_t2a = text_to_atom_avg.mean(axis=0)  # [num_atoms]

                # Get top-k indices
                top_atom_indices_t2a = atom_importance_t2a.argsort()[-top_k_atoms:][::-1]

                # Filter word indices to exclude stopwords/punctuation
                valid_word_indices_t2a = []
                for idx in range(len(word_importance_t2a)):
                    if idx < len(text_tokens) and not self.is_stopword(text_tokens[idx]):
                        valid_word_indices_t2a.append(idx)

                # Select top-k from valid (non-stopword) words
                if valid_word_indices_t2a:
                    valid_word_scores_t2a = [(idx, word_importance_t2a[idx]) for idx in valid_word_indices_t2a]
                    valid_word_scores_t2a.sort(key=lambda x: x[1], reverse=True)
                    top_word_indices_t2a = [idx for idx, _ in valid_word_scores_t2a[:top_k_words]]
                else:
                    # Fallback: if all words are stopwords, use original logic
                    top_word_indices_t2a = word_importance_t2a.argsort()[-top_k_words:][::-1].tolist()

                # Sort indices
                top_word_indices_t2a = np.sort(top_word_indices_t2a)
                top_atom_indices_t2a = np.sort(top_atom_indices_t2a)

                # Extract sub-matrix
                text_to_atom_sub = text_to_atom_avg[np.ix_(top_word_indices_t2a, top_atom_indices_t2a)]

                # Get labels
                selected_words_t2a = [text_tokens[i] if i < len(text_tokens) else f"[{i}]" for i in top_word_indices_t2a]
                selected_atoms_t2a = [f"{elements[i]}-{i}" for i in top_atom_indices_t2a]

                sns.heatmap(
                    text_to_atom_sub,
                    xticklabels=selected_atoms_t2a,
                    yticklabels=selected_words_t2a,
                    cmap='YlGnBu',
                    ax=axes[1],
                    cbar=True,
                    annot=top_k_atoms <= 15 and top_k_words <= 15,
                    fmt='.2f' if top_k_atoms <= 15 and top_k_words <= 15 else None
                )
                axes[1].set_title(f'Text â†’ Atom Attention\n(Top {top_k_words} words Ã— Top {top_k_atoms} atoms)', fontsize=12)
                axes[1].set_xlabel('Atoms (High Importance)', fontsize=10)
                axes[1].set_ylabel('Text Tokens (High Importance)', fontsize=10)
                plt.setp(axes[1].get_xticklabels(), rotation=45, ha='right', fontsize=9)
                plt.setp(axes[1].get_yticklabels(), fontsize=9)

            plt.suptitle('Fine-Grained Cross-Modal Attention (Filtered by Importance)', fontsize=14, fontweight='bold')
            plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… ç»†ç²’åº¦æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜: {save_path}")

        plt.close()

        # Analyze patterns
        analysis = {}

        if atom_to_text_avg is not None:
            # Top words attended by each atom (with stopword filtering)
            analysis['atom_top_words'] = {}
            for i, element in enumerate(elements):
                # è·å–æ‰€æœ‰è¯çš„é‡è¦æ€§
                all_words = [(text_tokens[idx], float(atom_to_text_avg[i, idx]))
                            for idx in range(len(text_tokens[:seq_len]))]
                # æŒ‰é‡è¦æ€§æ’åº
                all_words.sort(key=lambda x: x[1], reverse=True)
                # è¿‡æ»¤åœç”¨è¯
                if filter_stopwords:
                    filtered_words = self.filter_stopwords_from_analysis(all_words)
                else:
                    filtered_words = all_words
                analysis['atom_top_words'][f"{element}_{i}"] = filtered_words[:top_k_words]

            # Overall most important words (averaged over all atoms)
            word_importance = atom_to_text_avg.mean(axis=0)  # [seq_len]
            all_words = [(text_tokens[idx], float(word_importance[idx]))
                        for idx in range(len(text_tokens[:seq_len]))]
            all_words.sort(key=lambda x: x[1], reverse=True)
            # è¿‡æ»¤åœç”¨è¯
            if filter_stopwords:
                filtered_words = self.filter_stopwords_from_analysis(all_words)
            else:
                filtered_words = all_words
            analysis['overall_top_words'] = filtered_words[:top_k_words]

        if text_to_atom_avg is not None:
            # Top atoms attended by each word
            analysis['word_top_atoms'] = {}
            for i, token in enumerate(text_tokens[:seq_len]):
                top_atom_indices = text_to_atom_avg[i].argsort()[-top_k_atoms:][::-1]
                top_atoms = [(f"{elements[idx]}_{idx}", text_to_atom_avg[i, idx]) for idx in top_atom_indices]
                analysis['word_top_atoms'][token] = top_atoms

            # Overall most important atoms (averaged over all words)
            atom_importance = text_to_atom_avg.mean(axis=0)  # [num_atoms]
            top_atom_indices = atom_importance.argsort()[-top_k_atoms:][::-1]
            analysis['overall_top_atoms'] = [
                (f"{elements[idx]}_{idx}", atom_importance[idx]) for idx in top_atom_indices
            ]

        return analysis

    def analyze_attention_head_specialization(
        self,
        attention_weights,
        atoms_object,
        text_tokens,
        save_path=None
    ):
        """
        åˆ†ææ³¨æ„åŠ›å¤´çš„ä¸“ä¸šåŒ–æ¨¡å¼

        Args:
            attention_weights: ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡å­—å…¸
            atoms_object: Atomså¯¹è±¡
            text_tokens: æ–‡æœ¬tokensåˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            åˆ†æç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - head_patterns: æ¯ä¸ªå¤´çš„ä¸»è¦å…³æ³¨æ¨¡å¼
            - head_diversity: å¤´ä¹‹é—´çš„å¤šæ ·æ€§åˆ†æ•°
            - head_importance: æ¯ä¸ªå¤´çš„é‡è¦æ€§
        """
        import matplotlib.pyplot as plt
        import seaborn as sns
        import numpy as np
        from scipy.spatial.distance import cosine
        from scipy.stats import entropy

        atom_to_text = attention_weights.get('atom_to_text', None)
        if atom_to_text is None:
            print("âš ï¸  æ²¡æœ‰atom_to_textæ³¨æ„åŠ›æƒé‡")
            return None

        # Convert to numpy and get single sample
        atom_to_text = atom_to_text.cpu().numpy()[0]  # [heads, num_atoms, seq_len]
        num_heads, num_atoms, seq_len = atom_to_text.shape

        # Get atom elements
        elements = [str(atoms_object.elements[i]) for i in range(num_atoms)]

        # Merge WordPiece tokens for better display
        # Use average over all heads for merging
        avg_attn = atom_to_text.mean(axis=0)  # [num_atoms, seq_len]
        merged_tokens, merged_weights, token_mapping = self._merge_tokens_and_weights(text_tokens, avg_attn)

        # Merge attention for each head
        merged_head_attn = []
        for head in range(num_heads):
            head_attn = atom_to_text[head]  # [num_atoms, seq_len]
            # Merge weights for this head
            merged_head = np.array([[head_attn[atom, indices].max() for indices in token_mapping]
                                    for atom in range(num_atoms)])
            merged_head_attn.append(merged_head)
        merged_head_attn = np.array(merged_head_attn)  # [num_heads, num_atoms, merged_seq_len]

        analysis = {
            'head_patterns': {},
            'head_diversity': 0.0,
            'head_importance': {},
            'head_entropy': {}
        }

        # Analyze each head
        head_vectors = []  # For diversity calculation

        for head in range(num_heads):
            head_attn = merged_head_attn[head]  # [num_atoms, merged_seq_len]

            # Calculate entropy (lower = more focused)
            head_entropy_val = entropy(head_attn.flatten() + 1e-10)
            analysis['head_entropy'][f'head_{head+1}'] = float(head_entropy_val)

            # Find top words for this head
            word_importance = head_attn.mean(axis=0)  # Average over atoms
            top_word_indices = word_importance.argsort()[-5:][::-1]
            top_words = [(merged_tokens[idx], float(word_importance[idx])) for idx in top_word_indices]

            # Find top atoms for this head
            atom_importance = head_attn.mean(axis=1)  # Average over words
            top_atom_indices = atom_importance.argsort()[-3:][::-1]
            top_atoms = [(f"{elements[idx]}_{idx}", float(atom_importance[idx])) for idx in top_atom_indices]

            # Overall importance (sum of attention)
            head_importance_score = float(head_attn.sum())
            analysis['head_importance'][f'head_{head+1}'] = head_importance_score

            # Store pattern
            analysis['head_patterns'][f'head_{head+1}'] = {
                'top_words': top_words,
                'top_atoms': top_atoms,
                'entropy': float(head_entropy_val),
                'focus_level': 'high' if head_entropy_val < np.median([analysis['head_entropy'][f'head_{h+1}'] for h in range(head+1)]) else 'low'
            }

            # Store flattened attention for diversity calculation
            head_vectors.append(head_attn.flatten())

        # Calculate head diversity (average pairwise cosine distance)
        head_vectors = np.array(head_vectors)
        diversity_scores = []
        for i in range(num_heads):
            for j in range(i+1, num_heads):
                dist = cosine(head_vectors[i], head_vectors[j])
                diversity_scores.append(dist)

        analysis['head_diversity'] = float(np.mean(diversity_scores)) if diversity_scores else 0.0

        # Visualize head specialization
        if save_path:
            fig, axes = plt.subplots(2, 2, figsize=(14, 10))

            # 1. Head importance bar chart
            heads = list(analysis['head_importance'].keys())
            importances = list(analysis['head_importance'].values())
            axes[0, 0].bar(range(len(heads)), importances, color='steelblue')
            axes[0, 0].set_xticks(range(len(heads)))
            axes[0, 0].set_xticklabels(heads, rotation=45)
            axes[0, 0].set_ylabel('Total Attention Weight')
            axes[0, 0].set_title('Attention Head Importance')
            axes[0, 0].grid(axis='y', alpha=0.3)

            # 2. Head entropy (focus level)
            entropies = list(analysis['head_entropy'].values())
            axes[0, 1].bar(range(len(heads)), entropies, color='coral')
            axes[0, 1].set_xticks(range(len(heads)))
            axes[0, 1].set_xticklabels(heads, rotation=45)
            axes[0, 1].set_ylabel('Entropy (lower = more focused)')
            axes[0, 1].set_title('Attention Head Focus Level')
            axes[0, 1].grid(axis='y', alpha=0.3)

            # 3. Top words per head heatmap
            top_words_matrix = np.zeros((num_heads, 5))
            word_labels = []
            for head in range(num_heads):
                head_key = f'head_{head+1}'
                top_words = analysis['head_patterns'][head_key]['top_words']
                for i, (word, score) in enumerate(top_words):
                    top_words_matrix[head, i] = score
                    if head == 0:
                        word_labels.append(word[:15])  # Truncate long words

            sns.heatmap(top_words_matrix, ax=axes[1, 0], cmap='YlOrRd',
                       xticklabels=word_labels, yticklabels=heads,
                       annot=True, fmt='.3f', cbar_kws={'label': 'Attention Weight'})
            axes[1, 0].set_title('Top Words per Head')
            axes[1, 0].set_xlabel('Words')
            axes[1, 0].set_ylabel('Attention Head')

            # 4. Head diversity visualization (similarity matrix)
            similarity_matrix = np.zeros((num_heads, num_heads))
            for i in range(num_heads):
                for j in range(num_heads):
                    if i == j:
                        similarity_matrix[i, j] = 1.0
                    else:
                        similarity_matrix[i, j] = 1.0 - cosine(head_vectors[i], head_vectors[j])

            sns.heatmap(similarity_matrix, ax=axes[1, 1], cmap='coolwarm',
                       xticklabels=heads, yticklabels=heads,
                       annot=True, fmt='.2f', vmin=0, vmax=1,
                       cbar_kws={'label': 'Similarity (1-cosine distance)'})
            axes[1, 1].set_title(f'Head Similarity Matrix\nDiversity Score: {analysis["head_diversity"]:.3f}')
            axes[1, 1].set_xlabel('Attention Head')
            axes[1, 1].set_ylabel('Attention Head')

            plt.suptitle('Attention Head Specialization Analysis', fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… æ³¨æ„åŠ›å¤´ä¸“ä¸šåŒ–åˆ†æå·²ä¿å­˜: {save_path}")
            plt.close()

        return analysis

    def analyze_key_atom_word_pairs(
        self,
        attention_weights,
        atoms_object,
        text_tokens,
        top_k=20,
        save_path=None,
        filter_stopwords=True
    ):
        """
        è¯†åˆ«æœ€å…³é”®çš„åŸå­-è¯è¯­å¯¹åŠå…¶è¯­ä¹‰ç±»åˆ«

        Args:
            attention_weights: ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡å­—å…¸
            atoms_object: Atomså¯¹è±¡
            text_tokens: æ–‡æœ¬tokensåˆ—è¡¨
            top_k: è¿”å›top-kå¯¹
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            filter_stopwords: æ˜¯å¦è¿‡æ»¤åœç”¨è¯ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            åˆ†æç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - top_pairs: æœ€å¼ºçš„åŸå­-è¯è¯­å¯¹
            - semantic_categories: è‡ªåŠ¨åˆ†ç±»çš„è¯­ä¹‰ç±»åˆ«
        """
        import matplotlib.pyplot as plt
        import numpy as np

        atom_to_text = attention_weights.get('atom_to_text', None)
        if atom_to_text is None:
            print("âš ï¸  æ²¡æœ‰atom_to_textæ³¨æ„åŠ›æƒé‡")
            return None

        # Convert to numpy and get single sample
        atom_to_text = atom_to_text.cpu().numpy()[0]  # [heads, num_atoms, seq_len]

        # Average over heads
        atom_to_text_avg = atom_to_text.mean(axis=0)  # [num_atoms, seq_len]

        # Merge WordPiece tokens for better display
        merged_tokens, merged_weights, token_mapping = self._merge_tokens_and_weights(text_tokens, atom_to_text_avg)
        text_tokens = merged_tokens
        atom_to_text_avg = merged_weights

        num_atoms, seq_len = atom_to_text_avg.shape

        # Get atom elements
        elements = [str(atoms_object.elements[i]) for i in range(num_atoms)]

        # Find top-k atom-word pairs (with optional stopword filtering)
        flat_attention = atom_to_text_avg.flatten()
        # è·å–æ‰€æœ‰ç´¢å¼•æŒ‰æ³¨æ„åŠ›æƒé‡æ’åº
        sorted_indices = flat_attention.argsort()[::-1]

        top_pairs = []
        for idx in sorted_indices:
            if len(top_pairs) >= top_k:
                break

            atom_idx = idx // seq_len
            word_idx = idx % seq_len
            word = text_tokens[word_idx]

            # è¿‡æ»¤åœç”¨è¯
            if filter_stopwords and self.is_stopword(word):
                continue

            atom_name = f"{elements[atom_idx]}_{atom_idx}"
            weight = float(atom_to_text_avg[atom_idx, word_idx])
            top_pairs.append({
                'atom': atom_name,
                'word': word,
                'weight': weight,
                'atom_idx': int(atom_idx),
                'word_idx': int(word_idx)
            })

        # Semantic categorization (rule-based with detailed categories)
        # Element identification
        element_keywords = set(['mg', 'sn', 'ge', 'o', 'na', 'ba', 'bi', 'si', 'al', 'fe', 'cu', 'zn',
                               'li', 'hf', 'k', 'ca', 'ti', 'v', 'cr', 'mn', 'co', 'ni', 'ga', 'as',
                               'se', 'br', 'rb', 'sr', 'y', 'zr', 'nb', 'mo', 'ag', 'cd', 'in', 'sb',
                               'te', 'cs', 'la', 'ce', 'pr', 'nd', 'sm', 'eu', 'gd', 'tb', 'dy', 'ho',
                               'er', 'tm', 'yb', 'lu', 'ta', 'w', 're', 'os', 'ir', 'pt', 'au', 'hg',
                               'tl', 'pb', 'po', 'at', 'rn', 'fr', 'ra', 'ac', 'th', 'pa', 'u', 'np', 'pu'])

        # Crystal structure and symmetry
        structure_keywords = set(['cubic', 'monoclinic', 'orthorhombic', 'hexagonal', 'tetragonal',
                                  'triclinic', 'rhombohedral', 'trigonal', 'symmetry', 'lattice', 'cell',
                                  'crystallizes', 'crystal', 'phase'])

        # Space group related
        space_group_keywords = set(['group', 'space', 'p-1', 'pm-3m', 'fm-3m', 'fd-3m', 'i4/mmm',
                                   'p63/mmc', 'r-3m', 'c2/m', 'cmcm', 'pnma', 'pbca'])

        # Bonding and coordination
        bonding_keywords = set(['bond', 'bonded', 'bonding', 'length', 'distance', 'Ã¥', 'coordination',
                               'coordinate', 'valence', 'oxidation'])

        # Geometry and polyhedra
        geometry_keywords = set(['geometry', 'octahedral', 'tetrahedral', 'planar', 'trigonal',
                                'square', 'pyramidal', 'bipyramidal', 'prismatic', 'antiprismatic',
                                'linear', 'bent', 'seesaw', 't-shaped'])

        # Polyhedral connectivity
        connectivity_keywords = set(['sharing', 'corner', 'edge', 'face', 'vertex', 'connected',
                                    'linked', 'bridging', 'terminal'])

        # Distortion and disorder
        distortion_keywords = set(['distorted', 'distortion', 'tilted', 'rotated', 'displaced',
                                  'disordered', 'ordered', 'regular', 'irregular'])

        # Framework and structural units
        framework_keywords = set(['framework', 'cluster', 'chain', 'layer', 'sheet', 'network',
                                 'cage', 'channel', 'pore', 'void', 'consists', 'composed'])

        # Equivalence and multiplicity
        equivalence_keywords = set(['equivalent', 'inequivalent', 'unique', 'distinct', 'identical',
                                   'similar', 'different'])

        # Composition and formula
        composition_keywords = set(['formula', 'composition', 'stoichiometry', 'ratio', 'content',
                                   'concentration', 'doping', 'substitution'])

        semantic_categories = {
            'element_identification': [],
            'structure_symmetry': [],
            'space_group': [],
            'bonding_coordination': [],
            'geometry_polyhedra': [],
            'connectivity': [],
            'distortion': [],
            'framework_units': [],
            'equivalence': [],
            'composition': [],
            'other': []
        }

        for pair in top_pairs:
            word_lower = pair['word'].lower().replace('#', '').replace('-', '')
            categorized = False

            # Check each category
            if word_lower in element_keywords or any(word_lower.startswith(elem) for elem in element_keywords if len(elem) <= 2):
                semantic_categories['element_identification'].append(pair)
                categorized = True
            elif word_lower in space_group_keywords or any(sg in pair['word'].lower() for sg in ['43m', '63', 'mmm', '3m']):
                semantic_categories['space_group'].append(pair)
                categorized = True
            elif word_lower in structure_keywords or 'crystal' in word_lower:
                semantic_categories['structure_symmetry'].append(pair)
                categorized = True
            elif word_lower in bonding_keywords or 'bond' in word_lower or 'coordinate' in word_lower:
                semantic_categories['bonding_coordination'].append(pair)
                categorized = True
            elif word_lower in distortion_keywords or 'distort' in word_lower:
                semantic_categories['distortion'].append(pair)
                categorized = True
            elif word_lower in geometry_keywords or 'hedral' in word_lower:
                semantic_categories['geometry_polyhedra'].append(pair)
                categorized = True
            elif word_lower in connectivity_keywords:
                semantic_categories['connectivity'].append(pair)
                categorized = True
            elif word_lower in framework_keywords:
                semantic_categories['framework_units'].append(pair)
                categorized = True
            elif word_lower in equivalence_keywords:
                semantic_categories['equivalence'].append(pair)
                categorized = True
            elif word_lower in composition_keywords:
                semantic_categories['composition'].append(pair)
                categorized = True

            if not categorized:
                semantic_categories['other'].append(pair)

        # Calculate category statistics
        category_stats = {}
        for cat, pairs in semantic_categories.items():
            if pairs:
                total_weight = sum(p['weight'] for p in pairs)
                category_stats[cat] = {
                    'count': len(pairs),
                    'total_weight': float(total_weight),
                    'percentage': float(len(pairs) / len(top_pairs) * 100)
                }

        analysis = {
            'top_pairs': top_pairs,
            'semantic_categories': semantic_categories,
            'category_stats': category_stats
        }

        # Visualize
        if save_path:
            fig, axes = plt.subplots(1, 2, figsize=(16, 6))

            # 1. Top pairs bar chart
            pairs_labels = [f"{p['atom']}â†’{p['word'][:10]}" for p in top_pairs[:15]]
            pairs_weights = [p['weight'] for p in top_pairs[:15]]

            axes[0].barh(range(len(pairs_labels)), pairs_weights, color='steelblue')
            axes[0].set_yticks(range(len(pairs_labels)))
            axes[0].set_yticklabels(pairs_labels, fontsize=9)
            axes[0].set_xlabel('Attention Weight', fontsize=10)
            axes[0].set_title(f'Top {len(pairs_labels)} Atom-Word Pairs', fontsize=12, fontweight='bold')
            axes[0].invert_yaxis()
            axes[0].grid(axis='x', alpha=0.3)

            # 2. Semantic category pie chart
            cat_labels = []
            cat_sizes = []
            cat_colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']

            for i, (cat, stats) in enumerate(category_stats.items()):
                cat_name = cat.replace('_', ' ').title()
                cat_labels.append(f"{cat_name}\n({stats['count']} pairs)")
                cat_sizes.append(stats['percentage'])

            if cat_sizes:
                axes[1].pie(cat_sizes, labels=cat_labels, autopct='%1.1f%%',
                           colors=cat_colors[:len(cat_sizes)], startangle=90)
                axes[1].set_title('Semantic Category Distribution', fontsize=12, fontweight='bold')

            plt.suptitle('Key Atom-Word Pairs Analysis', fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… å…³é”®åŸå­-è¯è¯­å¯¹åˆ†æå·²ä¿å­˜: {save_path}")
            plt.close()

        return analysis

    def analyze_attention_statistics(
        self,
        attention_weights,
        atoms_object,
        text_tokens,
        save_path=None
    ):
        """
        åˆ†ææ³¨æ„åŠ›åˆ†å¸ƒçš„ç»Ÿè®¡ç‰¹æ€§

        Args:
            attention_weights: ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡å­—å…¸
            atoms_object: Atomså¯¹è±¡
            text_tokens: æ–‡æœ¬tokensåˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            ç»Ÿè®¡åˆ†æç»“æœå­—å…¸
        """
        import matplotlib.pyplot as plt
        import seaborn as sns
        import numpy as np
        from scipy.stats import entropy

        atom_to_text = attention_weights.get('atom_to_text', None)
        if atom_to_text is None:
            print("âš ï¸  æ²¡æœ‰atom_to_textæ³¨æ„åŠ›æƒé‡")
            return None

        # Convert to numpy and get single sample
        atom_to_text = atom_to_text.cpu().numpy()[0]  # [heads, num_atoms, seq_len]
        num_heads, num_atoms, seq_len = atom_to_text.shape

        # Get atom elements
        elements = [str(atoms_object.elements[i]) for i in range(num_atoms)]

        # Calculate statistics
        stats = {
            'global_stats': {},
            'per_atom_stats': {},
            'per_head_stats': {}
        }

        # Global statistics
        all_attention = atom_to_text.flatten()
        stats['global_stats'] = {
            'mean': float(np.mean(all_attention)),
            'std': float(np.std(all_attention)),
            'min': float(np.min(all_attention)),
            'max': float(np.max(all_attention)),
            'median': float(np.median(all_attention)),
            'entropy': float(entropy(all_attention + 1e-10)),
            'sparsity': float(np.sum(all_attention < 0.01) / len(all_attention) * 100),  # % below threshold
            'effective_connections': float(np.sum(all_attention >= 0.01) / len(all_attention) * 100)
        }

        # Per-atom statistics (averaged over heads)
        atom_to_text_avg = atom_to_text.mean(axis=0)  # [num_atoms, seq_len]
        for i, element in enumerate(elements):
            atom_attn = atom_to_text_avg[i]
            stats['per_atom_stats'][f"{element}_{i}"] = {
                'entropy': float(entropy(atom_attn + 1e-10)),
                'max_attention': float(np.max(atom_attn)),
                'focus_level': 'high' if entropy(atom_attn + 1e-10) < stats['global_stats']['entropy'] else 'low',
                'top_word': text_tokens[np.argmax(atom_attn)],
                'top_word_weight': float(np.max(atom_attn))
            }

        # Per-head statistics
        for head in range(num_heads):
            head_attn = atom_to_text[head]
            stats['per_head_stats'][f'head_{head+1}'] = {
                'entropy': float(entropy(head_attn.flatten() + 1e-10)),
                'mean': float(np.mean(head_attn)),
                'sparsity': float(np.sum(head_attn < 0.01) / head_attn.size * 100)
            }

        # Visualize
        if save_path:
            fig = plt.figure(figsize=(16, 10))
            gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

            # 1. Attention distribution histogram
            ax1 = fig.add_subplot(gs[0, :2])
            ax1.hist(all_attention, bins=50, color='steelblue', alpha=0.7, edgecolor='black')
            ax1.axvline(stats['global_stats']['mean'], color='red', linestyle='--',
                       label=f"Mean: {stats['global_stats']['mean']:.4f}")
            ax1.axvline(stats['global_stats']['median'], color='green', linestyle='--',
                       label=f"Median: {stats['global_stats']['median']:.4f}")
            ax1.set_xlabel('Attention Weight')
            ax1.set_ylabel('Frequency')
            ax1.set_title('Global Attention Distribution')
            ax1.legend()
            ax1.grid(alpha=0.3)

            # 2. Global stats table
            ax2 = fig.add_subplot(gs[0, 2])
            ax2.axis('off')
            stats_text = f"""
Global Statistics:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Mean:     {stats['global_stats']['mean']:.4f}
Std:      {stats['global_stats']['std']:.4f}
Entropy:  {stats['global_stats']['entropy']:.2f}
Sparsity: {stats['global_stats']['sparsity']:.1f}%
Effective: {stats['global_stats']['effective_connections']:.1f}%
"""
            ax2.text(0.1, 0.5, stats_text, fontsize=10, verticalalignment='center',
                    fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

            # 3. Per-atom entropy
            ax3 = fig.add_subplot(gs[1, :])
            atom_names = list(stats['per_atom_stats'].keys())
            atom_entropies = [stats['per_atom_stats'][a]['entropy'] for a in atom_names]
            colors = ['coral' if stats['per_atom_stats'][a]['focus_level'] == 'high' else 'steelblue'
                     for a in atom_names]

            ax3.bar(range(len(atom_names)), atom_entropies, color=colors)
            ax3.axhline(stats['global_stats']['entropy'], color='red', linestyle='--',
                       label=f"Global entropy: {stats['global_stats']['entropy']:.2f}")
            ax3.set_xticks(range(len(atom_names)))
            ax3.set_xticklabels(atom_names, rotation=45, ha='right')
            ax3.set_ylabel('Entropy (lower = more focused)')
            ax3.set_title('Per-Atom Attention Entropy (Coral = High Focus, Blue = Low Focus)')
            ax3.legend()
            ax3.grid(axis='y', alpha=0.3)

            # 4. Per-head statistics
            ax4 = fig.add_subplot(gs[2, 0])
            head_names = list(stats['per_head_stats'].keys())
            head_entropies = [stats['per_head_stats'][h]['entropy'] for h in head_names]
            ax4.bar(range(len(head_names)), head_entropies, color='#99cc99')
            ax4.set_xticks(range(len(head_names)))
            ax4.set_xticklabels(head_names, rotation=45, ha='right')
            ax4.set_ylabel('Entropy')
            ax4.set_title('Per-Head Entropy')
            ax4.grid(axis='y', alpha=0.3)

            # 5. Per-head sparsity
            ax5 = fig.add_subplot(gs[2, 1])
            head_sparsity = [stats['per_head_stats'][h]['sparsity'] for h in head_names]
            ax5.bar(range(len(head_names)), head_sparsity, color='#ff9999')
            ax5.set_xticks(range(len(head_names)))
            ax5.set_xticklabels(head_names, rotation=45, ha='right')
            ax5.set_ylabel('Sparsity (%)')
            ax5.set_title('Per-Head Sparsity')
            ax5.grid(axis='y', alpha=0.3)

            # 6. Box plot for attention distribution
            ax6 = fig.add_subplot(gs[2, 2])
            ax6.boxplot(all_attention, vert=True, patch_artist=True,
                       boxprops=dict(facecolor='lightblue'))
            ax6.set_ylabel('Attention Weight')
            ax6.set_title('Attention Distribution\n(Box Plot)')
            ax6.grid(axis='y', alpha=0.3)

            plt.suptitle('Attention Distribution Statistics', fontsize=14, fontweight='bold')
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… æ³¨æ„åŠ›ç»Ÿè®¡åˆ†æå·²ä¿å­˜: {save_path}")
            plt.close()

        return stats

    def analyze_text_semantic_regions(
        self,
        attention_weights,
        atoms_object,
        text,
        text_tokens,
        save_path=None
    ):
        """
        åˆ†ææ–‡æœ¬ä¸åŒè¯­ä¹‰åŒºåŸŸçš„é‡è¦æ€§

        Args:
            attention_weights: ç»†ç²’åº¦æ³¨æ„åŠ›æƒé‡å­—å…¸
            atoms_object: Atomså¯¹è±¡
            text: åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²
            text_tokens: æ–‡æœ¬tokensåˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        import matplotlib.pyplot as plt
        import numpy as np

        atom_to_text = attention_weights.get('atom_to_text', None)
        if atom_to_text is None:
            print("âš ï¸  æ²¡æœ‰atom_to_textæ³¨æ„åŠ›æƒé‡")
            return None

        # Convert to numpy and get single sample
        atom_to_text = atom_to_text.cpu().numpy()[0]  # [heads, num_atoms, seq_len]

        # Average over heads and atoms
        word_importance = atom_to_text.mean(axis=(0, 1))  # [seq_len]

        # Simple sentence segmentation (by period, or fixed length)
        sentences = text.split('. ')

        # Simple and robust approach: match tokens to sentences by comparing
        # the detokenized prefix against sentence boundaries
        token_to_sentence = []

        # Detokenize tokens to get approximate text position
        detokenized_parts = []
        for token in text_tokens:
            if token in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']:
                detokenized_parts.append('')
            elif token.startswith('##'):
                detokenized_parts.append(token[2:])
            else:
                detokenized_parts.append(' ' + token if detokenized_parts else token)

        # Build cumulative detokenized text and map to sentences
        cumulative_text = ""
        current_sentence_idx = 0
        sentence_end_positions = []

        # Calculate character positions where each sentence ends
        pos = 0
        for i, sent in enumerate(sentences):
            pos += len(sent)
            if i < len(sentences) - 1:
                pos += 2  # ". " separator
            sentence_end_positions.append(pos)

        for i, token_part in enumerate(detokenized_parts):
            cumulative_text += token_part

            # Skip special tokens
            if text_tokens[i] in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']:
                token_to_sentence.append(-1)
                continue

            # Find which sentence we're in based on cumulative text length
            # Remove spaces for more accurate matching
            text_len = len(cumulative_text.replace(' ', ''))

            # Update sentence index based on position
            while (current_sentence_idx < len(sentence_end_positions) - 1 and
                   text_len > sentence_end_positions[current_sentence_idx]):
                current_sentence_idx += 1

            token_to_sentence.append(current_sentence_idx)

        # Aggregate importance by sentence (skip special tokens marked as -1)
        sentence_importance = {}
        for i, sent_idx in enumerate(token_to_sentence):
            if sent_idx >= 0 and i < len(word_importance):
                if sent_idx not in sentence_importance:
                    sentence_importance[sent_idx] = []
                sentence_importance[sent_idx].append(word_importance[i])

        # Calculate sentence-level statistics
        regions = []
        for sent_idx in sorted(sentence_importance.keys()):
            if sent_idx < len(sentences):
                importance_scores = sentence_importance[sent_idx]
                regions.append({
                    'region_id': sent_idx,
                    'text': sentences[sent_idx][:100] + ('...' if len(sentences[sent_idx]) > 100 else ''),
                    'avg_importance': float(np.mean(importance_scores)),
                    'max_importance': float(np.max(importance_scores)),
                    'num_tokens': len(importance_scores),
                    'contribution': 'high' if np.mean(importance_scores) > word_importance.mean() else 'medium'
                })

        # Sort by importance
        regions = sorted(regions, key=lambda x: x['avg_importance'], reverse=True)

        analysis = {
            'regions': regions,
            'word_importance': word_importance.tolist(),
            'text_tokens': text_tokens
        }

        # Visualize
        if save_path:
            fig, axes = plt.subplots(2, 1, figsize=(14, 10))

            # 1. Token-level importance over sequence
            axes[0].plot(range(len(word_importance)), word_importance, color='steelblue', linewidth=2)
            axes[0].fill_between(range(len(word_importance)), word_importance, alpha=0.3)
            axes[0].axhline(word_importance.mean(), color='red', linestyle='--',
                          label=f'Mean: {word_importance.mean():.4f}')
            axes[0].set_xlabel('Token Position', fontsize=10)
            axes[0].set_ylabel('Attention Weight', fontsize=10)
            axes[0].set_title('Token-Level Importance Over Sequence', fontsize=12, fontweight='bold')
            axes[0].legend()
            axes[0].grid(alpha=0.3)

            # 2. Sentence/region importance
            if regions:
                region_labels = [f"Region {r['region_id']+1}" for r in regions[:10]]
                region_scores = [r['avg_importance'] for r in regions[:10]]
                colors_map = {'high': 'coral', 'medium': 'steelblue', 'low': 'lightgray'}
                colors = [colors_map.get(r['contribution'], 'lightgray') for r in regions[:10]]

                axes[1].barh(range(len(region_labels)), region_scores, color=colors)
                axes[1].set_yticks(range(len(region_labels)))
                axes[1].set_yticklabels(region_labels, fontsize=9)
                axes[1].set_xlabel('Average Attention Weight', fontsize=10)
                axes[1].set_title('Semantic Region Importance', fontsize=12, fontweight='bold')
                axes[1].invert_yaxis()
                axes[1].grid(axis='x', alpha=0.3)

                # Add text preview on the right
                for i, region in enumerate(regions[:10]):
                    text_preview = region['text'][:40] + '...'
                    axes[1].text(region_scores[i] + 0.001, i, text_preview,
                               va='center', fontsize=7, style='italic')

            plt.suptitle('Text Semantic Region Analysis', fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… æ–‡æœ¬è¯­ä¹‰åŒºåŸŸåˆ†æå·²ä¿å­˜: {save_path}")
            plt.close()

        return analysis


def batch_interpretability_analysis(
    analyzer,
    test_loader,
    save_dir,
    num_samples=10,
    analyze_feature_space=True
):
    """
    æ‰¹é‡å¯è§£é‡Šæ€§åˆ†æ

    Args:
        analyzer: EnhancedInterpretabilityAnalyzer
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        save_dir: ä¿å­˜ç›®å½•
        num_samples: åˆ†ææ ·æœ¬æ•°
        analyze_feature_space: æ˜¯å¦åˆ†æç‰¹å¾ç©ºé—´

    Returns:
        summary: åˆ†ææ‘˜è¦
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n{'='*80}")
    print(f"ğŸ“Š æ‰¹é‡å¯è§£é‡Šæ€§åˆ†æ")
    print(f"{'='*80}")
    print(f"  æ ·æœ¬æ•°: {num_samples}")
    print(f"  ä¿å­˜ç›®å½•: {save_dir}")
    print(f"{'='*80}\n")

    all_graph_features = []
    all_text_features = []
    all_labels = []
    all_predictions = []

    # åˆ†ææ¯ä¸ªæ ·æœ¬
    for i, batch in enumerate(tqdm(test_loader, desc="åˆ†ææ ·æœ¬", total=num_samples)):
        if i >= num_samples:
            break

        g, lg, text, labels = batch

        # æå–ç‰¹å¾
        result = analyzer.extract_attention_weights(g, lg, text)

        if result['graph_features'] is not None:
            all_graph_features.append(result['graph_features'].cpu())
        if result['text_features'] is not None:
            all_text_features.append(result['text_features'].cpu())

        all_labels.append(labels.cpu())
        all_predictions.append(torch.tensor(result['prediction']))

    # ç‰¹å¾ç©ºé—´å¯è§†åŒ–
    if analyze_feature_space and all_graph_features and all_text_features:
        print("\nğŸ“ˆ å¯è§†åŒ–ç‰¹å¾ç©ºé—´...")

        graph_features = torch.cat(all_graph_features, dim=0).numpy()
        text_features = torch.cat(all_text_features, dim=0).numpy()
        labels = torch.cat(all_labels, dim=0).numpy()

        # t-SNE
        analyzer.visualize_feature_space(
            graph_features, text_features, labels,
            method='tsne',
            save_path=save_dir / 'feature_space_tsne.png'
        )

        # PCA
        analyzer.visualize_feature_space(
            graph_features, text_features, labels,
            method='pca',
            save_path=save_dir / 'feature_space_pca.png'
        )

    print(f"\nâœ… æ‰¹é‡åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {save_dir}\n")

    return {
        'num_samples': min(num_samples, len(test_loader)),
        'save_dir': str(save_dir)
    }
